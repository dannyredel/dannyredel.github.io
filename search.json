[
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n\n\n\n\n\n\n\n\n\n\nMachine Learning with Python: A Quick Guide\n\n\n\nMachine Learning\n\n\nPython\n\n\nscikit-learn\n\n\n\n\n30 March 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAmsterdam Airbnb Price Prediction: A Machine Learning Approach\n\n\n\nPython\n\n\nGeospatial Analysis\n\n\nMachine Learning\n\n\ngeopandas\n\n\nscikit-learn\n\n\n\n\n11 March 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImpact of Transit-Oriented Development on Dutch Commercial Real Estate Prices\n\n\n\nMaster Thesis\n\n\nEconometrics\n\n\nCausal Inference\n\n\nDiff-in-Diff\n\n\n\n\n02 January 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCustomer Analytics with R\n\n\n\nR & Rstudio\n\n\nA/B Testing\n\n\nMachine Learning\n\n\n\n\n01 December 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVoting Patterns of the Ministers of the Supreme Court in Antitrust Cases: Evidence from Chile\n\n\n\nArticle\n\n\nCompetition & Antitrust\n\n\nR & Rstudio\n\n\n\n\n18 May 2022\n\n\n\n\n\n\n\n\nNo matching items\n\n Back to top"
  },
  {
    "objectID": "posts/merger_simulation/Assignment1.html",
    "href": "posts/merger_simulation/Assignment1.html",
    "title": "Merger Simulation of Broadband Internet Services",
    "section": "",
    "text": "Assessing merger competitive effects through merger simulation.\nLet’s assume our industry concerns providers of broadband internet services. Some providers offer multiple products (e.g. multiple download speeds). In this case, the industry is best described by a heterogeneous Bertrand competition with 5 firms offering 11 products. Demand follows a simple logit model.\nProviders A and B want to merge and we were asked to evaluate the effects of the merger. In this document, we will assess the potential anticompetitive effects that the proposed merger might carry."
  },
  {
    "objectID": "posts/merger_simulation/Assignment1.html#quantities-and-prices",
    "href": "posts/merger_simulation/Assignment1.html#quantities-and-prices",
    "title": "Merger Simulation of Broadband Internet Services",
    "section": "1.1 Quantities and Prices",
    "text": "1.1 Quantities and Prices\n\ndata &lt;- data.frame(provider = c(rep(\"A\", 3),\n                                rep(LETTERS[seq(from = 2, to = 5)], \n                                    each = 2)),\n                   product = c(\"100 Mbit/s\", \n                               rep(c(\"400 Mbit/s\", \"1000 Mbit/s\"), 3),\n                               rep(c(\"200 Mbit/s\", \"400 Mbit/s\"), 2)),\n                   price = c(40, 65, 95,#Provider A\n                             50, 70,     #Provider B\n                             70, 98,     #Provider C\n                             40, 50,     #Provider D\n                             35, 45),    #Provider E\n                   share = c(0.26, 0.2, 0.05, #Provider A\n                             0.1, 0.09,       #Provider B\n                             0.08, 0.03,          #Provider C\n                             0.03, 0.05,          #Provider D\n                             0.06, 0.05))         #Provider E \n\n# We assume 9 million households, all of them have internet\nM = 9e6\n# Volume Quantity\ndata &lt;- data %&gt;% mutate(quantity = share*M)\n# vectors of prices and quantities\nQQ &lt;- as.matrix(data$share*M)\nPP &lt;- as.matrix(data$price)"
  },
  {
    "objectID": "posts/merger_simulation/Assignment1.html#demand",
    "href": "posts/merger_simulation/Assignment1.html#demand",
    "title": "Merger Simulation of Broadband Internet Services",
    "section": "1.2 Demand",
    "text": "1.2 Demand\nThe consumer utility for product i:\n\nU_i=V_i+\\varepsilon_i\n\nWhere V_i is the observable part of the utility. We assume here that V_i takes the following form:\n\nV_i= \\delta_i -\\alpha p_i+\\text{asc}_i\n\nWe can rearrange the market in such that we end up with:\n\n\\ln s_i-\\ln s_0 = \\delta_i -\\alpha p_i+\\text{asc}_i\n\nThe price sensitivity is estimated to be -0.1.\n\nalpha &lt;- -0.1\n\nThe logit demand contains alternative-specific constants (asc) that set the prediction equal to observed market shares. They can be found using the observed prices, quantities and the price sensitivity parameter (if the shares sum to 1, then no outside good is included and by default \\delta_i is normalized to 0).\n\n#intercepts are relative to reference (let's pick product 1)\ndata$asc &lt;- log(data$share) - log(data$share[1]) - \n  alpha*(data$price - data$price[1])"
  },
  {
    "objectID": "posts/merger_simulation/Assignment1.html#firm-ownership",
    "href": "posts/merger_simulation/Assignment1.html#firm-ownership",
    "title": "Merger Simulation of Broadband Internet Services",
    "section": "1.3 Firm Ownership",
    "text": "1.3 Firm Ownership\nTable 1 presents the ownership matrix of which products belong to which providers.\n\n\n\n\nTable 1: Pre-Merger Ownership Structure"
  },
  {
    "objectID": "posts/merger_simulation/Assignment1.html#relevant-market",
    "href": "posts/merger_simulation/Assignment1.html#relevant-market",
    "title": "Merger Simulation of Broadband Internet Services",
    "section": "2.1 Relevant Market",
    "text": "2.1 Relevant Market\nTo assess the potential competitive effects of the proposed merger, we will start by discussing briefly the relevant market.\nAs already mentioned, providers of internet usually offer multiple products with certain degree of differentiation. In particular, the various internet offerings with various speeds could be a plausible segmentation of the market for the retail provision of internet services. According to our data, the actual providers offers four different internet speeds: 100 Megabits per second (Mbps), 200 Mbps, 400 Mbps and 1000 Mbps.\nIn recent cases regarding the retail market for internet access, the Dutch competition agency, Authority for Consumers & Markets (ACM), had considered that a market segmentation based on download speeds is not appropriate, primarily because there is substitution both at the demand side and at the supply-side of the market (“Marktanalyse ontbundelde toegang“, 2015). In his view, “the fact that an internet connection, regardless of speed, can be used for the same applications, a distinction based on usage in the market for fixed internet access based on certain speeds is unlikely” (pp. 156-157).\nThe European Commission had followed the a similar approach by either disregarding the possibility of further market segmentation based on internet speed, or by considering it a “left open” issue (DGCOMP/Case M.7978, DGCOMP/Case M.5532, DGCOMP/Case M.6990).\nAs we will see, the quantitative analysis of elasticities and diversion ratios confirms the presence of relevant product substitution between different internet speeds, such that products with different speeds exert competitive pressure on each other.\nTherefore, and also in line with previous European jurispudence, we will take the broadband of internet services as the product relevant market, without further distinctions based on internet speed. The geographic relevant market is at the national level."
  },
  {
    "objectID": "posts/merger_simulation/Assignment1.html#structural-analysis",
    "href": "posts/merger_simulation/Assignment1.html#structural-analysis",
    "title": "Merger Simulation of Broadband Internet Services",
    "section": "2.2 Structural Analysis",
    "text": "2.2 Structural Analysis\nOnce defined the relevant market, we can now study their structure in terms of market concentration. The left-hand side of Figure 1 reports the pre-merger market shares measured in terms of units sold (quantity):\n\n\n\n\n\n\n\n\nFigure 1: Market Shares\n\n\n\n\n\nFirst, we observe a clear leadership of firm A in the market, with an overall market share of 51%. In contrast, the individual market shares of their rivals are between 11% and 19%, with firm B being the second largest provider of internet.\nNote also that firm A is the only firm that still offers low-speed broadband services (100 Mbps), accounting for the 26% of the overall market. On the other hand, all providers participate on the “400 Mbps” segment. More to the point, the Parties (A and B) offers internet with speed of 400 Mbps and 1,000 Mbps, but do not offer 200 Mbps.\nThe right-hand side of Figure 1 plots the combined market share of the Parties.1 This figure suggests a plausible degree of dominance of the New Entity (AB) as a result of the merger, as the combined market share is now 70%.\nAccording to Commission’s Guidelines on Horizontal Mergers (see Commission 2004), market shares of 50% or more might be evidence of the existence of a dominant market position. However, smaller competitors may act as a sufficient constraining influence if, for example, they have the ability and incentive to increase their supplies.\nThe overall concentration level in a market may also provide useful information about the competitive situation. Table 1 shows the overall concentration levels in terms of Herfindahl-Hirschman Index (HHI).\n\nHerfindahl-Hirschman Index (HHI)\n\n\nHHI\nValue\n\n\n\n\nPre-Merger\n3,268\n\n\n\\Delta HHI\n1,938\n\n\nPost-Merger\n5,206\n\n\n\nTable 1 also seems to suggest high market concentration (above 2,000).2 While these results give an initial indication of the competitive concern, further investigation is needed."
  },
  {
    "objectID": "posts/merger_simulation/Assignment1.html#unilateral-effects",
    "href": "posts/merger_simulation/Assignment1.html#unilateral-effects",
    "title": "Merger Simulation of Broadband Internet Services",
    "section": "2.3 Unilateral Effects",
    "text": "2.3 Unilateral Effects\nWe will now study the potential unilateral effects that may arise due to the merger. By removing one competitor from the market, the merger also reduces competitive constraints that in turn lead to price increases in the relevant market.\n\n2.3.1 Price-Elasticities: Product Substitution\nThe price-elasticity matrix provides a good overview of the competitive pressure each product exerts to each other. On the one hand, both own and cross-elasticities allows us to understand the disciplinary pressure that firms A and B faces from their smaller rivals. On the other hand, elasticities give us information to assess closeness of competition between firm A and B. The underlying idea is that the closer competitors the merging parties are3, the more likely it is that a merger will give rise to significant unilateral effects.\nWe will write a function to calculate elasticities using the given data and price sensitivity. The assumptions underlying Logit demand imply that the probability that a consumer purchases product i is given by:\n\n\\Pr(U_{ij}&gt;U_{ik})=s_i=\\frac{\\exp(V_i)}{\\sum_{k} \\exp(V_k)}\n\nwith V_i=\\delta_i+\\alpha p_i+\\text{asc}_i (Berry 1994). The implies elasticities:\n\nThe own-price elasticity for product i is defined by:\n\n\n\\epsilon_i^{i}=\\alpha p_i(1-s_i)\n\n\nThe cross-price elasticity for product i in reaction to change in price for product j:\n\n\n\\epsilon_j^{i}=-\\alpha p_js_j\n\n\nelast &lt;- function(products, alpha) {\n  own &lt;- alpha*products$price*(1-Pr)\n  cross &lt;- -alpha*products$price %o% Pr\n  E &lt;- cross\n  diag(E) &lt;- own\n  return(E)\n}\n\nFigure 2 shows the matrix of own and cross-price elasticities between the products involved in the relevant market:\n\n\n\n\n\n\n\n\nFigure 2: Own and Cross-Price Elasticities\n\n\n\n\n\nRecall that when the cross elasticity of demand \\epsilon_{ij} is greater than 1, the cross elasticity of demand is elastic: a change in price of product j results in a more than proportionate change in quantity demanded for product i.\nFrom Figure 2 we can see high substitution between the Parties, specially from the products offered by B towards the products of A. For example, a 1% price increase in 400 Mbps internet offered by B increases by 1.3% the demand for 100 Mbps internet services of firm A. For the 1000 Mbps service offered by B, the demand increase is 2.45% towards firm A (100 Mbps). On the other hand, the elasticity from firm A towards B is less elastic (between 0.6 and 0.95 in the segments where they overlap).\nFinally, it’s important to note that firm C (400 and 1000 Mbps) and to some extent D (200 and 400 Mbps) exerts quite an important competitive pressure on various of the products offered by A (100 Mbps and 400 Mbps). This implies that, even post-merger, there is still high competitive pressure coming from other products from rivals.\nTherefore, we confirm the presence of high substitution across the different products of internet services."
  },
  {
    "objectID": "posts/merger_simulation/Assignment1.html#calibrate-marginal-costs",
    "href": "posts/merger_simulation/Assignment1.html#calibrate-marginal-costs",
    "title": "Merger Simulation of Broadband Internet Services",
    "section": "3.1 Calibrate Marginal Costs",
    "text": "3.1 Calibrate Marginal Costs\nWe will first calibrate the marginal costs.\n\n#calculate elasticities\ndata$PCand &lt;- data$price # candidate price is current price\ndata$s &lt;- data$share # idem\n\nelas &lt;- elast(data, alpha)\n\n#drop columns\ndata$PCand &lt;- data$s &lt;- NULL\n\n#calculate matrix of marginal effects\npart &lt;- E*(QQ%*%t(1/PP))\n\n#calculate implied marginal costs\nCC &lt;- PP + diag(own_pre)*solve(own_pre * part) %*% QQ"
  },
  {
    "objectID": "posts/merger_simulation/Assignment1.html#first-order-conditions-foc",
    "href": "posts/merger_simulation/Assignment1.html#first-order-conditions-foc",
    "title": "Merger Simulation of Broadband Internet Services",
    "section": "3.2 First-Order Conditions (FOC)",
    "text": "3.2 First-Order Conditions (FOC)\nSecond, we define a function calculating the first order conditions. The function will give a vector corresponding to FOC value for each product. In equilibrium, we should get a vector of zeros.\nThe part calculating the matrix of marginal effects \\dfrac{\\partial Q_i}{\\partial p_j} can be calculated by using the following relationship:\n\\epsilon_{ij}=\\dfrac{\\dfrac{\\partial Q_i}{\\partial p_j}}{\\dfrac{Q_i}{p_j}}\n\n\nFOC &lt;- function(delta, products, EIG, C, M) { \n  products$PCand = products$price*delta\n  products$s = exp(products$asc + \n                     alpha*products$PCand)/sum(exp(products$asc + \n                                                     alpha*products$PCand))\n  #quantities\n  Q &lt;- as.matrix(products$s*M)\n  #prices\n  P &lt;- as.matrix(products$PCand)\n  #calculate elasticities\n  E &lt;- elast(products, alpha)\n  #marginal effects\n  PART &lt;- E*(Q%*%t(1/P))\n  #FOCs\n  thisFOC &lt;- diag(EIG)*Q + (EIG * PART) %*% (P - C)\n  return(as.vector(thisFOC))\n}\n\nAll we need to do now is to adjust the ownership matrix to reflect the change in market structure due to the merger, as shown in Table 2 .\n\n\n\n\nTable 2: Post-Merger Ownership Structure\n\n\n\n\n\n\n\n\n\n\nAnd we now solve the FOC’s with the new ownership structure with the following function:\n\n#post-merger equilibrium\nresult &lt;- BBsolve(rep(1, 11),\n                       function(x){FOC(x,\n                                        products = data,\n                                        EIG = own_post,\n                                        C = CC,\n                                        M = M)})\n\n  Successful convergence."
  },
  {
    "objectID": "posts/merger_simulation/Assignment1.html#post-merger-effects",
    "href": "posts/merger_simulation/Assignment1.html#post-merger-effects",
    "title": "Merger Simulation of Broadband Internet Services",
    "section": "3.3 Post-Merger Effects",
    "text": "3.3 Post-Merger Effects\n\n\n\n\n\n\na) What is the average estimated price effect?\n\n\n\nTable 3 reports the average price effects of the merger according to our simulation exercise. We observe that the the loss of a competitor in the relevant market due to the merger leads to a new equilibrium with higher prices on all products. In particular, we found a weighted average price effect4 of around 1.54 euros, that is, a 3 percentage point increase approximately relative to the pre-merger scenario. Figure 3 reports the post-merger effects of each product that resulted from our simulation:\n\n\n\n\nTable 3: Post-Merger Price Effects\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 3: Post-Merger Price Effects\n\n\n\n\n\nFigure 3 shows that most of the price increase is primarily driven by the behavior of the New Entity. The simulation predicts a price-increase of around 4.5% for the 100 Mbps service offered by firm A and 4.3% for the 400 Mbps of firm B.\nTo put this in perspective, we can compare it to the results found in Grzybowski and Pereira (2007), were the authors carried a merger simulation in the Portuguese mobile telephony market. They found an average price increase of 7–10% without cost efficiencies. One firm could increase the price by as much as 13–22%.\nAccording to these results, the competitive pressure exerted from rivals is not enough to prevent the New Entity to raise their prices. These results, though modest, suggests competitive risk in the relevant market.\n\n\n\n\n\n\n\n\nFigure 4: Predicted Market Shares\n\n\n\n\n\nFinally, we can also estimate the new market shares, as shown in Figure 4. Here we see that practically the same results found in our initial assessment based on simple combined market share between the Parties (Figure 1). Taken all into account, we conclude the existence of unilateral price effects on the relevant market."
  },
  {
    "objectID": "posts/merger_simulation/Assignment1.html#entry-conditions",
    "href": "posts/merger_simulation/Assignment1.html#entry-conditions",
    "title": "Merger Simulation of Broadband Internet Services",
    "section": "4.1 Entry Conditions",
    "text": "4.1 Entry Conditions\nTypically, antitrust agencies evaluate whether entry will be likely, timely, and of sufficient magnitude as to discipline post-merger exercise of market power (see Commission 2004). When entering a market is sufficiently easy, a merger is unlikely to pose any significant anti-competitive risk.\nPotential barriers to entry in the broadband internet market could be related to technical advantages that incumbents enjoy, for example, access to essential facilities (network infrastructure, local loops, etc.), which make it difficult for potential entrants to compete successfully. All these investments presents high sunk costs for entrants. This could also implies the presence of large economies of scale and scope, that may also constitute barriers to entry.\nFor example, in Xiao and Orazem (2011), the authors study the importance of sunk costs in determining entry conditions. In the conventional framework, entrants incur sunk costs to enter, while incumbents disregard these costs in deciding on continuation or exit. The authors apply this framework to study entry and competition in the local U.S. broadband markets and find that entry costs for early entrants are smaller than for later entrants, implying the existence of early mover advantages in this market. They also find that entry conditions for the 4th firm and subsequent entrants are stable. This implies that, once the market has between one to three incumbent firms, the fourth entrant has little effect on competitive conduct in the local broadband market.\nThere is also literature that suggests that mergers create new incentives on incumbents to actually create entry barriers in a profitable way. For example, Das Varma and De Stefano (2022) introduced the idea that entry deterrence is a public good amongst incumbents, having the incentive to free-ride on each other’s efforts to deter entry. According to authors, a merger between two incumbents eliminates free-riding between the merging firms, leading to an increase in post-merger investment in entry deterrence.\nOn the other hand, we need to recall that the relevant market in our case is at the retail level. Because of the unbundling regulation that generally applies in the network industries in the Netherlands (CPB, 2005), new entrants can compete with incumbents (such as KPN) on the market for service provision without needing to invest in the essential network facilities. In this case, unbundling means that other firms have access to the local loop, which requires incumbents to allow competitors to install their own equipment on both sides of the loop to provide their own services (CPB, 2005).\nThis is an important factor to consider when assessing entry barriers in the market of broadband internet services, as it would mean lower barriers for entrants in the services market or retail market. An interesting aspect to analyze is whether cable network providers can provide easy access to their connections or if other technical or compatibility barriers still exist."
  },
  {
    "objectID": "posts/merger_simulation/Assignment1.html#efficiency-claims",
    "href": "posts/merger_simulation/Assignment1.html#efficiency-claims",
    "title": "Merger Simulation of Broadband Internet Services",
    "section": "4.2 Efficiency Claims",
    "text": "4.2 Efficiency Claims\nIts likely that the Parties had presented efficiencies that may counteract the effects on competition and in particular the potential harm to consumers that it might otherwise have. According to the Guidelines, for the Commission to take account of efficiency claims, the efficiencies have to benefit consumers, be merger-specific and be verifiable. These conditions are cumulative.\nSome theoretical research on the effects of mergers in the telecommunications industry has found the presence of relevant cost-reduction efficiencies. For example, Houngbonon and Jeanjean (2019) studied mobile internet traffic from 18 European markets and found that consumer surplus is maximized in generally markets with 3 symmetric operators. They also suggested that, in mobile mergers, dynamic efficiencies from investment outweigh static efficiencies from market power (except in the case of “3-to-2 mergers”).\nA useful way to take efficiencies into account is to incorporate them in our merger simulation model. For example, in Grzybowski and Pereira (2007), the authors simulated the post-merger effects of the Portuguese mobile telephony market and considered three scenarios: (i) there are no cost efficiencies, (ii) a 5% marginal cost reduction in the costs of the Parties, and (iii) a 10% reduction. On average, the authors found price increases of about 6–10% with a 10% marginal cost reduction.\nAccording the Taragin and Sandfort (2012), we can update our model to evaluate these efficiencies in two different ways. First, when computing the post-merger equilibrium prices, we can insert post-merger marginal costs by multiplying pre-merger marginal costs with the claimed proportional reduction in marginal costs, 1+\\Delta mc (“mcDelta”).\nSecond, we can compute the compensating marginal cost reduction (CMCR) on the merging parties’ products. CMCR is the percentage decrease in the marginal costs of the merging parties’ products necessary to prevent a post-merger price increase (Miller and Sheu 2021). For the Bertrand case, the matrix formula of the compensating marginal cost reduction, expressed as a percentage of pre-merger costs, is:\n\n\\text{CMCR}= (m_{\\text{post}}-m_{\\text{pre}}) \\circ \\frac{1}{1-m_{\\text{pre}}}"
  },
  {
    "objectID": "posts/merger_simulation/Assignment1.html#alternative-elasticity-function",
    "href": "posts/merger_simulation/Assignment1.html#alternative-elasticity-function",
    "title": "Merger Simulation of Broadband Internet Services",
    "section": "Alternative Elasticity Function",
    "text": "Alternative Elasticity Function\nAnother way to calculate the elasticities is to just plug-in the market shares s_i into the formula in the following way:\n\nelast1 &lt;- function(products, alpha) {\n  own &lt;- alpha*products$price*(1-products$share)\n  cross &lt;- -alpha*products$price %o% products$share\n  E &lt;- cross\n  diag(E) &lt;- own\n  return(E)\n}\n\n\n\n\n\n\n\n\n\nFigure 5: Own and Cross-Price Elasticities"
  },
  {
    "objectID": "posts/merger_simulation/Assignment1.html#footnotes",
    "href": "posts/merger_simulation/Assignment1.html#footnotes",
    "title": "Merger Simulation of Broadband Internet Services",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAs in current practice, post-merger market shares are calculated on the assumption that the post-merger combined market share of the merging parties is the sum of their pre-merger market shares.↩︎\nAs stated in the Guidelines, the Commission is unlikely to identify horizontal competition concerns in a merger with a post-merger HHI between 1000 and 2000 and a delta below 250, or a merger with a post-merger HHI above 2000 and a delta below 150↩︎\nCloseness of competition is defined with reference to the propensity of the customers of one of the merging parties to switch to products supplied by the other merging party. The greater that propensity, the closer competitors the two firms are said to be. See paragraph 28, EC Merger Guidelines.↩︎\nThe product-price effects are averaged using the post-merger market shares as weights.↩︎"
  },
  {
    "objectID": "posts/gasoline_merger_did/merger-report.html",
    "href": "posts/gasoline_merger_did/merger-report.html",
    "title": "Retail Gasoline Merger in Chile: An Ex-Post Merger Evaluation",
    "section": "",
    "text": "Ex-post assessments of merger effects in Chilean’s retail gasoline market."
  },
  {
    "objectID": "posts/gasoline_merger_did/merger-report.html#identification-strategy",
    "href": "posts/gasoline_merger_did/merger-report.html#identification-strategy",
    "title": "Retail Gasoline Merger in Chile: An Ex-Post Merger Evaluation",
    "section": "4.1. Identification Strategy",
    "text": "4.1. Identification Strategy\nA simple before-after comparison will lead to biased estimates of the merger effects, given that observed price changes might stem from changes in demand or costs. Therefore, we aim to compare price changes around the merger to a counterfactual scenario in which no merger took place. Based on ex-post merger evaluation literature (Dafny et al. [2012]; Ashenfelter et al. [2015]; Argentesi et al. [2021]), I compare the geographical market in which both the acquirer (COPEC) and the target (CGL) operated in the pre-merger period to control markets that did not experience a change in market concentration.\nMy identification strategy relies on the expectation that competitive effects of the merger are likely to be stronger in the so-called overlap area between the merging parties than in non-overlap areas where the parties did not compete with each other door to door, since only in overlap areas did the intensity of competition change because of the merger. We thus can compare areas that experienced a change in market concentration (treated group) to markets without a pre-merger overlap (control group). Finally, this causal effect can be identified by employing a Difference-in-Differences (DiD) methodology that compares overlap and non-overlap areas to get the Average Treatment Effects on the Treated at the local level.\nAnother assumption that is implicitly here is the assumption that in retail gasoline markets competition works at the local level. Any comparison between treated and control areas will be able to identify merger treatment effects only if competition is, at least to some extent, local. Although in this article we will not provide evidence to support this, previous literature tend to suggest that retail gasoline market is local rather than national (see here and here).\n\n4.1.1. Geographical Relevant Market\nThe impact of COPEC’s acquisition on the market structure around each gas station is measured as the change in the number of independent competing brands within a specific radius. However, determining the appropriate radius size is a critical consideration, not to be underestimated.\nThe Chilean competition authority (FNE), in its merger report, defined the relevant geographical market as the entire Castro County/Commune. According to their assessment, this determination was based on jurisprudence, which considers the local or communal market due to the difficulties and costs consumers face in traveling between gas stations in terms of time, convenience, and fuel expenses. Additionally, international jurisprudence has also noted that retail operators typically operate in local markets, monitoring competitor prices within a relatively limited radius, typically around 3 miles (approximately 5 kilometers), or based on travel isochrones of 10 minutes for urban areas and 20 minutes for rural areas (Case No. ME/3933/08). Consequently, I will adopt a 5 km radius to delineate the geographical relevant markets.\n\n\nCode\nimport geopandas as gpd\nfrom shapely.geometry import Point\n\n# Filter the DataFrame for the unit with Id='co1020101'\nunit_co1020101 = df[df['Id'] == 'co1020101']\n\n# Get the unique latitude and longitude values\nunique_latitudes = unit_co1020101['Latitud'].unique()\nunique_longitudes = unit_co1020101['Longitud'].unique()\n\n# Target coordinates (longitude, latitude)\ntarget_crs = (unique_longitudes, unique_latitudes)\n\n# Convert DataFrame to GeoDataFrame\ngrouped_df = df.groupby('Id').agg({'Latitud': 'first', 'Longitud': 'first'}).reset_index()\nnew_df = pd.DataFrame(grouped_df)\ngdf_units = gpd.GeoDataFrame(new_df, geometry=gpd.points_from_xy(new_df.Longitud, new_df.Latitud), crs=\"EPSG:4326\")\n\n# Create a Shapely Point object for the target coordinates\ntarget_point = gpd.GeoDataFrame({'geometry': [Point(target_crs)]}, crs=\"EPSG:4326\")\n# Buffer the point to create a circle with a radius of 5000 meters\nbuffer_radius_deg = 5000 / 111000  # 1 degree of latitude is approximately 111000 meters\n\nbuffer_circle = target_point.buffer(buffer_radius_deg)\nbuffer_area = buffer_circle.to_crs(gdf_units.crs)\n\n# Check if each unit's coordinates fall within the buffer circle\ngdf_units['treat'] = gdf_units.geometry.within(buffer_area.iloc[0])\n\n# Convert treat column to integer (1 for True, 0 for False)\ngdf_units['treat'] = gdf_units['treat'].astype(int)\ngdf_units['Treated'] = np.where(gdf_units['treat'] == 1, 'Treated', 'Control')\ndf = pd.merge(df, gdf_units[['Id', 'Treated', 'treat']], on='Id', how='left')\n\n# Count treated and control units\ntreated_count = gdf_units['treat'].value_counts()[1]\ncontrol_count = gdf_units['treat'].value_counts()[0]\n\nprint(\"Treated units:\", treated_count)\nprint(\"Control units:\", control_count)\n\n\nTreated units: 6\nControl units: 150\n\n\nThis selection process resulted in a total of 6 service stations falling within the defined overlap area, thus constituting the treated group. The remaining 150 stations outside this area will serve as the control group. Figure 2 provides a visual and interactive representation of the spatial distribution of these stations:\n\n\nCode\nimport folium\nfrom folium.plugins import MarkerCluster\n\n# Filter GeoDataFrame by Region=='Castro'\ndf_lagos = df[df['Region'] == 'Los Lagos']\ndf_lagos = df_lagos.dropna(subset=['Latitud', 'Longitud'])\nprice_id = df_lagos.groupby('Id').agg({'Precio': 'mean', 'Latitud': 'first', 'Longitud': 'first'}).reset_index()\ngdf = gpd.GeoDataFrame(price_id, geometry=gpd.points_from_xy(price_id.Longitud, price_id.Latitud), crs=\"EPSG:4326\")\n\n# Create a map centered around Los Lagos region\ntarget_crs = [-42.470071, -73.76461]\nm = folium.Map(location=target_crs, zoom_start=11)\n\n# Create a MarkerCluster to add all the points\nmarker_cluster = MarkerCluster().add_to(m)\n\n# # Iterate over each row in the GeoDataFrame and add a marker to the map\nfor idx, row in gdf.iterrows():\n    folium.Marker(location=[row['Latitud'], row['Longitud']], popup=row['Id'], \n                  ).add_to(marker_cluster)\n\nradius = 5000\nfolium.Circle(\n    location=target_crs,\n    radius=radius,\n    color=\"black\",\n    weight=1,\n    fill_opacity=0.1,\n    opacity=1,\n    fill_color=\"blue\",\n    fill=False,  # gets overridden by fill_color\n    popup=\"{} meters\".format(radius),\n    tooltip=\"I am in meters\",\n).add_to(m)\n\n# Show\nm\n\n\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\nFigure 2: Map with Treated Service Stations\n\n\n\n\n\n\n4.1.2. Treatment Period\nRegarding the treatment period, it’s important to acknowledge that certain treated service stations ceased price postings during the interval between the merger approval (December 2016) and the beginning of operations under the re-branded identity (around January 2018). Consequently, observations within this transitional period are excluded from analysis, aligning the last pre-merger period with November 2016 and the first post-merger period with February 2018.\n\n\nCode\nimport pandas as pd\n\n# Convert 'date' column to datetime\ndf['date'] = pd.to_datetime(df['date'])\ndf['year_month'] = df['date'].dt.to_period('M')\n\n# Define the date thresholds\npost_date_1 = pd.to_datetime(\"2016-12-02\")\npost_date_2 = pd.to_datetime(\"2017-11-01\")\n\n# Create 'Post' variable\ndf['Post'] = (df['date'] &gt;= post_date_1).astype(int)\n\n# Create 'post1' variable\ndf['post1'] = ((df['date'] &gt;= post_date_1) & (df['date'] &lt; post_date_2)).astype(int)\n\n# Create 'post2' variable\ndf['post2'] = (df['date'] &gt;= post_date_2).astype(int)\n\n\n# sh1020101 comes back at 2017-11-30 or 11\n# pe1020101 comes back at 2018-02-22 or 14\n\n# Filter the DataFrame for Balanced Panel\ndf = df[ (df['date'] &gt;= pd.to_datetime(\"2018-02-22\")) | (df['date'] &lt;= pd.to_datetime(\"2016-12-02\"))  ]\n\n# Get unique dates\nunique_dates = sorted(set(df['date']))\n\n# Create DataFrame from unique dates\nunique_dates_df = pd.DataFrame({'date': unique_dates})\n\n# Add a new column containing numbers from 1 to the number of rows\nunique_dates_df['event_date'] = range(1 - 25 , len(unique_dates_df) + 1 - 25)\n\n# Merge with original DataFrame based on 'date' column\ndf = pd.merge(df, unique_dates_df, on='date', how='left')\n\n\n\n\nCode\n# Count occurrences of each Id\nid_counts = df['Id'].value_counts()\n\n# Check if it's a balanced panel\nbalanced_panel = id_counts.nunique() == 1\n\n# If not balanced, identify Id with different counts\nif not balanced_panel:\n    unbalanced_ids = id_counts[id_counts != id_counts.iloc[0]].index\n    \n    # Remove rows corresponding to unbalanced Id\n    df = df[~df['Id'].isin(unbalanced_ids)]\n\n# Print the balanced panel status and the number of rows after removing unbalanced Ids\nprint(\"Is it a balanced panel?\", balanced_panel)\nprint(\"Number of rows after removing unbalanced Ids:\", len(df))\n\n# Check if 'sh1020101' and 'pe1020101' are in the filtered DataFrame\ntreated_ids = ['sh1020101', 'pe1020101']\nremoved_ids = [id for id in treated_ids if id not in df['Id'].unique()]\n\n# Print the result\nif removed_ids:\n    print(\"The following treated Ids were removed:\", removed_ids)\nelse:\n    print(\"All treated Ids are still present in the DataFrame.\")\n\n\nIs it a balanced panel? False\nNumber of rows after removing unbalanced Ids: 18423\nAll treated Ids are still present in the DataFrame.\n\n\nFurthermore, to ensure a balanced panel dataset, additional stations with irregular posting behavior during the relevant study period are excluded. Following these adjustments, the dataset comprises 18,423 observations."
  },
  {
    "objectID": "posts/gasoline_merger_did/merger-report.html#summary-statistics",
    "href": "posts/gasoline_merger_did/merger-report.html#summary-statistics",
    "title": "Retail Gasoline Merger in Chile: An Ex-Post Merger Evaluation",
    "section": "4.2. Summary Statistics",
    "text": "4.2. Summary Statistics\nTable 2 and Table 3 offer summary statistics on both the treated and non-treated units within the dataset.\n\n\nCode\n# Create a new column 'Brand' based on conditions\ndf['Brand'] = np.where(df['Nombre Distribuidor'] == 'COPEC', 'COPEC', 'Branded Rivals')\ndf.loc[df['Id'] == 'sh1020101', 'Brand'] = 'Non Branded'\n\n# Pivot the dataframe with 'treat' as columns\nsummary_stats = df.groupby(['treat', 'Brand'])['Id'].nunique().reset_index(name='Id')\n\npivot_summary_stats = summary_stats.pivot_table(index='Brand', columns='treat', values='Id', fill_value=0)\n\n# Print the pivot table\npivot_summary_stats\n\n\n\n\nTable 2: Pre-Merger Sample of Service Stations\n\n\n\n\n\n\n\n\n\n\ntreat\n0\n1\n\n\nBrand\n\n\n\n\n\n\nBranded Rivals\n46\n3\n\n\nCOPEC\n37\n2\n\n\nNon Branded\n0\n1\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Pivot the dataframe to have 'Combustible' values as columns\navg_summary = df.groupby(['treat', 'Post', 'Combustible'])['Precio'].mean().reset_index()\npivot_avg_summary = avg_summary.pivot_table(index=['Combustible', 'treat'], columns='Post', values='Precio')\npivot_avg_summary.round(1)\n\n\n\n\nTable 3: Preliminary Summary Statistics\n\n\n\n\n\n\n\n\n\n\n\nPost\n0\n1\n\n\nCombustible\ntreat\n\n\n\n\n\n\nGasolina 93\n0\n727.1\n844.8\n\n\n1\n735.4\n858.8\n\n\nGasolina 97\n0\n805.8\n902.9\n\n\n1\n815.4\n918.2\n\n\nPetroleo Diesel\n0\n501.1\n622.8\n\n\n1\n515.3\n639.4\n\n\n\n\n\n\n\n\n\n\n\nFigure 3 illustrates the comparative trajectory of average gasoline prices per type in the treated area against those in non-treated areas.\n\n\nCode\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Plot the average price over time for each treatment group\nfig, axes = plt.subplots(3, 2, figsize=(15, 12))\n\n# Filter DataFrame for Combustible=='Petroleo Diesel'\nfiltered_df = df[df['Combustible'] == 'Gasolina 93']\n\n# Group by 'Treated' and 'date' and calculate the mean price\navg_price = filtered_df.groupby(['Treated', 'event_date'])['Precio'].mean().reset_index()\n\n# First subplot\nfor Treated, data in avg_price.groupby('Treated'):\n    axes[0,0].plot(data['event_date'], data['Precio'], label=Treated)\n\n# Add vertical lines\naxes[0,0].axvline(-0.5, linestyle='--', color='grey')\n\n# Set labels and title\naxes[0,0].set_xlabel('Fecha')\naxes[0,0].set_ylabel('Precio')\naxes[0,0].set_title('Average Price of Gasolina 93 over Time')\naxes[0,0].legend(title='Treatment', loc='best')\n\n# Second subplot\n# Pivot the dataframe to have treated and control groups as columns\npivot_avg_price = avg_price.pivot(index='event_date', columns='Treated', values='Precio')\n\n# Calculate the difference between treated and control\ndifference = pivot_avg_price['Treated'] - pivot_avg_price['Control']\n\n# Plot the difference over time\naxes[0,1].plot(difference.index, difference.values, color='blue')\n\n# Add vertical lines\naxes[0,1].axvline(-0.5, linestyle='--', color='grey')\n\n# Set labels and title\naxes[0,1].set_xlabel('Fecha')\naxes[0,1].set_ylabel('Price Difference (Treated - Control)')\naxes[0,1].set_title('Difference in Average Price of Gasolina 93 between Treated and Control')\n\n\n\n# Filter DataFrame for Combustible=='Gasolina 97'\nfiltered_df = df[df['Combustible'] == 'Gasolina 97']\n\n# Group by 'Treated' and 'date' and calculate the mean price\navg_price = filtered_df.groupby(['Treated', 'event_date'])['Precio'].mean().reset_index()\n\n# First subplot\nfor Treated, data in avg_price.groupby('Treated'):\n    axes[1,0].plot(data['event_date'], data['Precio'], label=Treated)\n\n# Add vertical lines\naxes[1,0].axvline(-0.5, linestyle='--', color='grey')\n\n# Set labels and title\naxes[1,0].set_xlabel('Fecha')\naxes[1,0].set_ylabel('Precio')\naxes[1,0].set_title('Average Price of Gasolina 97 over Time')\naxes[1,0].legend(title='Treatment', loc='best')\n\n# Second subplot\n# Pivot the dataframe to have treated and control groups as columns\npivot_avg_price = avg_price.pivot(index='event_date', columns='Treated', values='Precio')\n\n# Calculate the difference between treated and control\ndifference = pivot_avg_price['Treated'] - pivot_avg_price['Control']\n\n# Plot the difference over time\naxes[1,1].plot(difference.index, difference.values, color='blue')\n\n# Add vertical lines\naxes[1,1].axvline(-0.5, linestyle='--', color='grey')\n\n# Set labels and title\naxes[1,1].set_xlabel('Fecha')\naxes[1,1].set_ylabel('Price Difference (Treated - Control)')\naxes[1,1].set_title('Difference in Average Price of Gasolina 97 between Treated and Control')\n\n\n\n# Filter DataFrame for Combustible=='Petroleo Diesel'\nfiltered_df = df[df['Combustible'] == 'Petroleo Diesel']\n\n# Group by 'Treated' and 'date' and calculate the mean price\navg_price = filtered_df.groupby(['Treated', 'event_date'])['Precio'].mean().reset_index()\n\n# First subplot\nfor Treated, data in avg_price.groupby('Treated'):\n    axes[2,0].plot(data['event_date'], data['Precio'], label=Treated)\n\n# Add vertical lines\naxes[2,0].axvline(-0.5, linestyle='--', color='grey')\n\n# Set labels and title\naxes[2,0].set_xlabel('Fecha')\naxes[2,0].set_ylabel('Precio')\naxes[2,0].set_title('Average Price of Diesel over Time')\naxes[2,0].legend(title='Treatment', loc='best')\n\n# Second subplot\n# Pivot the dataframe to have treated and control groups as columns\npivot_avg_price = avg_price.pivot(index='event_date', columns='Treated', values='Precio')\n\n# Calculate the difference between treated and control\ndifference = pivot_avg_price['Treated'] - pivot_avg_price['Control']\n\n# Plot the difference over time\naxes[2,1].plot(difference.index, difference.values, color='blue')\n\n# Add vertical lines\naxes[2,1].axvline(-0.5, linestyle='--', color='grey')\n\n# Set labels and title\naxes[2,1].set_xlabel('Fecha')\naxes[2,1].set_ylabel('Price Difference (Treated - Control)')\naxes[2,1].set_title('Difference in Average Price of Diesel between Treated and Control')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nFigure 3: Price Evolution in Combustibles, by Treatment\n\n\n\n\n\nNotably, the market’s high price transparency is reflected in closely aligned pre-merger prices between treated and non-treated stations. However, this coherence is less pronounced in the case of Diesel, where price trends in time appear to diverge.\nNevertheless, the graphical assessment suggests that the crucial assumption of the Difference-in-Differences (DiD) methodology —the parallel trend assumption— appears to be upheld within our sample, particularly evident for Gasoline 93 and 97. Our event-study framework aims to further elucidate and quantify these patterns."
  },
  {
    "objectID": "posts/gasoline_merger_did/merger-report.html#empirical-model",
    "href": "posts/gasoline_merger_did/merger-report.html#empirical-model",
    "title": "Retail Gasoline Merger in Chile: An Ex-Post Merger Evaluation",
    "section": "4.3. Empirical Model",
    "text": "4.3. Empirical Model\nI examined the impact of the merger on retail gasoline prices within affected local markets using a difference-in-differences (DiD) framework that compares the price changes in a selection of stations that were located in overlap areas with the change in the same outcome variable in other stations from the non-overlap areas before and after the merger. My baseline specification takes the following form:\n\nP_{it} = \\beta \\times Treat_i\\times \\textbf{1}[t-t^*_0\\geq0] + \\gamma_t + \\lambda_i + \\varepsilon_{it}\n\nwhere the outcome variable, P_{it}, denotes the price of service station i in year-month t. The treatment variable, Treat_i, takes the value of 1 if station i is located within the 5 kilometers of radius distance towards the target firm. The post-period indicator variable, \\textbf{1}[t-t^*_0\\geq0], equals 1 if period t belongs to the post-merger period (i.e., after January 2018) whose starting period is $t^*_0 $ , and 0 otherwise. Year-month and unit fixed effects are captured by \\gamma_t and \\lambda_i, respectively. The coefficient of interest is \\beta, which measures the average treatment effect of the merger. It identifies the additional variation in prices experienced by the stations in overlap areas compared to the control stations after the merger took place.\nUnder certain assumptions, parameter \\beta capture the causal effect of the treatment on the outcome. In our context, these effects are primarily identified by comparing units in the treated areas and units in no-treated areas, before and after the merger phase. The crucial identifying assumption is the so-called parallel trends assumption of a DiD model: that absent the merger, the treated service stations would have experienced the same outcome trend as the control stations.\nIn addition, I also report estimates from a more flexible specification that allows the coefficient to vary by the relative periods after the merger, by estimating the following equation:\n\nP_{it} = \\sum_{h=T_1}^{T_0}(\\beta_h \\times Treat_i\\times \\textbf{1}[t-t^*_s\\geq0]) + \\gamma_t + \\lambda_i + \\varepsilon_{it}\n\nwhere T_0 and T_1 are the lowest and highest number of lags and leads, respectively, to consider surrounding the treatment period. Because I normalize the coefficient on the periods just prior to the merger announcement to zero (i.e., \\beta_{-1}=0), each coefficient of \\beta_h can be interpreted as the price change in treated stations relative to no-treated stations after h periods of the merger, with all of the \\beta_h’s being estimated relative to the omitted year (i.e., h=-1).\nUsing this flexible model has two advantages. First, I can visually test the key identifying assumption, which is the parallel-trends assumption: abstent the acquisition, the prices between overlap and non-overlap areas would have evolved in parallel. Although this assumption is fundamentally untestable, plotting the \\beta_h’s of pre-periods (i.e., \\beta_{-8} to \\beta_{-2}) can provide visual evidence. Second, this event-study allows the assessment of the time-evolving effects of the merger. This flexible specification enables me to capture such time-varying effects."
  },
  {
    "objectID": "posts/gasoline_merger_did/merger-report.html#two-way-fixed-effects-results",
    "href": "posts/gasoline_merger_did/merger-report.html#two-way-fixed-effects-results",
    "title": "Retail Gasoline Merger in Chile: An Ex-Post Merger Evaluation",
    "section": "5.1. Two-Way Fixed Effects Results",
    "text": "5.1. Two-Way Fixed Effects Results\nTable 4, Table 5 and Table 6 report the baseline results from estimating the specification described in Equation 1 on Gasoline 93, 97 and Diesel prices, respectively:\n\n\nCode\nimport pyfixest as pf\n\n# Filter by Gasolina 93\ndf93 = df[(df['Combustible'] == 'Gasolina 93')]\n\n# TWFE Estimation\nfeols1 = pf.feols(\"Precio ~ treat*Post | Id + date\", data=df93, vcov={\"CRV1\": \"Id\"})\nfeols1.tidy().reset_index()\n\n\n\n\nTable 4: TWFE Results (Gasoline 93)\n\n\n\n\n            \n            \n            \n\n\n\n\n\n\n\n\n\n\nCoefficient\nEstimate\nStd. Error\nt value\nPr(&gt;|t|)\n2.5%\n97.5%\n\n\n\n\n0\ntreat:Post\n5.747088\n0.595788\n9.646201\n1.998401e-15\n4.563085\n6.931091\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nimport pyfixest as pf\n\n# Filter by Gasolina 97\ndf97 = df[(df['Combustible'] == 'Gasolina 97')]\n\n# TWFE Estimation\nfeols1 = pf.feols(\"Precio ~ treat*Post | Id + date\", data=df97, vcov={\"CRV1\": \"Id\"})\nfeols1.tidy().reset_index()\n\n\n\n\nTable 5: TWFE Results (Gasoline 97)\n\n\n\n\n\n\n\n\n\n\n\nCoefficient\nEstimate\nStd. Error\nt value\nPr(&gt;|t|)\n2.5%\n97.5%\n\n\n\n\n0\ntreat:Post\n5.689687\n0.62263\n9.138146\n2.131628e-14\n4.45234\n6.927033\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nimport pyfixest as pf\n\n# Filter by Diesel\ndfdiesel = df[(df['Combustible'] == 'Petroleo Diesel')]\n\n# TWFE Estimation\nfeols1 = pf.feols(\"Precio ~ treat*Post | Id + date\", data=dfdiesel, vcov={\"CRV1\": \"Id\"})\nfeols1.tidy().reset_index()\n\n\n\n\nTable 6: TWFE Results (Diesel)\n\n\n\n\n\n\n\n\n\n\n\nCoefficient\nEstimate\nStd. Error\nt value\nPr(&gt;|t|)\n2.5%\n97.5%\n\n\n\n\n0\ntreat:Post\n2.496932\n0.971307\n2.570692\n0.01183\n0.566663\n4.427201\n\n\n\n\n\n\n\n\n\n\n\nThe estimated average effect on all these cases suggests that the merger caused a statistically significant average increase in price in overlap areas by around $5-6 Chilean pesos for the Gasoline 93 and 97 and around $2-3 for Diesel. These price effects are all below 1%."
  },
  {
    "objectID": "posts/gasoline_merger_did/merger-report.html#event-study-results",
    "href": "posts/gasoline_merger_did/merger-report.html#event-study-results",
    "title": "Retail Gasoline Merger in Chile: An Ex-Post Merger Evaluation",
    "section": "5.2. Event-Study Results",
    "text": "5.2. Event-Study Results\nI now estimate the event-study specification outlined in Equation 2 with the period before the merger as the reference period. Figure 4, Figure 5 and Figure 6 shows the the estimated coefficients (and the corresponding 95% confidence intervals) after controlling for observed characteristics of the TWFE model.\n\n\n\n            \n            \n            \n\n\n\n\nCode\ndf93 = df[(df['Combustible'] == 'Gasolina 93')]\n\nfeols_event_study = pf.feols(\n    \"Precio ~ i(event_date, treat, ref=-1) | Id + event_date\",\n    data=df93,\n    vcov={\"CRV1\": \"Id\"}\n)\n\n\nfeols_event_study.iplot(\n    coord_flip=False,\n    #title=\"TWFE-Estimator\",\n    figsize=[900, 400],\n    xintercept=22.5,\n    yintercept=0\n) + theme(legend_position='none') + scale_color_viridis(option=\"magma\", end=0.5)\n\n\n\n\n   \n   \n\n\nFigure 4: Event Study Results - Gasoline 93\n\n\n\n\n\n\nCode\ndf97 = df[(df['Combustible'] == 'Gasolina 97')]\n\nfeols_event_study = pf.feols(\n    \"Precio ~ i(event_date, treat, ref=-1) | Id + event_date\",\n    data=df97,\n    vcov={\"CRV1\": \"Id\"}\n)\n\nfeols_event_study.iplot(\n    coord_flip=False,\n    title=\"TWFE-Estimator\",\n    figsize=[900, 400],\n    xintercept=22.5,\n    yintercept=0,\n) + theme(legend_position='none') + scale_color_viridis(option=\"magma\")\n\n\n\n\n   \n   \n\n\nFigure 5: Event Study Results - Gasoline 97\n\n\n\n\n\n\nCode\ndfdiesel = df[(df['Combustible'] == 'Petroleo Diesel')]\n\nfeols_event_study = pf.feols(\n    \"Precio ~ i(event_date, treat, ref=-1) | Id + event_date\",\n    data=dfdiesel,\n    vcov={\"CRV1\": \"Id\"}\n)\n\nfeols_event_study.iplot(\n    coord_flip=False,\n    #title=\"TWFE-Estimator\",\n    figsize=[900, 400],\n    xintercept=22.5,\n    yintercept=0,\n) + theme(legend_position='none') + scale_color_viridis(option=\"magma\", begin=0.7)\n\n\n\n\n   \n   \n\n\nFigure 6: Event Study Results - Diesel\n\n\n\n\nThe visual evidence support that the parallel trend assumption holds, which is the main identifying assumption to interpret the effects as causal. In all models, the estimated coefficients for the pre-treatment period are close to zero and statistically insignificant. After the merger, I found that service station prices increased significantly, confirming my baseline regression results.\nIt’s worth noting that the results for Gasoline 93 are particularly clear in Figure 4, with treatment effects exhibiting persistence or stability post-merger. Conversely, in the case of Gasoline 97, the treatment effects tend to diminish after approximately 20 months. Similar downward trends are observed for diesel prices."
  },
  {
    "objectID": "posts/ab_test_bayesian/bayesian_ab_test.html",
    "href": "posts/ab_test_bayesian/bayesian_ab_test.html",
    "title": "Bayesian Approach to A/B Testing",
    "section": "",
    "text": "Bayesian A/B testing Analysis using Python.\nAs data scientists, we are frequently confronted with questions like: “Does X affects Y?” Here, Y is the outcome that we care about, while X could be a new feature, product, policy or experience. How do we go about answering such causal questions? For online platforms, the answer lies in experimentation.\nBy randomly assigning some kind of treatment (new interface design, new product, etc.) to a subset of users (Group A) and comparing their behavior or outcomes (such as revenue, visits, clicks, etc.) to those who did not receive the treatment (Group B), we can effectively isolate the causal impact of the change. This is at the core of A/B testing (also known as Randomized Control Trials), which is considered the workhorse method of experimentation for platforms like Spotify, Uber, Netflix, an more.\nIn my previous article, we learned the basics of A/B testing using Python, employing traditional traditional methods. However, one distinguishing feature of platforms like Spotify is their execution of thousands of simultaneous tests. With so many experiments available, is there a way to leverage information from previous tests and improve the inferences we make?\nIn this article, I aim to address this question by introducing the Bayesian approach to A/B testing. The Bayesian framework proves to be highly suitable for this task as it inherently facilitates the integration of prior knowledge with new data, enabling more robust conclusions. Let’s see how!"
  },
  {
    "objectID": "posts/ab_test_bayesian/bayesian_ab_test.html#classical-approaches-normal-distribution-vs-nonparametric-bootstrap",
    "href": "posts/ab_test_bayesian/bayesian_ab_test.html#classical-approaches-normal-distribution-vs-nonparametric-bootstrap",
    "title": "Bayesian Approach to A/B Testing",
    "section": "2.1. Classical Approaches: Normal Distribution vs Nonparametric Bootstrap",
    "text": "2.1. Classical Approaches: Normal Distribution vs Nonparametric Bootstrap\nWe want to know the uncertainty of the response rates p. One standard way to measure uncertainty of the possible values of the response rate is the classical framework of inference, which is to assume that p comes from a normal distribution.\nWe start by calculating the mean and standard deviation. Then, we’ll use those parameters to plot confidence intervals (CI) and get a sense of the lay of the land:\n\n\\text{CI}_{\\text{lower}} = \\bar{x} - 1.96 \\times \\frac{\\text{SE}_{\\bar{x}}}{\\sqrt{n}}\n\nIt’s classic statistical inference!\n\n# Calculate the mean\nxbar = df['respmail'].mean()\nprint(\"Sample Response Rate:\", xbar.round(3))\n\n# Calculate the standard error of the mean (SE)\nxbse = np.sqrt(df['respmail'].var() / len(df))\n\n# Calculate the confidence interval\nci_lower = xbar - 1.96 * xbse\nci_upper = xbar + 1.96 * xbse\n\nprint(\"95% Confidence Interval:\")\nprint(\"Lower bound:\", ci_lower.round(3))\nprint(\"Upper bound:\", ci_upper.round(3))\n\nSample Response Rate: 0.124\n95% Confidence Interval:\nLower bound: 0.118\nUpper bound: 0.131\n\n\nOur estimated response rate for the catalog campaign falls within a confidence interval ranging from 11.8% to 13.1% with 95% confidence. This means that, assuming normality, we’re pretty confident that the true response rate lies somewhere in this range. Now, let’s visualize these bounds:\n\n\nCode\n# Generate xx values\nxx = np.linspace(df['respmail'].min(), df['respmail'].max(), 10000)\n\n# Calculate normal density\nnorm_density = pd.DataFrame({'xx': xx, 'm': np.exp(-(xx - xbar) ** 2 / (2 * xbse ** 2)) / (xbse * np.sqrt(2 * np.pi))})\n\n# Plot using Seaborn with confidence interval\nplt.figure(figsize=(8, 6))\nsns.lineplot(data=norm_density, x='xx', y='m', color=\"#00A087B2\")\nplt.axvline(xbar, color=\"black\", linestyle=\"--\", label=\"Mean\")\nplt.axvline(ci_lower, color=\"#481f70\", linestyle=\"--\", label=\"95% CI Lower Bound\")\nplt.axvline(ci_upper, color=\"#481f70\", linestyle=\"--\", label=\"95% CI Upper Bound\")\nplt.xlim(0.108, 0.14)  # Limit x-axis range\nplt.xlabel(\"Response Rate (p)\")\nplt.ylabel(\"Density\")\nplt.title(\"Normal Distribution\")\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\nIf we don’t want to make any distributional assumptions, we can give the nonparametric bootstrap method a shot. This approach allows us to leverage bootstrap sampling techniques for robust estimates of response rates. Here is the code to do re-sampling with replacement:\n\n# Define the number of bootstrap samples\nB = 10000\n\n# Define a function to calculate response rate\ndef calculate_response_rate(data):\n    return data.mean()\n\n# Initialize an array to store bootstrap sample response rates\nbootstrap_response_rates = np.zeros(B)\n\n# Generate bootstrap samples and calculate response rates\nfor i in range(B):\n    bootstrap_sample = df['respmail'].sample(frac=1, replace=True)  # Sampling with replacement\n    bootstrap_response_rates[i] = calculate_response_rate(bootstrap_sample)\n\n# Calculate mean and standard error of bootstrap response rates\nbootstrap_mean = bootstrap_response_rates.mean()\n\n# Calculate 95% confidence interval\nconfidence_interval = np.percentile(bootstrap_response_rates, [2.5, 97.5])\nprint(\"Bootstrap Mean:\", bootstrap_mean.round(3))\nprint(\"95% Confidence Interval:\", confidence_interval.round(3))\n\nBootstrap Mean: 0.124\n95% Confidence Interval: [0.115 0.134]\n\n\n\n\nCode\n# Plot using Seaborn\nplt.figure(figsize=(8, 6))\nsns.histplot(bootstrap_response_rates, bins=100, kde=True, color=\"#00A087B2\", edgecolor=\"black\", fill=True)\nplt.axvline(np.mean(bootstrap_response_rates), color=\"black\", linestyle=\"--\", linewidth=1.5)\nplt.axvline(ci_lower, color=\"#481f70\", linestyle=\"--\", label=\"95% CI Lower Bound\")\nplt.axvline(ci_upper, color=\"#481f70\", linestyle=\"--\", label=\"95% CI Upper Bound\")\nplt.xlabel(\"Response Rate (p)\")\nplt.ylabel(\"Density\")\nplt.title(\"Bootstrap Distribution\")\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\nUpon running the resampling, we obtain a 95% confidence interval of [0.115, 0.134]. Interestingly, we notice that the uncertainty level is slightly higher in the nonparametric case (more uncertainty). This is a common trade-off when opting for fewer assumptions in our analysis."
  },
  {
    "objectID": "posts/ab_test_bayesian/bayesian_ab_test.html#bayesian-approach",
    "href": "posts/ab_test_bayesian/bayesian_ab_test.html#bayesian-approach",
    "title": "Bayesian Approach to A/B Testing",
    "section": "2.3. Bayesian Approach",
    "text": "2.3. Bayesian Approach\nSo far, everything has been very standard. But let’s consider this: what if our catalog campaign isn’t the only experiment we’ve conducted to boost our browser (and ultimately ad revenue)? Among the thousands of experiments we’ve run in the past, this campaign is just one idea in the mix. Can we efficiently leverage this wealth of additional information?\nThis is how Bayesian statistics offers a powerful advantage over the frequentist approach: the seamless integration of additional information into our model. The idea directly follows from the main results behind all Bayesian statistics: Bayes Theorem. By inverting the inference problem, Bayes’ Theorem allows us to transition from the probability of the model given the data to the probability of the data given the model:\n\n\\underbrace{ \\Pr \\big( \\text{model} \\ \\big| \\ \\text{data} \\big) }_{\\text{posterior}} = \\underbrace{ \\Pr(\\text{model}) }_{\\text{prior}} \\ \\underbrace{ \\frac{ \\Pr \\big( \\text{data} \\ \\big| \\ \\text{model} \\big) }{ \\Pr(\\text{data}) } }_{\\text{likelihood}}\n\nWe can split the right-hand side of Bayes Theorem (or Rule) into two components: the prior and the likelihood. The likelihood is the information about the model that comes from the data, the prior instead is any additional information about the model.\n\nPriors\nAs the name suggests, priors contain information that was available even before looking at the data. This leads to one of the most relevant questions in Bayesian statistics: How do you choose a prior? In practice, there are several considerations to keep in mind:\n\nBeta Distribution Assumption: Under the Bayesian framework, we assume that the true conversion rates follow a Beta distribution. This particular distribution is chosen for its flexibility and ability to capture uncertainty.\nUninformative or Weakly Informative Priors: these priors are characterized by setting low values for alpha and beta. For example, alpha = 1, beta = 1 leads to a uniform distribution as a prior. If we were considering one distribution in isolation, setting this prior is a statement that we don’t know anything about the value of the parameter, nor our confidence around it.\nStrong Priors: Conversely, strong priors are characterized by high values for alpha and beta, implying that the relative uplift distribution is thin, i.e. our prior belief is that the variants are not very different from each other.\n\nGoing back to our emailing campaign, let’s assume our response rate p comes from a beta distribution. The mean of the beta distribution is:\n\n\\text{E}[p|a,b] = \\frac{a}{a+b}\n\nDrawing insights from past experiments, we’ve observed that p tends to hover around 12.3%. To reflect this knowledge, we set the parameters a and b of the beta distribution accordingly: a = 9 and b = 64.\n\nprior_a = 9\nprior_b = 64\n\n# Mean\np_hat = prior_a / (prior_a + prior_b)\nprint(round(p_hat,3))\n\n0.123\n\n\n\n\nCode\n# Create DataFrame with samples\nB = 10000  # number draws from distribution\nsamples = np.random.beta(prior_a, prior_b, size=B)\nbeta_df = pd.DataFrame({'Samples': samples})\n\n# Plot using Seaborn\nplt.figure(figsize=(8, 6))\nsns.histplot(beta_df, bins=100, kde=True, color=\"#00A087B2\", edgecolor=\"black\", fill=True)\nplt.axvline(p_hat, color=\"black\", linestyle=\"--\", linewidth=1.5)\nplt.xlabel(\"Response Rate (p)\")\nplt.ylabel(\"Density\")\nplt.title(\"Prior Distribution\")\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\nIt’s worth noting that the beta distribution exhibits a slight leftward skewness, suggesting that lower response rates may be more likely than higher response rates, which is a common pattern in scenarios where there are factors such as limited customer engagement or market saturation.\n\n\nPosterior\nBayes rule tells you how you should update your beliefs after you see some data:\n \\textrm{posterior} \\propto \\textrm{likelihood} \\times \\textrm{prior} \nNow, we already assume some prior and now we run the experiment and we see our data results. With this, we can update our prior belief by calculating the posterior. Our observed experiment had the following characteristics:\n\n\nCode\nprint('Number Emails sent:', df.mailing.sum() )\nprint('Number of responses:', df[df['mailing'] == 1].respmail.sum() )\nprint('Response Rate', df[df['mailing'] == 1].respmail.mean().round(4)*100 )\n\n\nNumber Emails sent: 4952\nNumber of responses: 616.0\nResponse Rate 12.44\n\n\nIf we observe s responses in n observations, we can update our data by using the following equations:\n\nf(p)=p^{a+x-1}(1-p)^{b+n-s-1}\\approx\\text{beta}[a+s,b+n-s]\n\n\n# Define parameters\nB = 10000\nn = df.mailing.sum()                       # number in test sample\ns = df[df['mailing'] == 1].respmail.sum()  # number of responses\n\npost_a = prior_a + s\npost_b = prior_b + n - s\n\n\nfrom scipy.stats import beta\n\n# Calculate mean posterior response rate\np_hat1 = post_a / (post_a + post_b)\n\n# Print mean response rate\nprint(\"The Posterior Mean response rate is\", round(p_hat1, 3))\n\n# Calculate prior and posterior confidence intervals\nprior_ci = beta.ppf([0.025, 0.975], prior_a, prior_b)\nposterior_ci = beta.ppf([0.025, 0.975], post_a, post_b)\n\n# Print confidence intervals\nprint(\"Prior Response Rate 95% CI:\", prior_ci.round(3))\nprint(\"Posterior Response Rate 95% CI:\", posterior_ci.round(3))\n\nThe Posterior Mean response rate is 0.124\nPrior Response Rate 95% CI: [0.059 0.207]\nPosterior Response Rate 95% CI: [0.115 0.134]\n\n\n\n\nCode\n# Generate posterior density samples\nposterior_density = np.random.beta(a=post_a, b=post_b, size=B)\n\n# Plot using Seaborn\nplt.figure(figsize=(8, 6))\nsns.histplot(posterior_density, bins=100, kde=True, color=\"#00A087B2\", edgecolor=\"black\", fill=True)\nplt.axvline(p_hat1, color=\"black\", linestyle=\"--\", linewidth=1.5)\nplt.xlabel(\"Response Rate (p)\")\nplt.ylabel(\"Density\")\nplt.title(\"Posterior Distribution\")\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\nThe Bayesian approach provides a posterior mean response rate of 0.124, along with a 95% confidence interval (CI) for the response rate ranging from 0.115 to 0.134. In this case, the standard approaches reported similar confidence intervals.\n\n\nBreakeven Point\nPlatforms often seek to determine whether the response rate from a campaign surpasses a minimum threshold necessary to cover the costs incurred. This minimum threshold is known as the breakeven point. For instance, let’s consider a scenario where the cost per mailing (c) amounts to $5.75, while the profits generated per response (m) stand at $50.\nWe can calculate the probability that the posterior is above the breakeven point, which in this case is:  p &gt;\\frac{c}{\\pi} \\equiv \\frac{5.75}{50} = 0.115 \nSo that we are interested in:  \\Pr(p&lt;0.115)  We estimate the probability by drawing from a beta distribution with above parameters.\n\nc = 5.75   # cost per mailing\nm = 50   # profit if respond\n\n# Calculate breakeven point\nbrk = c / m\nprint(\"Breakeven point:\", brk)\n\nBreakeven point: 0.115\n\n\n\n# Set random seed\nnp.random.seed(19312)\n\n# Generate random deviates\npost_draws = np.random.beta(post_a, post_b, size=B)\n\n# Calculate probability of passing breakeven point\nprob_passing = np.sum(post_draws &lt; brk) / B\n\n# Print probability\nprint(\"The Probability that the response rate is below the breakeven point is\", round(prob_passing, 3))\n\nThe Probability that the response rate is below the breakeven point is 0.02\n\n\nWith only a 2% probability of the response rate falling below the breakeven point, the results indicate a favorable outcome, indicating that the campaign’s effectiveness in generating responses is promising in relation to its profitability."
  },
  {
    "objectID": "posts/ab_test_bayesian/bayesian_ab_test.html#classical-approach-linear-regression",
    "href": "posts/ab_test_bayesian/bayesian_ab_test.html#classical-approach-linear-regression",
    "title": "Bayesian Approach to A/B Testing",
    "section": "3.1. Classical Approach: Linear Regression",
    "text": "3.1. Classical Approach: Linear Regression\nAs explored in my previous post on A/B Testing, we can quantify the impact of our catalog campaign using a standard linear regression model. In this case, I would like to know the impact of the campaign on Purchase Value M of customers on the E-beer store during a given period:\n\nY_{i} = \\alpha + \\beta D_i + \\varepsilon_i\n\nwhere D_i is the treatment indicator (equal to 1 for treated units and 0 for control units), \\alpha is the intercept term and \\beta is our coefficient of interest representing the Average Treatment Effect.\n\nimport statsmodels.formula.api as smf\n\nsmf.ols('M ~ mailing', data=df).fit().summary().tables[1]\n\n\n\nTable 3: A/B Regression Results I\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n92.2839\n1.066\n86.589\n0.000\n90.195\n94.373\n\n\nmailing\n2.9149\n1.512\n1.928\n0.054\n-0.049\n5.878\n\n\n\n\n\n\n\n\n\nAccording to Table 3, while the coefficient associated with the treatment variable (mailing) suggests a positive impact of the catalog campaign, these effects do not achieve statistical significance. The 95% Confidence Intervals for the coefficient range from -0.049 to 5.878, indicating substantial uncertainty surrounding the estimated treatment effect.\nCan we do better? Let’s leverage information from previous tests with the Bayesian framework."
  },
  {
    "objectID": "posts/ab_test_bayesian/bayesian_ab_test.html#bayesian-approach-uninformative-prior",
    "href": "posts/ab_test_bayesian/bayesian_ab_test.html#bayesian-approach-uninformative-prior",
    "title": "Bayesian Approach to A/B Testing",
    "section": "3.2. Bayesian Approach: Uninformative Prior",
    "text": "3.2. Bayesian Approach: Uninformative Prior\n\nPrior\nWhen choosing a prior, one analytically appealing restriction is to have a prior distribution such that the posterior belongs to the same family. For example, in the case of a continuous dependent variable (M), I can assume my treatment effect is normally distributed and I would like it to be normally distributed also after incorporating the information contained in the data.\nNow, before observing any data, I knew nothing about how much people might spend on beers online. They might spend 2 euros or they might stay for 200 euros. Formally, I can describe my weakly informative prior beliefs with a prior distribution:\n\\textrm{mean Purchase-Value by group} \\sim N(0, 100^2)\nHere is how the distribution looks like:\n\n\nCode\n# Prior parameters\nprior_mean = 0\nprior_sd = 100\n\n# Generate x-values for plotting\nxx = np.linspace(-300, 300, num=1000)\n\n# Calculate prior density\nprior_density = (1 / (prior_sd * np.sqrt(2 * np.pi))) * np.exp(-0.5 * ((xx - prior_mean) / prior_sd) ** 2)\n\n# Plot prior density using Seaborn\nplt.figure(figsize=(10, 6))\nsns.lineplot(x=xx, y=prior_density, color='#00A087B2')\nplt.axvline(prior_mean, color=\"black\", linestyle=\"--\", linewidth=1.5)\nplt.title('Prior Density')\nplt.xlabel('Mean Purchase Value (M)')\nplt.ylabel('Density')\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nPosterior\nThen Bayes rule tells us that the posterior distribution for mean purchase-value for each group should be:\n\n\\textrm{mean time-on-site (m)} \\sim \\mathcal{N}\\left(\\mu, \\sigma^2\\right)\nwhere\n\n\\sigma = \\left(\\frac{1}{\\sigma_{0}^2} + \\frac{n}{s^2}\\right)^{-1}\n\nand\n\n\\mu = \\sigma^2 \\left(\\frac{\\mu_0}{\\sigma_{0}^2} + \\frac{n \\bar{y}}{s^2}\\right)\n\n\n# Posterior parameters\nn_A = df.shape[0] - df.mailing.sum()\nn_B = df.mailing.sum()\ns = df.M.std()  # Standard deviation of data (approx==2)\nybar_A = df[df['mailing'] == 0].M.mean()\nybar_B = df[df['mailing'] == 1].M.mean()\n\n# Posterior standard deviation\nposterior_sd_A = (1 / (prior_sd ** 2) + n_A / (s ** 2)) ** (-0.5)\nposterior_sd_B = (1 / (prior_sd ** 2) + n_B / (s ** 2)) ** (-0.5)\n\n# Posterior mean\nposterior_mean_A = posterior_sd_A ** 2 * ((prior_mean / (prior_sd ** 2)) + (n_A * ybar_A / (s ** 2)))\nposterior_mean_B = posterior_sd_B ** 2 * ((prior_mean / (prior_sd ** 2)) + (n_B * ybar_B / (s ** 2)))\n\n\nprint(\"Posterior Mean Estimate for Group A:\", posterior_mean_A.round(1))\nprint(\"Posterior Mean Estimate for Group B:\", posterior_mean_B.round(1))\n\nPosterior Mean Estimate for Group A: 92.3\nPosterior Mean Estimate for Group B: 95.2\n\n\nWe can plot each posterior distribution to compare:\n\n\nCode\n# Plot posterior distributions\nxx = np.linspace(min(df['M']), max(df['M']), 1000)\nposterior_distribution_A = (1 / (posterior_sd_A * np.sqrt(2 * np.pi))) * np.exp(-0.5 * ((xx - posterior_mean_A) / posterior_sd_A) ** 2)\nposterior_distribution_B = (1 / (posterior_sd_B * np.sqrt(2 * np.pi))) * np.exp(-0.5 * ((xx - posterior_mean_B) / posterior_sd_B) ** 2)\n\nplt.figure(figsize=(10, 6))\nsns.lineplot(x=xx, y=posterior_distribution_A, label='Group A', color='#440154')\nsns.lineplot(x=xx, y=posterior_distribution_B, label='Group B', color='#00A087B2')\nplt.xlim(85, 102)  # Limit x-axis range\n\nplt.title('Posterior Distributions of Monetary Value (M)')\nplt.xlabel('Monetary Value (M)')\nplt.ylabel('Density')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\nExcellent! With the Bayesian approach, we’ve estimated the posterior mean purchase value for Group A to be $92.3 and for Group B to be $95.2. Once we have these distribution for the difference in the mean, we can compute the probability that the mean of B is greater than the mean of A and Confidence Intervals:\n\nfrom scipy.stats import norm\n\n# Calculate the difference in means\npost_mean_diff = posterior_mean_B - posterior_mean_A\nprint(\"Posterior Average Treatment Effect:\", post_mean_diff.round(3))\n\n# Calculate the standard deviation of the difference\npost_sd_diff = np.sqrt(posterior_sd_B ** 2 + posterior_sd_A ** 2)\n\n# Compute the probability\nprob = 1 - norm.cdf(0, loc=post_mean_diff, scale=post_sd_diff)\nprint(\"Probability that mean of B is greater than mean of A:\", prob.round(3))\n\n# Calculate the 95% confidence interval\nci_lower = norm.ppf(0.025, loc=post_mean_diff, scale=post_sd_diff)\nci_upper = norm.ppf(0.975, loc=post_mean_diff, scale=post_sd_diff)\nprint(\"95% Confidence Interval for the Difference in Means:\", (ci_lower.round(3), ci_upper.round(3)))\n\nPosterior Average Treatment Effect: 2.914\nProbability that mean of B is greater than mean of A: 0.973\n95% Confidence Interval for the Difference in Means: (-0.049, 5.878)\n\n\nOn average, the treatment (catalog campaign) leads to an increase in purchase value by approximately $2.914 compared to the control group. The confidence interval for the difference in means ranges from -0.049 to 5.878. Since this interval includes zero, we cannot conclude with 95% confidence that there is a statistically significant difference in purchase values between the treated and control groups.\nThis suggests that the catalog campaign may not have a significant impact on purchase behavior compared to the control group, based on the observed data.\n\n\nCode\n# Generate x-values for plotting\nxx_diff = np.linspace(-2, 7, num=100)\n\n# Calculate the posterior density of the difference\npost_density_diff = norm.pdf(xx_diff, loc=post_mean_diff, scale=post_sd_diff)\n\n# Plot the posterior distribution of the difference using Seaborn\nplt.figure(figsize=(10, 6))\nsns.lineplot(x=xx_diff, y=post_density_diff, color='#00A087B2')\nplt.title('Posterior Distribution of Difference in Means (B - A)')\nplt.axvline(x=0, color='black', linestyle='--', label='Zero difference')\nplt.xlabel('Difference in Means')\nplt.ylabel('Density')\nplt.grid(True)\n\n# Shade the area to the right of 0 to calculate the probability\nplt.fill_between(xx_diff, post_density_diff, where=(xx_diff &gt;= 0), color='#00A087B2', alpha=0.2)\n\nplt.show()"
  },
  {
    "objectID": "posts/ab_test_bayesian/bayesian_ab_test.html#bayesian-approach-student-t-prior",
    "href": "posts/ab_test_bayesian/bayesian_ab_test.html#bayesian-approach-student-t-prior",
    "title": "Bayesian Approach to A/B Testing",
    "section": "3.3. Bayesian Approach: Student-t Prior",
    "text": "3.3. Bayesian Approach: Student-t Prior\nWhat if we have a more informative prior? What if past experiments tell us that the distribution is actually more similar to a t-student distribution, for example?\nWhile we’ve previously assumed a normal distribution for our prior, which is appropriate for continuous data with a large sample size, we can explore also the t-student distribution, which is more robust to outliers and has heavier tails compared to the normal distribution. This is a setting that happens often in the industry.\n\nfrom scipy.stats import t\n\n# Assuming you have a DataFrame df with columns 'mailing' and 'M'\n# Filter the data for group A and group B\ngroup_A = df[df['mailing'] == 0]['M']\ngroup_B = df[df['mailing'] == 1]['M']\n\n# Prior parameters for Student's t-distribution\nprior_df = 1.3  # degrees of freedom\nprior_loc = 0  # mean\nprior_scale = 100  # scale (similar to standard deviation)\n\n# Likelihood function (assuming a normal likelihood)\ndef likelihood(data, mean):\n    sd = data.std()  # Standard deviation from the data\n    return np.prod(1 / (sd * np.sqrt(2 * np.pi)) * np.exp(-0.5 * ((data - mean) / sd) ** 2))\n\n# Posterior parameters for group A\nposterior_mean_A = group_A.mean()\nposterior_sd_A = np.sqrt(prior_scale ** 2 / (prior_scale ** 2 + (group_A.std() ** 2) / len(group_A)))\n\n# Posterior parameters for group B\nposterior_mean_B = group_B.mean()\nposterior_sd_B = np.sqrt(prior_scale ** 2 / (prior_scale ** 2 + (group_B.std() ** 2) / len(group_B)))\n\n\n\nCode\nfrom scipy.stats import norm\n\n# Prior parameters\nprior_mean = 0\nprior_sd = 100\n\n# Generate x-values for plotting\nxx = np.linspace(-300, 300, num=1000)\n\n# Calculate prior density\nprior_density = (1 / (prior_sd * np.sqrt(2 * np.pi))) * np.exp(-0.5 * ((xx - prior_mean) / prior_sd) ** 2)\n\n# Calculate the difference in means\npost_mean_diff = posterior_mean_B - posterior_mean_A\n\n# Calculate the standard deviation of the difference\npost_sd_diff = np.sqrt(posterior_sd_B ** 2 + posterior_sd_A ** 2)\n\n# Generate x-values for plotting\nxx_diff = np.linspace(-2, 7, num=100)\n\n# Calculate the posterior density of the difference using Normal prior\npost_density_diff_normal = norm.pdf(xx_diff, loc=post_mean_diff, scale=post_sd_diff)\n\n# Plot prior density and posterior distribution of the difference using Seaborn\nplt.figure(figsize=(10, 6))\n\n# Plot prior density\nsns.lineplot(x=xx, y=prior_density, color='#00A087B2', label='Normal Prior ')\nplt.axvline(prior_mean, color=\"black\", linestyle=\"--\", linewidth=1.5)\n\n# Plot posterior distribution of the difference\nsns.lineplot(x=xx_diff, y=post_density_diff_normal, color='#440154', label='t-Student Prior')\n\nplt.title('Comparison of Distributions')\nplt.xlabel('Mean Purchase Value (M)')\nplt.ylabel('Density')\nplt.grid(True)\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\nAs we can see from this plot, all distributions are centered on zero, but they have very different shapes. Let’s estimate our Confidence Intervals:\n\nprint(\"Posterior Average Treatment Effect:\", post_mean_diff.round(3))\n\n# Calculate the 95% confidence interval using the t-distribution\nci_lower = t.ppf(0.025, df=(len(group_A) + len(group_B)) - 2, loc=post_mean_diff, scale=post_sd_diff)\nci_upper = t.ppf(0.975, df=(len(group_A) + len(group_B)) - 2, loc=post_mean_diff, scale=post_sd_diff)\nprint(\"95% Confidence Interval for the Difference in Means:\", (ci_lower.round(3), ci_upper.round(3)))\n\nPosterior Average Treatment Effect: 2.915\n95% Confidence Interval for the Difference in Means: (0.143, 5.687)\n\n\n\n\nCode\nfrom scipy.stats import t\n\n# Generate x-values for plotting\nxx_diff = np.linspace(-2, 7, num=100)\n\n# Calculate the posterior density of the difference using Student's t-distribution\npost_density_diff = t.pdf(xx_diff, df=(len(group_A) + len(group_B)) - 2, loc=post_mean_diff, scale=post_sd_diff)\n\n\n# Plot the posterior distribution of the difference using Seaborn\nplt.figure(figsize=(10, 6))\nsns.lineplot(x=xx_diff, y=post_density_diff, color='#00A087B2')\nplt.title('Posterior Distribution of Difference in Means (B - A) with Student-t Prior')\nplt.xlabel('Difference in Means')\nplt.ylabel('Density')\nplt.grid(True)\n\n# Shade the area to the right of 0 to calculate the probability\nplt.fill_between(xx_diff, post_density_diff, where=(xx_diff &gt;= 0), color='#00A087B2', alpha=0.2)\n\n# Add a vertical line at the point where the difference in means is zero\nplt.axvline(x=0, color='black', linestyle='--', label='Zero difference')\n\n# Add confidence interval lines\nplt.axvline(x=ci_lower, color='blue', linestyle='--', label='95% CI')\nplt.axvline(x=ci_upper, color='blue', linestyle='--')\n\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\nAt first glance, the Posterior Average Treatment Effect (ATE) appears to be similar to our previous findings. But now, these results are statistically significant. Unlike our previous analysis, the 95% Confidence Interval for the Difference in Means now ranges from 0.143 to 5.687. The change in significance might be attributed to the adoption of a more informative prior distribution, which has provided greater precision in our estimates."
  },
  {
    "objectID": "myposts.html",
    "href": "myposts.html",
    "title": "Daniel Redel",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n\n\nCausal Inference\n\n\nR\n\n\nTime Series\n\n\nARIMA\n\n\n\n\n\n\n\nDaniel Redel\n\n\nMay 22, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBayesian Statistics\n\n\nCausal Inference\n\n\nA/B Testing\n\n\nPython\n\n\n\n\n\n\n\nDaniel Redel\n\n\nMay 10, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCompetition & Antitrust\n\n\nR\n\n\nBinary Choice Models\n\n\nSimulation\n\n\n\n\n\n\n\nDaniel Redel\n\n\nMay 9, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCausal Inference\n\n\nCompetition & Antitrust\n\n\nDiff-in-Diff\n\n\nEconometrics\n\n\nSpatial Analysis\n\n\nGasoline Industry\n\n\n\n\n\n\n\nDaniel Redel\n\n\nApr 22, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCausal Inference\n\n\nA/B Testing\n\n\nPython\n\n\n\n\n\n\n\nDaniel Redel\n\n\nApr 5, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNonparametric Regression\n\n\nSemiparametric Regression\n\n\nBinary Choice Models\n\n\nKernel Density Estimation\n\n\n\n\n\n\n\nDaniel Redel\n\n\nDec 11, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npython\n\n\ncode\n\n\nanalysis\n\n\n\nMy first Jupyter Notebook\n\n\n\nDaniel Redel\n\n\nMay 22, 2021\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "myposts.html#my-posts",
    "href": "myposts.html#my-posts",
    "title": "Daniel Redel",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n\n\nCausal Inference\n\n\nR\n\n\nTime Series\n\n\nARIMA\n\n\n\n\n\n\n\nDaniel Redel\n\n\nMay 22, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBayesian Statistics\n\n\nCausal Inference\n\n\nA/B Testing\n\n\nPython\n\n\n\n\n\n\n\nDaniel Redel\n\n\nMay 10, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCompetition & Antitrust\n\n\nR\n\n\nBinary Choice Models\n\n\nSimulation\n\n\n\n\n\n\n\nDaniel Redel\n\n\nMay 9, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCausal Inference\n\n\nCompetition & Antitrust\n\n\nDiff-in-Diff\n\n\nEconometrics\n\n\nSpatial Analysis\n\n\nGasoline Industry\n\n\n\n\n\n\n\nDaniel Redel\n\n\nApr 22, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCausal Inference\n\n\nA/B Testing\n\n\nPython\n\n\n\n\n\n\n\nDaniel Redel\n\n\nApr 5, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNonparametric Regression\n\n\nSemiparametric Regression\n\n\nBinary Choice Models\n\n\nKernel Density Estimation\n\n\n\n\n\n\n\nDaniel Redel\n\n\nDec 11, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npython\n\n\ncode\n\n\nanalysis\n\n\n\nMy first Jupyter Notebook\n\n\n\nDaniel Redel\n\n\nMay 22, 2021\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "I am passionate about the intersection between Economics and Data Science. Experienced in Competition Economics and Quantitative Research. Currently working as a Junior Data Consultant at FactX, where I engage in diverse projects across different industries. Oriented toward analytical and interdisciplinary work, with high learning opportunities.\n \n  \n   \n  \n    \n     Twitter\n  \n  \n    \n     LinkedIn\n  \n  \n    \n     Github"
  },
  {
    "objectID": "about.html#skills",
    "href": "about.html#skills",
    "title": "About Me",
    "section": "Skills",
    "text": "Skills"
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "About Me",
    "section": "Education",
    "text": "Education\n\n\nMSc. in Econometrics & Mathematical Economics\nTilburg University\n\n\nJan 2024\n\n\n\n\nMSc. in Applied Economics\nPUC Chile\n\n\n2021\n\n\n\n\nBSc. in Economics & Business Administration\nPUC Chile\n\n\n2020"
  },
  {
    "objectID": "about.html#experience",
    "href": "about.html#experience",
    "title": "About Me",
    "section": "Experience",
    "text": "Experience\n\n\nJunior Data Consultant\nFactX, Utrecht\n\n\nMarch 2024 – present\n\n\n\n\nThesis Internship\nCPB Netherlands Bureau for Economic Policy Analysis, The Hague\n\n\nMay 2023 - Oct 2023\n\n\n\n\nResearch Analyst\nCentroCompetencia, Santiago\n\n\nJul 2021 - Apr 2023"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "👋 Hey, I’m Daniel",
    "section": "",
    "text": "I am passionate about the intersection between Economics and Data Science. Experienced in Competition Economics and Quantitative Research. Currently working as a Junior Data Consultant at FactX, where I engage in diverse projects across different industries. Oriented toward analytical and interdisciplinary work, with high learning opportunities.\n \n  \n   \n  \n    \n     Twitter\n  \n  \n    \n     LinkedIn\n  \n  \n    \n     Github"
  },
  {
    "objectID": "index.html#latest-posts",
    "href": "index.html#latest-posts",
    "title": "👋 Hey, I’m Daniel",
    "section": "Latest Posts",
    "text": "Latest Posts\nClick here to check out more posts.\n\n\n\n\n\n\n\n\n\n\nCausal ARIMA Approach to Estimate Price Policy Changes on Sales\n\n\n\n\n\n\nDaniel Redel\n\n\nMay 22, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBayesian Approach to A/B Testing\n\n\n\n\n\n\nDaniel Redel\n\n\nMay 10, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMerger Simulation of Broadband Internet Services\n\n\n\n\n\n\nDaniel Redel\n\n\nMay 9, 2024\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/ab_testing1/1_ab_testing.html",
    "href": "posts/ab_testing1/1_ab_testing.html",
    "title": "A/B Testing Analysis",
    "section": "",
    "text": "A/B testing Analysis using Python.\nAs data scientists, we are frequently confronted with questions like: “Does X affects Y?” Here, Y is the outcome that we care about, while X could be a new feature, product, policy or experience. For example, a website owner would like to ask if a new web page design leads to a higher click-through rate or sale. Similarly, Spotify may to know whether their new interface leads to more minutes of music played, while Zalando aims to assess the impact of their marketing actions on purchases.\nBut how do we go about answering such causal questions? For the Platform Economy, the solution lies in experimentation. By randomly assigning some kind of treatment (new interface design, new product, etc.) to a subset of users (Group A) and comparing their behavior or outcomes (such as revenue, visits, clicks, etc.) to those who did not receive the treatment (Group B), we can effectively isolate the causal impact of the change. This is at the core of A/B testing (also known as Randomized Control Trials), which is considered the workhorse method of experimentation for platforms like Spotify, Uber, Netflix, an more.\nIn this article, we will delve into the analysis involved in A/B testing using Python, with a specific focus on Randomization Checks and various methods or statistical tools for studying Average Treatment Effects. Let’s go!"
  },
  {
    "objectID": "posts/ab_testing1/1_ab_testing.html#violinplots",
    "href": "posts/ab_testing1/1_ab_testing.html#violinplots",
    "title": "A/B Testing Analysis",
    "section": "1.1. Violinplots",
    "text": "1.1. Violinplots\nA first visual approach is the violinplot. The violinplot plots depicts distributions of numeric data for one or more groups using density curves. These densities —usually smoothed by a kernel density estimator— are displayed along the y-axis so that we can compare them. By default, the .violintplot() function from the seaborn library also adds a miniature boxplot inside. Figure 1 shows the violinplots of our baseline variables:\n\n\nCode\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n\n# Create subplots\nfig, axs = plt.subplots(3, 3, figsize=(15, 12))\n\n# Plot histogram for 'days_since'\nfiltered_df = df[df['days_since'] &gt; 0]\nsns.violinplot(data=filtered_df, x='group', y='days_since', palette='viridis', ax=axs[0, 0])\naxs[0, 0].set_xlabel('Days Since Last Activity')\naxs[0, 0].set_ylabel('Customers')\naxs[0, 0].set_title('Violinplot of Days Since')\n\n# Plot histogram for 'visits'\nfiltered_df = df[df['visits'] &gt; 0]\nsns.violinplot(data=filtered_df, x='group', y='visits', palette='viridis', ax=axs[0, 1])\naxs[0, 1].set_xlabel('Visits')\naxs[0, 1].set_ylabel('Customers')\naxs[0, 1].set_title('Violinplot of Website Visits')\n\n# Plot histogram for 'past_purch'\nfiltered_df = df[(df['past_purch'] &gt; 0) & (df['past_purch'] &lt; 2000)]\nsns.violinplot(data=filtered_df, x='group', y='past_purch', palette='viridis', ax=axs[0, 2])\naxs[0, 2].set_xlabel('Past Purchases ($)')\naxs[0, 2].set_ylabel('Customers')\naxs[0, 2].set_title('Violinplot of Past Purchases')\n\n# Plot histogram for 'chard'\nfiltered_df = df[(df['chard'] &gt; 0) & (df['chard'] &lt; 2000)]\nsns.violinplot(data=filtered_df, x='group', y='chard', palette='viridis', ax=axs[1, 0])\naxs[1, 0].set_xlabel('Chardonnay')\naxs[1, 0].set_ylabel('Customers')\naxs[1, 0].set_title('Violinplot of Chardonnay Purchases')\n\n# Plot histogram for 'sav_blanc'\nfiltered_df = df[(df['sav_blanc'] &gt; 0) & (df['sav_blanc'] &lt; 2000)]\nsns.violinplot(data=filtered_df, x='group', y='sav_blanc', palette='viridis', ax=axs[1, 1])\naxs[1, 1].set_xlabel('Sauvignon Blanc')\naxs[1, 1].set_ylabel('Customers')\naxs[1, 1].set_title('Violinplot of Sauvignon Blanc Purchases')\n\n# Plot histogram for 'syrah'\nfiltered_df = df[(df['syrah'] &gt; 0) & (df['syrah'] &lt; 1000)]\nsns.violinplot(data=filtered_df, x='group', y='syrah', palette='viridis', ax=axs[1, 2])\naxs[1, 2].set_xlabel('Syrah')\naxs[1, 2].set_ylabel('Customers')\naxs[1, 2].set_title('Violinplot of Syrah Purchases')\n\n# Plot histogram for 'cab'\nfiltered_df = df[(df['cab'] &gt; 0) & (df['cab'] &lt; 500)]\nsns.violinplot(data=filtered_df, x='group', y='cab', palette='viridis', ax=axs[2, 0])\naxs[2, 0].set_xlabel('Cabernet')\naxs[2, 0].set_ylabel('Customers')\naxs[2, 0].set_title('Violinplot of Cabernet Purchases')\n\n# Hide the empty subplots\naxs[2, 1].axis('off')\naxs[2, 2].axis('off')\n\n# Adjust layout\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nFigure 1: Violinplot of Baseline Variables\n\n\n\n\n\nThe shape of the distributions across group is quite similar, indicating comparable distributions of baseline variables. This similarity confirms that the randomization process was successful in achieving balance between the treatment and control groups."
  },
  {
    "objectID": "posts/ab_testing1/1_ab_testing.html#histograms",
    "href": "posts/ab_testing1/1_ab_testing.html#histograms",
    "title": "A/B Testing Analysis",
    "section": "1.2. Histograms",
    "text": "1.2. Histograms\nAnother intuitive way to plot a distribution is the histogram. The histogram groups the data into equally wide bins and plots the number of observations within each bin.\nThe full distributions of baseline variables should also be the same between treatment groups in this case:\n\n\nCode\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Create subplots\nfig, axs = plt.subplots(3, 3, figsize=(18, 12))\n\n# Plot histogram for 'days_since'\nsns.histplot(data=df, x='days_since', hue='group', binwidth=25, stat='density', common_norm=False, alpha=0.8, multiple='stack', palette='viridis', ax=axs[0, 0])\naxs[0, 0].set_xlim(0, 700)\naxs[0, 0].set_xlabel('Days Since Last Activity')\naxs[0, 0].set_ylabel('Customers')\naxs[0, 0].set_title('Distribution of days_since by treatment group')\n\n# Plot histogram for 'visits'\nfiltered_df = df[df['visits'] &gt; 0]\nsns.histplot(data=filtered_df, x='visits', hue='group', binwidth=1, stat='density', common_norm=False, alpha=0.8, multiple='stack', palette='viridis', ax=axs[0, 1])\naxs[0, 1].set_xlim(0, 35)\naxs[0, 1].set_xlabel('Visits')\naxs[0, 1].set_ylabel('Customers')\naxs[0, 1].set_title('Distribution of website visits by treatment group')\n\n# Plot histogram for 'past_purch'\nfiltered_df = df[df['past_purch'] &gt; 0]\nsns.histplot(data=filtered_df, x='past_purch', hue='group', binwidth=25, stat='density', common_norm=False, alpha=0.8, multiple='stack', palette='viridis', ax=axs[0, 2])\naxs[0, 2].set_xlim(0, 2000)\naxs[0, 2].set_xlabel('Past Purchases ($)')\naxs[0, 2].set_ylabel('Customers')\naxs[0, 2].set_title('Distribution of past purchases by treatment group')\n\n# Plot histogram for 'chard'\nfiltered_df = df[df['chard'] &gt; 0]\nsns.histplot(data=filtered_df, x='chard', hue='group', binwidth=25, stat='density', common_norm=False, alpha=0.8, multiple='stack', palette='viridis', ax=axs[1, 0])\naxs[1, 0].set_xlim(0, 2000)\naxs[1, 0].set_xlabel('Chardonnay')\naxs[1, 0].set_ylabel('Customers')\naxs[1, 0].set_title('Distribution of Chardonnay by treatment group')\n\n# Plot histogram for 'sav_blanc'\nfiltered_df = df[df['sav_blanc'] &gt; 0]\nsns.histplot(data=filtered_df, x='sav_blanc', hue='group', binwidth=25, stat='density', common_norm=False, alpha=0.8, multiple='stack', palette='viridis', ax=axs[1, 1])\naxs[1, 1].set_xlim(0, 2000)\naxs[1, 1].set_xlabel('Sauvignon Blanc')\naxs[1, 1].set_ylabel('Customers')\naxs[1, 1].set_title('Distribution of Sauvignon Blanc by treatment group')\n\n# Plot histogram for 'syrah'\nfiltered_df = df[df['syrah'] &gt; 0]\nsns.histplot(data=filtered_df, x='syrah', hue='group', binwidth=25, stat='density', common_norm=False, alpha=0.8, multiple='stack', palette='viridis', ax=axs[1, 2])\naxs[1, 2].set_xlim(0, 1000)\naxs[1, 2].set_xlabel('Syrah')\naxs[1, 2].set_ylabel('Customers')\naxs[1, 2].set_title('Distribution of Syrah by treatment group')\n\n# Plot histogram for 'cab'\nfiltered_df = df[df['cab'] &gt; 0]\nsns.histplot(data=filtered_df, x='cab', hue='group', binwidth=25, stat='density', common_norm=False, alpha=0.8, multiple='stack', palette='viridis', ax=axs[2, 0])\naxs[2, 0].set_xlim(0, 500)\naxs[2, 0].set_xlabel('Cabernet')\naxs[2, 0].set_ylabel('Customers')\naxs[2, 0].set_title('Distribution of Cabernet by treatment group')\n\n# Hide empty subplots\nfor ax in axs.flat[7:]:\n    ax.axis('off')\n\n# Adjust layout\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nFigure 2: Histograms of Baseline Variables\n\n\n\n\n\nRandomization checks out again!"
  },
  {
    "objectID": "posts/ab_testing1/1_ab_testing.html#t-tests",
    "href": "posts/ab_testing1/1_ab_testing.html#t-tests",
    "title": "A/B Testing Analysis",
    "section": "1.3. T-Tests",
    "text": "1.3. T-Tests\nNow that we’ve visualized differences between distributions, let’s move on to a more rigorous approach for assessing statistical significance. Visualization provides intuition by allowing us to visually inspect differences, but we also want to be able to quantify the significance of these differences.\nThe most common test is the student t-test. T-tests are generally used to compare means. In this case, we want to test whether the means of the days_since, visits and past_purch distribution are the same across two groups: Treated (Email A + Email B) vs. Control. This test statistic is given by:\n\nstat = \\frac{|\\bar x_1 - \\bar x_2|}{\\sqrt{s^2 / n }}\n\nWhere \\bar x is the sample mean and s is the sample standard deviation. Under mild conditions, the test statistic is asymptotically distributed as a student t distribution.\nWe use the ttest_ind function from scipy to perform the t-test. The function returns both the test statistic and the implied p-value.\n\nfrom scipy.stats import ttest_ind\n\n# Treated vs Control Categorical Variable\ndf['treat'] = df['group'].map({'email_A': 'treated', 'email_B': 'treated', 'ctrl': 'control'})\n\n## Days Since\ndays_since_treated = df.loc[df.treat=='treated', 'days_since'].values\ndays_since_ctrl = df.loc[df.treat=='control', 'days_since'].values\n\nstat, p_value = ttest_ind(days_since_treated, days_since_ctrl)\nprint(f\"days_since t-test: statistic={stat:.4f}, p-value={p_value:.4f}\")\n\ndays_since t-test: statistic=0.0154, p-value=0.9877\n\n\n\n\nvisits t-test: statistic=-0.2451, p-value=0.8064\npast_purch t-test: statistic=0.4345, p-value=0.6639\n\n\nThe p-values from these tests all are above 0.1, therefore we do not reject the null hypothesis of no difference in means across treatment and control groups.\nThe thing is, our example has more than 2 groups to compare. With multiple groups, the most popular test is the F-test. The F-test compares the variance of a variable across different groups. The F-test statistic is\n\n\\text{stat} = \\frac{\\text{between-group variance}}{\\text{within-group variance}} = \\frac{\\sum_{g} \\big( \\bar x_g - \\bar x \\big) / (G-1)}{\\sum_{g} \\sum_{i \\in g} \\big( \\bar x_i - \\bar x_g \\big) / (N-G)}\n\nWhere G is the number of groups, N is the number of observations, \\bar x is the overall mean and \\bar x_g is the mean within group g. Under the null hypothesis of group independence, the f-statistic is F-distributed.\n\nfrom scipy.stats import f_oneway\n\n# Days Since\ndays_since_A = df.loc[df.group=='email_A', 'days_since'].values\ndays_since_B = df.loc[df.group=='email_B', 'days_since'].values\ndays_since_ctrl = df.loc[df.group=='ctrl', 'days_since'].values\n\nstat, p_value = f_oneway(days_since_A, days_since_B, days_since_ctrl)\nprint(f\"days_since F-statistic={stat:.4f}, p-value={p_value:.4f}\")\n\ndays_since F-statistic=0.1642, p-value=0.8485\n\n\n\n\nvisits F-statistic=0.0336, p-value=0.9670\npast_purch F-statistic=0.2618, p-value=0.7697\n\n\nHere again, the p-values are telling us that these distributions across treatment are likely similar."
  },
  {
    "objectID": "posts/ab_testing1/1_ab_testing.html#standardized-mean-difference",
    "href": "posts/ab_testing1/1_ab_testing.html#standardized-mean-difference",
    "title": "A/B Testing Analysis",
    "section": "1.4. Standardized Mean Difference",
    "text": "1.4. Standardized Mean Difference\nIn general, it is good practice to always perform a test for difference in means on all variables across the treatment and control group, when we are running A/B tests. However, since the denominator of the t-test statistic depends on the sample size, the corresponding p-values are not directly comparable across studies.\nAs alternative, we can use the standardized mean difference (SMD), which is just a standardized difference, which can be computed as:\n\nSMD = \\frac{|\\bar x_1 - \\bar x_2|}{\\sqrt{(s^2_1 + s^2_2) / 2}}\n\nUsually a value below 0.1 is considered a “small” difference.\nIt is good practice to collect average values of all variables across treatment and control group and a measure of distance between the two — either the t-test or the SMD — into a table that is called balance table.\nLet’s use the create_table_one function from the causalml library to generate it.\n\n\nCode\nimport numpy as np\nimport pandas as pd\n\ndef smd(feature, treatment):\n    \"\"\"Calculate the standard mean difference (SMD) of a feature between the\n    treatment and control groups.\n\n    Args:\n        feature (pandas.Series): a column of a feature to calculate SMD for\n        treatment (pandas.Series): a column that indicate whether a row is in\n                                   the treatment group or not\n\n    Returns:\n        (float): The SMD of the feature\n    \"\"\"\n    t = feature[treatment == 1]\n    c = feature[treatment == 0]\n    return (t.mean() - c.mean()) / np.sqrt(0.5 * (t.var() + c.var()))\n\n\n\ndef create_table_one(data, treatment_col, features):\n    \"\"\"Report balance in input features between the treatment and control groups.\n\n    Args:\n        data (pandas.DataFrame): total or matched sample data\n        treatment_col (str): the column name for the treatment\n        features (list of str): the column names of features\n\n    Returns:\n        (pandas.DataFrame): A table with the means and standard deviations in\n            the treatment and control groups, and the SMD between two groups\n            for the features.\n    \"\"\"\n    t1 = pd.pivot_table(\n        data[features + [treatment_col]],\n        columns=treatment_col,\n        aggfunc=[lambda x: \"{:.2f} ({:.2f})\".format(x.mean(), x.std())],\n    )\n    t1.columns = t1.columns.droplevel(level=0)\n    t1[\"SMD\"] = data[features].apply(lambda x: smd(x, data[treatment_col])).round(4)\n\n    n_row = pd.pivot_table(\n        data[[features[0], treatment_col]], columns=treatment_col, aggfunc=[\"count\"]\n    )\n    n_row.columns = n_row.columns.droplevel(level=0)\n    n_row[\"SMD\"] = \"\"\n    n_row.index = [\"n\"]\n\n    t1 = pd.concat([n_row, t1], axis=0)\n    t1.columns.name = \"\"\n    t1.columns = [\"Control\", \"Treatment\", \"SMD\"]\n    t1.index.name = \"Variable\"\n\n    return t1\n\n\n\ndf['treat'] = df['group'].map({'email_A': 1, 'email_B': 1, 'ctrl': 0})\n\ncreate_table_one(df, 'treat', ['days_since', 'visits', 'past_purch', 'chard', 'sav_blanc', 'syrah', 'cab'])\n\n\n\nTable 3: Balance Table\n\n\n\n\n\n\n\n\n\n\n\nControl\nTreatment\nSMD\n\n\nVariable\n\n\n\n\n\n\n\nn\n41330\n82658\n\n\n\ncab\n16.52 (47.27)\n16.26 (46.67)\n-0.0053\n\n\nchard\n71.67 (199.39)\n74.14 (211.84)\n0.012\n\n\ndays_since\n89.98 (89.50)\n89.99 (89.95)\n0.0001\n\n\npast_purch\n188.27 (298.18)\n189.06 (303.33)\n0.0026\n\n\nsav_blanc\n73.63 (203.37)\n71.87 (197.25)\n-0.0088\n\n\nsyrah\n26.45 (73.91)\n26.79 (75.04)\n0.0045\n\n\nvisits\n5.95 (2.85)\n5.94 (2.86)\n-0.0015\n\n\n\n\n\n\n\n\n\n\n\nIn the first two columns of Table 3, we observe the average of the different variables across the treatment and control groups, with standard errors provided in parentheses. However, it’s in the last column where the values of the standardized mean difference (SMD) are presented. A standardized difference below |0.1| for all variables suggests that the two groups are likely similar."
  },
  {
    "objectID": "posts/ab_testing1/1_ab_testing.html#difference-in-means",
    "href": "posts/ab_testing1/1_ab_testing.html#difference-in-means",
    "title": "A/B Testing Analysis",
    "section": "2.1. Difference in Means",
    "text": "2.1. Difference in Means\nLet’s start with the first causal question (Treated vs Control). We can just compare the average outcome post-treatment Y_1 across control and treated units and randomization guarantees that this difference is, on average, due to the treatment alone:\n\n\\widehat {ATE}^{simple} = \\bar Y_{t=1, d=1} - \\bar Y_{t=1, d=0}\n\nIn our case, we compute the average purchases (purch) after the campaign in the treatment group, minus the average revenue post ad campaign in the control group:\n\n\nCode\nate = df.loc[df.treat == 'treated', 'purch'].mean() - df.loc[df.treat == 'control', 'purch'].mean()\nn_treat = df[df.treat == 'treated'].shape[0]\n\nprint( f'Average Treatment Effects: ${ate.round(2)}' )\nprint( f'Average Increased Revenues: ${round(ate*n_treat):,.0f}' )\n\n\nAverage Treatment Effects: $13.32\nAverage Increased Revenues: $1,101,358\n\n\nThis \\text{ATE} is telling us that, on average, each recipient of the email campaign resulted in an additional revenue of $13.32 compared to those who did not receive the campaign. In other words, the email campaign had a positive impact on increasing purchases.\nThis suggests that, overall, the email campaign resulted in an increased revenues of $1,101,358."
  },
  {
    "objectID": "posts/ab_testing1/1_ab_testing.html#linear-regression",
    "href": "posts/ab_testing1/1_ab_testing.html#linear-regression",
    "title": "A/B Testing Analysis",
    "section": "2.2. Linear Regression",
    "text": "2.2. Linear Regression\nWe can obtain the same estimate by regressing the post-treatment outcome purch on the treatment indicator treat:\n\nY_{i} = \\alpha + \\beta D_i + \\varepsilon_i\n\nwhere D_i is the treatment indicator (equal to 1 for treated units and 0 for control units), \\alpha is the intercept term and \\beta is our coefficient of interest representing the Average Treatment Effect.\n\nimport statsmodels.formula.api as smf\n\nsmf.ols('purch ~ treat', data=df).fit().summary().tables[1]\n\n\n\nTable 6: A/B Regression Results I\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n12.4203\n0.268\n46.360\n0.000\n11.895\n12.945\n\n\ntreat[T.treated]\n13.3243\n0.328\n40.608\n0.000\n12.681\n13.967\n\n\n\n\n\n\n\n\n\nThe advantage of running a linear regression is that we can directly test whether the difference in outcomes between the treatment and control groups is statistically significant, providing us with more robust information regarding the effectiveness of the campaign.\nTable 6 is not only confirming an \\text{ATE} of $13.32, but also telling us that this difference is statistically different from zero. This provides stronger evidence regarding the effectiveness of the email campaign in increasing purchases compared to the control group.\nWe can also compare both types of campaigns against our control group:\n\nsmf.ols('purch ~ group', data=df).fit().summary().tables[1]\n\n\n\nTable 7: A/B Regression Results II\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n12.4203\n0.268\n46.360\n0.000\n11.895\n12.945\n\n\ngroup[T.email_A]\n13.2026\n0.379\n34.846\n0.000\n12.460\n13.945\n\n\ngroup[T.email_B]\n13.4460\n0.379\n35.488\n0.000\n12.703\n14.189\n\n\n\n\n\n\n\n\n\nIt’s important to note that this regression results in Table 7 are not directly addressing our second causal question (Email A vs. Email B). Instead, the coefficients obtained from the regression are comparing each email campaign against the control group. In any case, the table suggests that, on average, Campaign B generates more purchases. It remains to be determined whether this difference is statistically significant."
  },
  {
    "objectID": "posts/ab_testing1/1_ab_testing.html#distribution-comparison",
    "href": "posts/ab_testing1/1_ab_testing.html#distribution-comparison",
    "title": "A/B Testing Analysis",
    "section": "2.3. Distribution Comparison",
    "text": "2.3. Distribution Comparison\nFigure 3 illustrates the purchase distribution by treatment, providing a comprehensive view of the data. However, it’s important to note that while we have established a statistically significant difference, our estimation of this distribution assumes a Normal Distribution, which may not always hold true. In future articles, we will delve deeper into alternative methods that do not rely on such assumptions, ensuring a more robust analysis.\n\n\nCode\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Control vs Treated\ndf_control = df[df['treat'] == 'control']\ndf_treated = df[df['treat'] == 'treated']\n\n# Calculate mean and standard error for email_A and email_B\nmean_open_A = df_control['purch'].mean()\nstd_error_A = df_control['purch'].std() / np.sqrt(len(df_control))\n\nmean_open_B = df_treated['purch'].mean()\nstd_error_B = df_treated['purch'].std() / np.sqrt(len(df_treated))\n\nnormal_data_A = np.random.normal(mean_open_A, std_error_A, 10000)\nnormal_data_B = np.random.normal(mean_open_B, std_error_B, 10000)\n\nfig, axs = plt.subplots(1, 1)\n\nsns.histplot(normal_data_A, kde=True, stat='density', label='Control', bins=20, palette='viridis')\nsns.histplot(normal_data_B, kde=True, stat='density', label='Treated', bins=20, palette='viridis')\n\nplt.xlabel('Purchases')\nplt.ylabel('Density')\nplt.title('Distribution of Purchases, by Treatment')\nplt.axvline(mean_open_A, color=sns.color_palette()[0], linestyle='--', label='Mean Control')\nplt.axvline(mean_open_B, color=sns.color_palette()[1], linestyle='--', label='Mean Treated')\nplt.errorbar(mean_open_A, 0.02, xerr=std_error_A, fmt='o', color=sns.color_palette()[0], label='95% CI Control')\nplt.errorbar(mean_open_B, 0.02, xerr=std_error_B, fmt='o', color=sns.color_palette()[1], label='95% CI Treated')\nplt.legend(ncol=3, bbox_to_anchor=(0.5, -0.15), loc='upper center') \n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nFigure 3: Purchase Distribution, by Treatment\n\n\n\n\n\nAgain, assuming Normality, we can also estimate confidence intervals:\n\n\nCode\nimport scipy.stats as stats\n\ndf_control = df[df['treat'] == 'control']\ndf_treated = df[df['treat'] == 'treated']\n        \n# Calculate mean and standard error for email_A and email_B\nmean_open_A = df_control['purch'].mean()\nstd_error_A = df_control['purch'].std() / np.sqrt(len(df_control))\n\nmean_open_B = df_treated['purch'].mean()\nstd_error_B = df_treated['purch'].std() / np.sqrt(len(df_treated))\n    \n# Calculate the confidence interval using the t-distribution\nconfidence_interval_A = stats.norm.interval(0.95, loc=mean_open_A, scale=std_error_A)\nconfidence_interval_B = stats.norm.interval(0.95, loc=mean_open_B, scale=std_error_B)\n\n\n\n\nMean Response Control: 12.4\nMean Response Treated: 25.7\n95% Confidence Interval for Control: [12.00, 12.84]\n95% Confidence Interval for Treated: [25.34, 26.15]"
  },
  {
    "objectID": "posts/ab_testing1/1_ab_testing.html#email-a-vs-email-b",
    "href": "posts/ab_testing1/1_ab_testing.html#email-a-vs-email-b",
    "title": "A/B Testing Analysis",
    "section": "2.4. Email A vs Email B",
    "text": "2.4. Email A vs Email B\nNow, let’s turn our attention to the second causal question: Which email design, Email A or Email B, is more effective in driving engagement and sales?\n\n\nCode\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef plot_variable_comparison(df):\n    variables = ['open', 'click', 'purch']\n\n    fig, axs = plt.subplots(1, 3, figsize=(18, 6))\n\n    for i, variable in enumerate(variables):\n        # Control vs Treated\n        df_control = df[df['group'] == 'email_A']\n        df_treated = df[df['group'] == 'email_B']\n\n        # Calculate mean and standard error for control and treated groups\n        mean_control = df_control[variable].mean()\n        std_error_control = df_control[variable].std() / np.sqrt(len(df_control))\n        mean_treated = df_treated[variable].mean()\n        std_error_treated = df_treated[variable].std() / np.sqrt(len(df_treated))\n\n        # Generate normal distribution data for control and treated groups\n        normal_data_control = np.random.normal(mean_control, std_error_control, 10000)\n        normal_data_treated = np.random.normal(mean_treated, std_error_treated, 10000)\n\n        # Plot histograms for control and treated groups\n        sns.histplot(normal_data_control, kde=True, stat='density', label='Control', bins=20, palette='viridis', ax=axs[i])\n        sns.histplot(normal_data_treated, kde=True, stat='density', label='Treated', bins=20, palette='viridis', ax=axs[i])\n\n        # Set labels and title\n        axs[i].set_xlabel(f'{variable.capitalize()}')\n        axs[i].set_ylabel('Density')\n        axs[i].set_title(f'Distribution of {variable.capitalize()}, by Treatment')\n\n        # Plot mean and error bars\n        axs[i].axvline(mean_control, color=sns.color_palette()[0], linestyle='--', label='Mean Control')\n        axs[i].axvline(mean_treated, color=sns.color_palette()[1], linestyle='--', label='Mean Treated')\n        axs[i].errorbar(mean_control, 0.02, xerr=std_error_control, fmt='o', color=sns.color_palette()[0], label='95% CI Control')\n        axs[i].errorbar(mean_treated, 0.02, xerr=std_error_treated, fmt='o', color=sns.color_palette()[1], label='95% CI Treated')\n\n        # Add legend\n        #axs[i].legend(ncol=3, bbox_to_anchor=(0.5, -0.15), loc='upper center') \n\n    #\n    handles, labels = axs[i].get_legend_handles_labels()\n    fig.legend(handles, labels, loc='upper center', ncol=3, bbox_to_anchor=(0.5, 0.0))\n    \n    # Adjust layout\n    plt.tight_layout()\n    plt.show()\n\n\n# Example usage:\nplot_variable_comparison(df)\n\n\n\n\n\n\n\n\nFigure 4: Outcome Distribution, by AB Treatment\n\n\n\n\n\n\n\nCode\nimport scipy.stats as stats\n\ndf_email_A = df[df['group'] == 'email_A']\ndf_email_B = df[df['group'] == 'email_B']\n        \n# Calculate mean and standard error for email_A and email_B\nmean_open_A = df_email_A['purch'].mean()\nstd_error_A = df_email_A['purch'].std() / np.sqrt(len(df_email_A))\n\nmean_open_B = df_email_B['purch'].mean()\nstd_error_B = df_email_B['purch'].std() / np.sqrt(len(df_email_B))\n    \n# Calculate the confidence interval using the t-distribution\nconfidence_interval_A = stats.norm.interval(0.95, loc=mean_open_A, scale=std_error_A)\nconfidence_interval_B = stats.norm.interval(0.95, loc=mean_open_B, scale=std_error_B)\n\n\n\n\nMean Response email_A: 25.6\nMean Response email_B: 25.9\n95% Confidence Interval for email_A: [25.06, 26.19]\n95% Confidence Interval for email_B: [25.29, 26.44]\n\n\nInterestingly enough, Email A appears to outperform Email B in terms of opens and clicks (Figure 4). However, this level of engagement does not necessarily translate to higher purchase rates. Purchases from Email B are higher on average, but the large overlap in their distribution is telling us that this difference might not be significant. While purchases from Email B are higher on average, the large overlap in their distribution suggests that this difference might not be significant.\nTo confirm this conclusion, we can conduct a linear regression comparing Email A and Email B purchases:\n\n\nCode\ndf_ab = df[df['group'] != 'ctrl']\nsmf.ols('purch ~ group', data=df_ab).fit().summary().tables[1]\n\n\n\n\nTable 8: A/B Regression Results III\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n25.6228\n0.291\n88.070\n0.000\n25.053\n26.193\n\n\ngroup[T.email_B]\n0.2435\n0.411\n0.592\n0.554\n-0.563\n1.050\n\n\n\n\n\n\n\n\n\nIndeed, despite observing higher purchases associated with Email B, the obtained p-value of 0.554 indicates that this difference is not statistically significant.\nThese findings show how important it is to look at different metrics and really dig into the data to figure out what’s working best for engaging customers and boosting sales."
  },
  {
    "objectID": "posts/causal_arima/carima_data.html",
    "href": "posts/causal_arima/carima_data.html",
    "title": "Causal ARIMA Approach to Estimate Price Policy Changes on Sales",
    "section": "",
    "text": "C-ARIMA to estimate the causal effects in time series settings where no control unit is available.\nWhat is the effect of price reduction on sales? Imagine you are a popular e-commerce platform for books, such as Waterstones. As a part of your marketing strategy, you decide to reduce prices on all books. Now, you’re eager to assess the impact of this decision on the daily volume of sales. How can you measure this effect?\nObservational studies such as this pose significant challenges to identifying and estimating causal effects. Unlike A/B testing or randomized experiments, where the assignment mechanism (the process determining which units receive treatment and which receive control) is controlled and known, observational studies lack this clarity.\nPopular methods such as Difference-in-Differences (DiD) (Angrist and Pischke 2008) and Synthetic Control Methods (SCM) (Abadie, Diamond, and Hainmueller 2010) have been extensively used to evaluate the impact of interventions in the absence of experimental data across various fields, including economics and marketing. Recent advancements even combine these approaches, as seen in the Synthetic Difference-in-Differences (SDiD) (Arkhangelsky et al. 2019) estimator.\nHowever, these methods require the presence of control units that did not experience the intervention. In cases of widespread policy changes affecting all units—such as our book price reduction example—finding untreated units is often impossible.\nOn a recent article published in The Econometrics Journal (Volume 26, Issue 1), Menchetti, Cipollini, and Mealli (2022) propose a novel approach, Causal-ARIMA (C-ARIMA), to estimate the causal effect of an intervention in observational time series settings where no control unit is available.\nIn this post, we will explore how to use C-ARIMA in R estimate the impact of a treatment in settings where no controls or comparisons are available. We will demonstrate that, under certain structural assumptions, the Causal ARIMA approach can effectively recover the average treatment effect, providing insights for decision-making in scenarios where traditional methods may not suffice."
  },
  {
    "objectID": "posts/causal_arima/carima_data.html#estimation-procedure-and-inference",
    "href": "posts/causal_arima/carima_data.html#estimation-procedure-and-inference",
    "title": "Causal ARIMA Approach to Estimate Price Policy Changes on Sales",
    "section": "2.1 Estimation Procedure and Inference",
    "text": "2.1 Estimation Procedure and Inference\nIn order to estimate the causal effects with C-ARIMA, we need to follow a three-step process:\n\nEstimate the ARIMA model only in the pre-intervention period, so as to learn the dynamics of the dependent variable and the links with the covariates without being influenced by the treatment;\nBased on the process learned in the pre-intervention period, perform a prediction step and obtain an estimate of the counterfactual outcome during the post-intervention period in the absence of intervention;\nBy comparing the observations with the corresponding forecasts at any time point in the post-intervention period, evaluate the resulting differences, which represent the estimated point causal effects.\n\nLet W_{i,t} ∈ (0, 1) be a random variable describing the treatment assignment of unit i ∈ {1, . . . , N} at time t ∈ {1, . . . , T}, where 1 denotes that a “treatment” (or “intervention”) has taken place and 0 denotes control. Then, our estimands of interest are:\n\n\\text{Point Causal Effect: } \\tau_t(w,w') = Y_t(w) - Y_t(w') \\\\\n\\text{Cumulative Causal Effect: } \\Delta_t(w,w') = \\sum^{t}_{s+t^*+1} \\tau_t(w,w') \\\\\n\\text{Average Causal Effect: } \\bar\\tau_t(w,w') = \\frac{\\Delta_t(w,w')}{t-t^*}\n\nThen, we have two options to perform inference on the estimated effects: (i) we can rely in the Normality of the error terms, or (ii) we can resort to a Bootstrap Strategy by using resampled residuals in order to compute empirical critical values."
  },
  {
    "objectID": "posts/causal_arima/carima_data.html#c-arima-using-fable-package",
    "href": "posts/causal_arima/carima_data.html#c-arima-using-fable-package",
    "title": "Causal ARIMA Approach to Estimate Price Policy Changes on Sales",
    "section": "3.1 C-ARIMA using fable package",
    "text": "3.1 C-ARIMA using fable package\nLet’s start by fitting an ARIMA model on our pre-intervention period (between January and March 15):\n\nfit &lt;- df1 |&gt;\n    filter_index(\"2014-01-05\" ~ \"2014-03-15\") |&gt;\n    model(ARIMA(y ~ x1)\n)\n\nreport(fit) ## ARIMA(0,0,0)(0,0,1)[7]\n\nSeries: y \nModel: LM w/ ARIMA(0,0,0)(0,0,1)[7] errors \n\nCoefficients:\n        sma1      x1\n      0.2050  1.1993\ns.e.  0.1229  0.0019\n\nsigma^2 estimated as 1.429:  log likelihood=-110.95\nAIC=227.9   AICc=228.26   BIC=234.64\n\n\nNow we estimate the counterfactual outcome during the post-intervention period in the absence of intervention. For this, we first need to include the post-intervention evolution of the x1 regressor:\n\ndf1_x1 &lt;- df1 %&gt;% \n  select(Date, x1) %&gt;% \n  filter_index(\"2014-03-16\" ~ \"2014-04-14\")\n\nNow we generate the forecasted values:\n\n# Generate the forecasted values using the fitted model\nforecast_result &lt;- forecast(fit, new_data = df1_x1)\n\n\n\n\n\n\n\n\n\nFigure 2: Sales ($) ARIMA Forecast\n\n\n\n\n\nFigure 2 provides a graphical representation of the observed time series and the forecasted series in the absence of intervention. At the 1-month time horizon, the causal effect is significantly positive at the 5% level. Additionally, our ARIMA model is able to closely follow the series during the pre-intervention period. We can summarize this fitness level by calculating RMSE and R-Squared scores:\n\n##RMSE & R2_Score\nfore1 &lt;- fitted(fit) %&gt;% select(Date, .fitted)\n\naccuracy1 &lt;- df1 %&gt;% \n  left_join(fore1, by=\"Date\") %&gt;% \n  filter_index(\"2014-01-05\" ~ \"2014-03-15\")\n\nRMSE_CARIMA &lt;- RMSE(accuracy1$.fitted, accuracy1$y)\nR2_CARIMA &lt;- R2_Score(accuracy1$.fitted, accuracy1$y)\n\n\n\nPre-Intervention RMSE: 1.178076 \n\n\nPre-Intervention R-Squared: 0.8794006 \n\n\n\n3.1.1 Point Causal Effects\nThe point causal effect at time t can be estimated with the following code:\n\nATE_df &lt;- df1 %&gt;% \n  filter(Date &gt;= ymd(\"2014-03-16\") & Date &lt;= ymd(\"2014-04-14\"))\n\n# Point Effect\npoint_effect &lt;- ATE_df$y - forecast_result$y\n\n# Calculate the point estimate\npoint_estimate &lt;- mean(point_effect)\n\n# Calculate the lower bound of the 95% confidence interval\nlower_bound &lt;- quantile(point_effect, 0.025)\n\n# Calculate the upper bound of the 95% confidence interval\nupper_bound &lt;- quantile(point_effect, 0.975)\n\n# Create a data frame\ncausal_effect_df &lt;- data.frame(\n  Date = ATE_df$Date,\n  Point_Estimate = point_estimate,\n  Lower_Bound = lower_bound,\n  Upper_Bound = upper_bound\n)\n\n\n\n\n\n\n\n\n\nFigure 3: Causal Effect with 95% Confidence Intervals\n\n\n\n\n\nIn Figure 3 we observe that the impact of the price policy change is quite constant in time, around $10.\n\n\n3.1.2 Average Causal Effects\nThe temporal average effect indicates the number of additional books sold daily, on average, due to the permanent price reduction. We also want to do some inference, standard errors and confidence intervals. A confidence interval gives an interval within which we expect y_t to lie with a specified probability. For example, assuming that distribution of observations is normal, a 95% prediction interval for the h- step forecast is:\n\n\\hat{y}_{T+h|T} ± (1.96) \\hat \\sigma_h\n\nLet’s code this:\n\n# Calculate the point estimate\naverage_treatment_effect  &lt;- mean(point_estimate)\n\n# Standard error of the point estimates\nvariance_point_effect &lt;- distributional::variance(point_effect)\nstandard_deviation &lt;- sqrt(variance_point_effect)\nstandard_error &lt;- standard_deviation / sqrt(length(point_effect))\n\n# Confidence interval for the average treatment effect\nci_lower &lt;- average_treatment_effect - 1.96 * mean(standard_error)\nci_upper &lt;- average_treatment_effect + 1.96 * mean(standard_error)\n\n\n\nAverage Causal Effect: 10.39115 \n\n\nStandard Error: 0.2217059 \n\n\n95% Confidence Interval: [ 9.956609 ,  10.8257 ]\n\n\n\n\n3.1.3 Bootstrapped Confidence Intervals\nWhen a normal distribution for the residuals is an unreasonable assumption, one alternative is to use bootstrapping, which only assumes that the residuals are uncorrelated with constant variance. This is easily achieved by simply adding bootstrap=TRUE in the forecast() function.\n\nforecast_result &lt;- forecast(fit, new_data = df1_x1, bootstrap=TRUE)\n\n# Point Effect\npoint_effect &lt;- ATE_df$y - forecast_result$y\n\n# Calculate the point estimate\npoint_estimate &lt;- mean(point_effect)\n\n# Calculate the point estimate\naverage_treatment_effect  &lt;- mean(point_estimate)\n\n# Standard error of the point estimates\nvariance_point_effect &lt;- distributional::variance(point_effect)\nstandard_deviation &lt;- sqrt(variance_point_effect)\nstandard_error &lt;- standard_deviation / sqrt(length(point_effect))\n\n\n# Confidence interval for the average treatment effect\nci_lower &lt;- average_treatment_effect - 1.96 * mean(standard_error)\nci_upper &lt;- average_treatment_effect + 1.96 * mean(standard_error)\n\n\n\nAverage Treatment Effect: 10.38964 \n\n\nStandard Error: 0.2178212 \n\n\n95% Confidence Interval: [ 9.962708 ,  10.81657 ]\n\n\nThe results are quite similar in this case."
  },
  {
    "objectID": "posts/causal_arima/carima_data.html#the-causalarima-package",
    "href": "posts/causal_arima/carima_data.html#the-causalarima-package",
    "title": "Causal ARIMA Approach to Estimate Price Policy Changes on Sales",
    "section": "3.2 The CausalARIMA Package",
    "text": "3.2 The CausalARIMA Package\nThe authors of the paper are developing an R package called CausalArima for easier and faster application of the proposed method. The development version of the package can be accessed from https://github.com/FMenchetti/CausalArima.\nLet’s see how it works:\n\n# install.packages(\"tidybayes\")\n#devtools::install_github(\"FMenchetti/CausalArima\")\nlibrary(CausalArima)\n\n\n# Causal effect estimation\n# fit the model - Causal effect estimation\nce &lt;- CausalArima(y = ts(y, start = start, frequency = 1), \n                  dates = dates, \n                  int.date = int.date,\n                  xreg =x1, \n                  nboot = 1000)\n\nHow to obtain the plot of the forecast:\n\n\n\n\n\n\n\n\nFigure 4: Sales ($) ARIMA Forecast\n\n\n\n\n\n\nimpact_p &lt;- plot(ce, type=\"impact\", color_line=mycolors[1], color_intervals=\"#91D1C2B2\")\ngrid.arrange(impact_p$plot, impact_p$cumulative_plot)\n\n\n\n\n\n\n\n\n\n3.2.1 Inference\nDoing inference with this package is straightforward, with the options of using the normality assumption or the bootstrap alternative:\n\nsummary(ce)\n\n                                      \nPoint causal effect            12.257 \nStandard error                 1.211  \nLeft-sided p-value             1      \nBidirectional p-value          0      \nRight-sided p-value            0      \n                                      \nCumulative causal effect       310.709\nStandard error                 6.634  \nLeft-sided p-value             1      \nBidirectional p-value          0      \nRight-sided p-value            0      \n                                      \nTemporal average causal effect 10.357 \nStandard error                 0.221  \nLeft-sided p-value             1      \nBidirectional p-value          0      \nRight-sided p-value            0      \n\nsummary_model &lt;- impact(ce, format=\"html\")\n\n\nsummary_model$impact_norm$average\n\n\n\n\n\nestimate\nsd\np_value_left\np_value_bidirectional\np_value_right\n\n\n\n\n10.357\n0.221\n1\n0\n0\n\n\n\n\n\n\n\n\n\nsummary_model$impact_boot$average\n\n\n\n\n\n\nestimates\ninf\nsup\nsd\n\n\n\n\nobserved\n117.049\nNA\nNA\nNA\n\n\nforecasted\n106.692\n106.264\n107.142\n0.222\n\n\nabsolute_effect\n10.357\n9.907\n10.784\n0.222\n\n\nrelative_effect\n0.097\n0.093\n0.101\n0.002"
  },
  {
    "objectID": "posts/jupyter-first/hello.html",
    "href": "posts/jupyter-first/hello.html",
    "title": "Quarto Basics",
    "section": "",
    "text": "For a demonstration of a line plot on a polar axis, see Figure 1.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nr = np.arange(0, 2, 0.01)\ntheta = 2 * np.pi * r\nfig, ax = plt.subplots(\n  subplot_kw = {'projection': 'polar'} \n)\nax.plot(theta, r)\nax.set_rticks([0.5, 1, 1.5, 2])\nax.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\nFigure 1: A line plot on a polar axis"
  },
  {
    "objectID": "posts/jupyter-first/hello.html#polar-axis",
    "href": "posts/jupyter-first/hello.html#polar-axis",
    "title": "Quarto Basics",
    "section": "",
    "text": "For a demonstration of a line plot on a polar axis, see Figure 1.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nr = np.arange(0, 2, 0.01)\ntheta = 2 * np.pi * r\nfig, ax = plt.subplots(\n  subplot_kw = {'projection': 'polar'} \n)\nax.plot(theta, r)\nax.set_rticks([0.5, 1, 1.5, 2])\nax.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\nFigure 1: A line plot on a polar axis"
  },
  {
    "objectID": "posts/nonparametrics/nonparametrics.html",
    "href": "posts/nonparametrics/nonparametrics.html",
    "title": "Nonparametric & Semiparametric Binary-Choice Regressions",
    "section": "",
    "text": "Nonparametric Kernel Density, Nonparametric Regression and Binary-Choice Models.\nThe data set used in this assignment comes from Pindyck and Rubinfeld (1998) “Econometric Models and Economic Forecasts” and contains the following variables: a dummy whether children attend private school (private), number of years the family has been at the present residence (years), log of property tax (logptax), log of income (loginc), and whether one voted for an increase in property taxes (vote). There are two dependent variables of interest -private and vote- which can be modeled individually by two univariate probit models or jointly by one bivariate probit model, for instance.\nLink to PDF HERE."
  },
  {
    "objectID": "posts/nonparametrics/nonparametrics.html#kernel-density-estimation",
    "href": "posts/nonparametrics/nonparametrics.html#kernel-density-estimation",
    "title": "Nonparametric & Semiparametric Binary-Choice Regressions",
    "section": "Kernel Density Estimation",
    "text": "Kernel Density Estimation\nBefore running a binary choice model to explain the voting choice, we plot different Kernel Density estimates of loginc to check whether it follows approximately a normal distribution. The idea is to test visually if the normal distribution falls within the 95% confidence interval area of each kernel density estimate:\n\n\n\n\n\n\nFigure 1: Kernel Density Estimation of Log(Income)\n\n\n\nAll these Kernel Density estimators in Figure 1 use an alternative Epanechnikov Kernel function (this is the default in the kdens command in STATA). The differences among the estimator are explained by the different methods of choosing the Optimal Bandwidth \\(h^*\\) that minimizes the Mean Integrated Squared Errors (MISE)1. Comparing the kernel densities of the log income against the normal distribution, we observe that the normal distribution seems to not quite fit inside the confidence intervals in none of the kernel estimators. While the case of an optimal bandwidth of \\(h^*=0.188\\) in Figure 1.b. seems to present some evidence about normality, you can still see how the purple line appears outside the boundaries around the 9 to 9.5 value.\nBoth Plug-in methods (Figure 1.a. and 1.c.) use a lower Optimal Bandwidth (0.11 and 0.12, respectively), thus, leading to less bias but more variance in their estimates. In these cases, the normal distribution line appears outside the 95% confidence boundaries in several places. Overall, the visual test seems to reject the hypothesis that log incomes follow a normal distribution."
  },
  {
    "objectID": "posts/nonparametrics/nonparametrics.html#binary-choice-model-probit",
    "href": "posts/nonparametrics/nonparametrics.html#binary-choice-model-probit",
    "title": "Nonparametric & Semiparametric Binary-Choice Regressions",
    "section": "Binary Choice Model (probit)",
    "text": "Binary Choice Model (probit)\nTable 1 reports the results of different Binary choice models to predict voting behavior using years, logptax and loginc as independent variables:\n\n\n\nTable 1: Binary Choice Model Results\n\n\n\n\n\n\nColumn 1 in Table 1 presents the results of the probit model. Here we see a positive and statistically significant relationship between the level of log income and the probability of voting for higher property taxes. Also, once again we observe that property taxes have a negative effect on voting in favor of higher taxes of this kind. Finally, the number of years in the residence did not report a significant result."
  },
  {
    "objectID": "posts/nonparametrics/nonparametrics.html#normality-assumption",
    "href": "posts/nonparametrics/nonparametrics.html#normality-assumption",
    "title": "Nonparametric & Semiparametric Binary-Choice Regressions",
    "section": "Normality Assumption",
    "text": "Normality Assumption\nWe can use a visual test to check the normality assumption of the probit model estimated in Column 1. Recall that in the probit model, the probability that \\(y_i\\) takes on the value 1 is modeled as a nonlinear function of a linear combination of a set of independent variables:\n\\[\n\\Pr(y_i=1|x_i)  = \\Phi(x_i'\\beta)\n\\]\nwhere \\(\\Phi(\\cdot)\\) is assumed to be the standard normal cumulative distribution function (CDF). To check if this assumption is reasonable, we performed nonparametric estimations of \\(\\Pr(y_i=1|x_{i'\\beta})\\). The first estimator is a Nadaraya-Watson Regression (or constant constant estimator) that assumes a constant \\(m(x)=b_0(x_0)\\) around some neighborhood of \\(x_0\\):\n\\[\n\\hat{m}_h(x_0)=\\frac{\\sum^N_{i=1} K_x\\left(\\frac{x_0-x_i}{h} \\right)}{\\sum^N_{i=1} K_x\\left(\\frac{x_0-x_j}{h} \\right)}y_i\n\\]\nThe Nadaraya-Watson estimator is essentially a weighted local average of the observations (weights defined by a Kernel function) at some neighborhood defined by the choice of bandwidth \\(h\\). The second estimator is a Local Linear Regression that, instead of assuming a constant average, lets \\(m(x)\\) be linear in the neighborhood of \\(x_0\\). More concretely, the local linear regression minimizes with respect to \\(b_0(x_0)\\) and \\(b_1(x_0)\\):\n\\[\n\\sum^N_{i=1} K_x\\left(\\frac{x_i-x_0}{h} \\right)[y_i-b_0(x_0)-b_1(x_0)(x_i-x_0)]^2\n\\]\nBoth nonparametric regressions use the Epanechnikov Kernel function. Regarding the choice of bandw\n\n\n\nTable 2: Binary Choice Model Results\n\n\n\n\n\n\nidth \\(h^*\\), I use the command npregress that performs the Leave-One-Out Cross Validation method (LOOCV) to estimate the Optimal Bandwidth size2. Figure 2 compares these regressions against the probit estimation:\n\n\n\n\n\n\nFigure 2: Nonparametric Estimation of Het Probit\n\n\n\nIn general, we see that the normality assumption about the error term \\(\\varepsilon_i\\) in the probit model is actually not rejected, as the probit curve of the \\(\\Pr(y_i=1|x_i'\\beta)\\) tends to be within the 95% confidence intervals of both the Local Constant and Local Linear nonparametric estimators. This means that the standard probit model does not suffer from misspecification."
  },
  {
    "objectID": "posts/nonparametrics/nonparametrics.html#heteroscedastic-probit-model",
    "href": "posts/nonparametrics/nonparametrics.html#heteroscedastic-probit-model",
    "title": "Nonparametric & Semiparametric Binary-Choice Regressions",
    "section": "Heteroscedastic Probit Model",
    "text": "Heteroscedastic Probit Model\nColumn 2 of Table 2 presents the results of the heteroscedastic probit model. This specification still assumes that the error term of the latent model follows a normal distribution, but is more flexible in the sense that it no longer assumes its variance to be 1, but that it can vary as a function of some set of explanatory variables. In this case, we only use the variable years to model the variance:\n\\[\nV(\\varepsilon_i|x_i)=\\sigma^2_i=[\\exp(\\text{years}_{i}'\\gamma)]^2\n\\]\nwhere \\(\\varepsilon_i\\sim N[0,\\exp(\\text{years}_{i}'\\gamma)]\\). The probability of success is now represented by:\n\\[\n\\Pr(y_i=1|x_i)  = \\Phi\\left( \\frac{x_i'\\beta}{\\sigma_i} \\right) = \\Phi\\left( \\frac{x_i'\\beta}{\\exp(\\text{years}_{i}'\\gamma)} \\right)\n\\]\nHere we are primarily interested in testing whether \\(\\gamma\\neq 0\\), which seems to be the case as we see in Table 2 a statistically significant coefficient. Additionally, the likelihood-ratio test of heteroskedasticity, which tests the full model with heteroskedasticity against the full model without, is significant as \\(\\chi^2(1) = 11.55\\). Regarding the coefficients, we now see that the number of years in the residence impacts negatively the probability of voting for higher property taxes. The remaining variables are still significant and with the same sign relative to the standard probit model, but the coefficients are more pronounced."
  },
  {
    "objectID": "posts/nonparametrics/nonparametrics.html#nonparametric-regression",
    "href": "posts/nonparametrics/nonparametrics.html#nonparametric-regression",
    "title": "Nonparametric & Semiparametric Binary-Choice Regressions",
    "section": "Nonparametric Regression",
    "text": "Nonparametric Regression\nWe now test again the normality assumption of the heteroscedastic probit model by visually comparing its CDF with the two nonparametric alternatives already discussed:\n\n\n\n\n\n\nFigure 3: Nonparametric Estimation of Heteroscedastic Probit\n\n\n\nClearly, the heteroscedastic alternative -that assumes normality- highly deviates from both nonparametric estimations. Hence, it seems that the normality assumption in the hetprobit model is rejected. Furthermore, the probit link function is no longer smooth when compared to the standard probit. Modeling the variance as dependent on the explanatory variable years makes the prediction less linear, leading to a more noisy curve than smoothed."
  },
  {
    "objectID": "posts/nonparametrics/nonparametrics.html#klein-spady-semiparametric-regression",
    "href": "posts/nonparametrics/nonparametrics.html#klein-spady-semiparametric-regression",
    "title": "Nonparametric & Semiparametric Binary-Choice Regressions",
    "section": "Klein & Spady Semiparametric Regression",
    "text": "Klein & Spady Semiparametric Regression\nSemiparametric alternatives do not rely on the parametric assumptions about the shape of the error term distribution \\(\\varepsilon_i\\). The Klein & Spady method is a Single Index model with a \\(\\Pr(y_i|x_i)= F(x_i'\\beta)\\) structure that only depends on \\(x_i\\) through a single linear combination, \\(x_i'\\beta\\), whereas the function \\(F(\\cdot)\\) is left unspecified. The estimation is based on some kind of Maximum Likelihood Estimation:\n\\[\nL(\\beta,h)=\\sum^N_{i=1}(1-y_i)\\ln[1-\\hat{F}_{-i,n}(x_i'\\beta)]+\\sum^N_{i=1}y_i\\ln[\\hat{F}_{-i,n}(x_i'\\beta)]\n\\]\nSince \\(F(\\cdot)\\) is unknown, Klein & Spady suggest replacing it with a Leave-One-Out Nadaraya-Watson estimator \\(\\hat{F}_{-i,n}(\\cdot)\\). In order to guarantee point identification, the coefficient of one continuous variable is normalized to 1, which in our case will be the variable loginc. Additionally, in single index models, the function \\(F(\\cdot)\\) will include any location and level shift, so the vector \\(x_i\\) cannot include an intercept. Column 3 of Table 2 shows the results of this semiparametric regression. But because we want to make some comparisons between the different models so far presented, we will also need to scale normalize the previous models, as in Table 2 they are not directly comparable. Table 3 shows the results of each model after scale normalization:\n\n\n\nTable 3: Binary Choice Models: Normalized Results\n\n\n\n\n\n\nHere we see that the direction of the signs in every specification is the same: years and logptax are negatively associated with the probability of voting in favor of the tax reform. The coefficients of the property tax variable are to some degree similar across models, but there are important differences regarding the coefficient linked to the number of years in residence. First, the probit model did not show an effect statistically distinct from zero, whereas the other two models did show significant and more pronounced coefficients.\nBetween the heteroscedastic probit and the Klein & Spady regression, the hetprobit seems to report a more negative effect of years on the probability of voting for higher taxes. In order to understand the magnitude of these differences between models, we will need to estimate marginal effects."
  },
  {
    "objectID": "posts/nonparametrics/nonparametrics.html#semiparametric-regression-visualization",
    "href": "posts/nonparametrics/nonparametrics.html#semiparametric-regression-visualization",
    "title": "Nonparametric & Semiparametric Binary-Choice Regressions",
    "section": "Semiparametric Regression Visualization",
    "text": "Semiparametric Regression Visualization\nFigure 4 visualize the Klein & Spady semiparametric estimate of \\(\\Pr(y_i=1|x_i'\\beta)\\) and compares it against the heteroskedastic estimation:\n\n\n\n\n\n\nFigure 4: Semiparametric Estimation of Probits\n\n\n\nThe semiparametric regression in Figure 4 this case is quite similar to the ``U’’ shape of the nonparametric (Nadara-Watson) regression from Figure 3.a. that is based on the predicted values \\(x_i'\\beta\\) of the heteroscedastic probit to estimate the probabilities. However, we also see that the (parametric) heteroscedastic probit falls outside the 95% confidence interval area at various points ( Figure 4 in green). Hence, while we cannot use the hetprobit model to draw conclusions on the probability of voting for higher property taxes, we do can use a nonparametric version of the predicted values of the heteroscedastic model for that purposes.\nMost of the behavior behind this strange shape of the link function -behavior that is mostly happening at \\(x_i'\\beta&lt;-5\\) in Figure 4 -, can be explained by the distribution of the years variable. From Figure 5 we can observe that this group consists of 5 observations that have an average number of years of 39, versus the 6.8 from the rest of the sample:\n\n\n\n\n\n\nFigure 5: Kernel Density Estimation of Years\n\n\n\nAs we have seen from the models, the number of years has in general a negative effect on the probability of voting for more taxes, but here we have that all these 5 observations have a high number of years in the residence and all of them have vote=1. Hence, this group of ``outliers’’ might be generating a countervailing effect on the true point estimates of years."
  },
  {
    "objectID": "posts/nonparametrics/nonparametrics.html#restricted-model",
    "href": "posts/nonparametrics/nonparametrics.html#restricted-model",
    "title": "Nonparametric & Semiparametric Binary-Choice Regressions",
    "section": "Restricted Model",
    "text": "Restricted Model\nWe now re-run the binary choice models so far discussed, but restricting the estimation for observations where the number of years in residence is smaller than 25. Table 4 shows the main results after normalization to make them comparable:\n\n\n\nTable 4: Binary Choice Models: Normalized Results (years &lt; 25)\n\n\n\n\n\n\nHere we see surprisingly similar conclusions on the point estimates across different specifications. In the standard probit model, we now see that years have statistically significant results, with a more intense negative effect on our dependent variable than before. The heteroscedastic probit now reports a more negative coefficient on the variable logptax, significant only at the 10% level, but for the number of years, we see a less pronounced effect and is no longer significant. Looking at \\(\\gamma\\), note that now we cannot reject the null hypothesis of homoscedastic errors so that, after filtering for the extreme values of years, there is no evidence of this model being better than the standard probit.\nFinally, the semiparametric model seems to be the model that changed the least. Both the direction of the signs and their significance remain as before, with years having a slightly less pronounced coefficient, and logptax reporting now a more negative effect on the probability of voting for higher taxes than before.\nWe want now to test the normality assumption, so we re-estimate the nonparametric regressions from previous exercises to make the visual comparison. In this case, I only perform the Nadaraya-Watson Estimator:\n\n\n\n\n\n\nFigure 6: Non and Semiparametric Estimation (years&lt;25)\n\n\n\nAgain, we see that all both the probit and hetprobit models falls within the boundaries of the confidence interval of the nonparametric estimates. Taking all this evidence together, we can conclude that, after filtering the extreme values of years, the three binary choice models are no longer much different from each other. Finally, because now there is no evidence of heteroscedasticity and we did not reject the normality assumption about the error terms \\(\\varepsilon_i\\), the standard probit model is the preferred model in my view."
  },
  {
    "objectID": "posts/nonparametrics/nonparametrics.html#nonparametric-and-semiparametric-regression-comparison",
    "href": "posts/nonparametrics/nonparametrics.html#nonparametric-and-semiparametric-regression-comparison",
    "title": "Nonparametric & Semiparametric Binary-Choice Regressions",
    "section": "Nonparametric and Semiparametric Regression Comparison",
    "text": "Nonparametric and Semiparametric Regression Comparison\nUsing the standard probit estimates, we want now to graphically compare the estimates between the Nadaraya-Watson (Local Constant), the Local Linear, and the Local Quadratic regression. The following figure summarizes the results:\n\n\n\n\n\n\nFigure 7: Nonparametric and Semiparametric Regression - Comparison\n\n\n\nAlthough the curves seem to be very similar, we can identify some differences at specific points. Generally speaking, the degree of bias of the Nadaraya-Watson highly depends on the curvature or second derivative \\(m''(x)\\) of the regression function, as well on the slope of the density of the regressors \\(f'(x)\\):\n\\[\n\\text{bias}[\\hat{m}(x)]=\\frac{h^2\\mu_2}{2}\\left\\{m''(x)+2\\frac{f'_x(x)m'(x)}{f_x(x)} \\right\\}+O(\\frac{1}{nh})+o(h^2)\n\\]\nNamely, the bias will be very large at a point where the density function curves a lot. In Figure 7 (and assuming the normality distribution is the true function), we can actually observe that the Local Constant estimator has a large and positive bias (i) between \\(-1&lt;x_i'\\beta&lt;0.5\\), which is the place where the density is curving positively and very quickly (positive curvature) and (ii) between \\(0.5&lt;x_i'\\beta&lt;1.55\\), marking the space where the density function is decreasing quickly (negative curvature). The local linear and local quadratic regressions also present this described behavior, but with less biasedness. The local linear is the more smoothed function among the three.\nOne reason that explains this difference may be due to the fact that the bias of the Local Linear (and Quadratic) regression does not depend on the density function of the regressors, also called design bias:\n\\[\\begin{align*}\n    \\text{bias}[\\hat{m}(x)]=\\frac{h^2\\mu_2}{2}m''(x)+o(h^2)\n\\end{align*}\\]\nIn that sense, the design bias adds an extra bias to the Nadaraya-Watson estimator. Finally, we observe more differences between the estimators on the extremes. On the bottom left (between -1.2 and -2), for example, the Nadaraya-Watson starts with higher bias, but then it shifts quickly to be equal to 0 towards the left, whereas the other two (which are very similar to each other) start with less bias, but then they end up with negative probabilities towards the left. On the other extreme we find the same behavior, where the local and quadratic estimators are not bounded to be within 0 and 1."
  },
  {
    "objectID": "posts/nonparametrics/nonparametrics.html#footnotes",
    "href": "posts/nonparametrics/nonparametrics.html#footnotes",
    "title": "Nonparametric & Semiparametric Binary-Choice Regressions",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAs the optimal bandwidth formula also depends on the curvature of the density \\(f''\\), which is unknown, there are different ways to deal with this issue. Some impose assumptions about the unknown distribution (Plug-in methods), and others are more data-driven such as cross-validation techniques.↩︎\nIn contrast, the command lpoly uses the Rule-of-Thumb method of bandwidth selection, which is a Plug-In estimator.↩︎"
  }
]