[
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n\n\n\n\n\n\n\n\n\n\nMachine Learning with Python: A Quick Guide\n\n\n\nMachine Learning\n\n\nPython\n\n\nscikit-learn\n\n\n\n\n30 March 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAmsterdam Airbnb Price Prediction: A Machine Learning Approach\n\n\n\nPython\n\n\nGeospatial Analysis\n\n\nMachine Learning\n\n\ngeopandas\n\n\nscikit-learn\n\n\n\n\n11 March 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImpact of Transit-Oriented Development on Dutch Commercial Real Estate Prices\n\n\n\nMaster Thesis\n\n\nEconometrics\n\n\nCausal Inference\n\n\nDiff-in-Diff\n\n\n\n\n02 January 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCustomer Analytics with R\n\n\n\nR & Rstudio\n\n\nA/B Testing\n\n\nMachine Learning\n\n\n\n\n01 December 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVoting Patterns of the Ministers of the Supreme Court in Antitrust Cases: Evidence from Chile\n\n\n\nArticle\n\n\nCompetition & Antitrust\n\n\nR & Rstudio\n\n\n\n\n18 May 2022\n\n\n\n\n\n\n\n\nNo matching items\n\n Back to top"
  },
  {
    "objectID": "posts/jupyter-first/hello.html",
    "href": "posts/jupyter-first/hello.html",
    "title": "Quarto Basics",
    "section": "",
    "text": "For a demonstration of a line plot on a polar axis, see Figure 1.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nr = np.arange(0, 2, 0.01)\ntheta = 2 * np.pi * r\nfig, ax = plt.subplots(\n  subplot_kw = {'projection': 'polar'} \n)\nax.plot(theta, r)\nax.set_rticks([0.5, 1, 1.5, 2])\nax.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\nFigure 1: A line plot on a polar axis"
  },
  {
    "objectID": "posts/jupyter-first/hello.html#polar-axis",
    "href": "posts/jupyter-first/hello.html#polar-axis",
    "title": "Quarto Basics",
    "section": "",
    "text": "For a demonstration of a line plot on a polar axis, see Figure 1.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nr = np.arange(0, 2, 0.01)\ntheta = 2 * np.pi * r\nfig, ax = plt.subplots(\n  subplot_kw = {'projection': 'polar'} \n)\nax.plot(theta, r)\nax.set_rticks([0.5, 1, 1.5, 2])\nax.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\nFigure 1: A line plot on a polar axis"
  },
  {
    "objectID": "posts/ab_testing1/1_ab_testing.html",
    "href": "posts/ab_testing1/1_ab_testing.html",
    "title": "A/B Testing Analysis",
    "section": "",
    "text": "A/B testing Analysis using Python.\nAs data scientists, we are frequently confronted with questions like: “Does X affects Y?” Here, Y is the outcome that we care about, while X could be a new feature, product, policy or experience. For example, a website owner would like to ask if a new web page design leads to a higher click-through rate or sale. Similarly, Spotify may to know whether their new interface leads to more minutes of music played, while Zalando aims to assess the impact of their marketing actions on purchases.\nBut how do we go about answering such causal questions? For the Platform Economy, the solution lies in experimentation. By randomly assigning some kind of treatment (new interface design, new product, etc.) to a subset of users (Group A) and comparing their behavior or outcomes (such as revenue, visits, clicks, etc.) to those who did not receive the treatment (Group B), we can effectively isolate the causal impact of the change. This is at the core of A/B testing (also known as Randomized Control Trials), which is considered the workhorse method of experimentation for platforms like Spotify, Uber, Netflix, an more.\nIn this article, we will delve into the analysis involved in A/B testing using Python, with a specific focus on Randomization Checks and various methods or statistical tools for studying Average Treatment Effects. Let’s go!"
  },
  {
    "objectID": "posts/ab_testing1/1_ab_testing.html#violinplots",
    "href": "posts/ab_testing1/1_ab_testing.html#violinplots",
    "title": "A/B Testing Analysis",
    "section": "1.1. Violinplots",
    "text": "1.1. Violinplots\nA first visual approach is the violinplot. The violinplot plots depicts distributions of numeric data for one or more groups using density curves. These densities —usually smoothed by a kernel density estimator— are displayed along the y-axis so that we can compare them. By default, the .violintplot() function from the seaborn library also adds a miniature boxplot inside. Figure 1 shows the violinplots of our baseline variables:\n\n\nCode\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n\n# Create subplots\nfig, axs = plt.subplots(3, 3, figsize=(15, 12))\n\n# Plot histogram for 'days_since'\nfiltered_df = df[df['days_since'] &gt; 0]\nsns.violinplot(data=filtered_df, x='group', y='days_since', palette='viridis', ax=axs[0, 0])\naxs[0, 0].set_xlabel('Days Since Last Activity')\naxs[0, 0].set_ylabel('Customers')\naxs[0, 0].set_title('Violinplot of Days Since')\n\n# Plot histogram for 'visits'\nfiltered_df = df[df['visits'] &gt; 0]\nsns.violinplot(data=filtered_df, x='group', y='visits', palette='viridis', ax=axs[0, 1])\naxs[0, 1].set_xlabel('Visits')\naxs[0, 1].set_ylabel('Customers')\naxs[0, 1].set_title('Violinplot of Website Visits')\n\n# Plot histogram for 'past_purch'\nfiltered_df = df[(df['past_purch'] &gt; 0) & (df['past_purch'] &lt; 2000)]\nsns.violinplot(data=filtered_df, x='group', y='past_purch', palette='viridis', ax=axs[0, 2])\naxs[0, 2].set_xlabel('Past Purchases ($)')\naxs[0, 2].set_ylabel('Customers')\naxs[0, 2].set_title('Violinplot of Past Purchases')\n\n# Plot histogram for 'chard'\nfiltered_df = df[(df['chard'] &gt; 0) & (df['chard'] &lt; 2000)]\nsns.violinplot(data=filtered_df, x='group', y='chard', palette='viridis', ax=axs[1, 0])\naxs[1, 0].set_xlabel('Chardonnay')\naxs[1, 0].set_ylabel('Customers')\naxs[1, 0].set_title('Violinplot of Chardonnay Purchases')\n\n# Plot histogram for 'sav_blanc'\nfiltered_df = df[(df['sav_blanc'] &gt; 0) & (df['sav_blanc'] &lt; 2000)]\nsns.violinplot(data=filtered_df, x='group', y='sav_blanc', palette='viridis', ax=axs[1, 1])\naxs[1, 1].set_xlabel('Sauvignon Blanc')\naxs[1, 1].set_ylabel('Customers')\naxs[1, 1].set_title('Violinplot of Sauvignon Blanc Purchases')\n\n# Plot histogram for 'syrah'\nfiltered_df = df[(df['syrah'] &gt; 0) & (df['syrah'] &lt; 1000)]\nsns.violinplot(data=filtered_df, x='group', y='syrah', palette='viridis', ax=axs[1, 2])\naxs[1, 2].set_xlabel('Syrah')\naxs[1, 2].set_ylabel('Customers')\naxs[1, 2].set_title('Violinplot of Syrah Purchases')\n\n# Plot histogram for 'cab'\nfiltered_df = df[(df['cab'] &gt; 0) & (df['cab'] &lt; 500)]\nsns.violinplot(data=filtered_df, x='group', y='cab', palette='viridis', ax=axs[2, 0])\naxs[2, 0].set_xlabel('Cabernet')\naxs[2, 0].set_ylabel('Customers')\naxs[2, 0].set_title('Violinplot of Cabernet Purchases')\n\n# Hide the empty subplots\naxs[2, 1].axis('off')\naxs[2, 2].axis('off')\n\n# Adjust layout\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nFigure 1: Violinplot of Baseline Variables\n\n\n\n\n\nThe shape of the distributions across group is quite similar, indicating comparable distributions of baseline variables. This similarity confirms that the randomization process was successful in achieving balance between the treatment and control groups."
  },
  {
    "objectID": "posts/ab_testing1/1_ab_testing.html#histograms",
    "href": "posts/ab_testing1/1_ab_testing.html#histograms",
    "title": "A/B Testing Analysis",
    "section": "1.2. Histograms",
    "text": "1.2. Histograms\nAnother intuitive way to plot a distribution is the histogram. The histogram groups the data into equally wide bins and plots the number of observations within each bin.\nThe full distributions of baseline variables should also be the same between treatment groups in this case:\n\n\nCode\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Create subplots\nfig, axs = plt.subplots(3, 3, figsize=(18, 12))\n\n# Plot histogram for 'days_since'\nsns.histplot(data=df, x='days_since', hue='group', binwidth=25, stat='density', common_norm=False, alpha=0.8, multiple='stack', palette='viridis', ax=axs[0, 0])\naxs[0, 0].set_xlim(0, 700)\naxs[0, 0].set_xlabel('Days Since Last Activity')\naxs[0, 0].set_ylabel('Customers')\naxs[0, 0].set_title('Distribution of days_since by treatment group')\n\n# Plot histogram for 'visits'\nfiltered_df = df[df['visits'] &gt; 0]\nsns.histplot(data=filtered_df, x='visits', hue='group', binwidth=1, stat='density', common_norm=False, alpha=0.8, multiple='stack', palette='viridis', ax=axs[0, 1])\naxs[0, 1].set_xlim(0, 35)\naxs[0, 1].set_xlabel('Visits')\naxs[0, 1].set_ylabel('Customers')\naxs[0, 1].set_title('Distribution of website visits by treatment group')\n\n# Plot histogram for 'past_purch'\nfiltered_df = df[df['past_purch'] &gt; 0]\nsns.histplot(data=filtered_df, x='past_purch', hue='group', binwidth=25, stat='density', common_norm=False, alpha=0.8, multiple='stack', palette='viridis', ax=axs[0, 2])\naxs[0, 2].set_xlim(0, 2000)\naxs[0, 2].set_xlabel('Past Purchases ($)')\naxs[0, 2].set_ylabel('Customers')\naxs[0, 2].set_title('Distribution of past purchases by treatment group')\n\n# Plot histogram for 'chard'\nfiltered_df = df[df['chard'] &gt; 0]\nsns.histplot(data=filtered_df, x='chard', hue='group', binwidth=25, stat='density', common_norm=False, alpha=0.8, multiple='stack', palette='viridis', ax=axs[1, 0])\naxs[1, 0].set_xlim(0, 2000)\naxs[1, 0].set_xlabel('Chardonnay')\naxs[1, 0].set_ylabel('Customers')\naxs[1, 0].set_title('Distribution of Chardonnay by treatment group')\n\n# Plot histogram for 'sav_blanc'\nfiltered_df = df[df['sav_blanc'] &gt; 0]\nsns.histplot(data=filtered_df, x='sav_blanc', hue='group', binwidth=25, stat='density', common_norm=False, alpha=0.8, multiple='stack', palette='viridis', ax=axs[1, 1])\naxs[1, 1].set_xlim(0, 2000)\naxs[1, 1].set_xlabel('Sauvignon Blanc')\naxs[1, 1].set_ylabel('Customers')\naxs[1, 1].set_title('Distribution of Sauvignon Blanc by treatment group')\n\n# Plot histogram for 'syrah'\nfiltered_df = df[df['syrah'] &gt; 0]\nsns.histplot(data=filtered_df, x='syrah', hue='group', binwidth=25, stat='density', common_norm=False, alpha=0.8, multiple='stack', palette='viridis', ax=axs[1, 2])\naxs[1, 2].set_xlim(0, 1000)\naxs[1, 2].set_xlabel('Syrah')\naxs[1, 2].set_ylabel('Customers')\naxs[1, 2].set_title('Distribution of Syrah by treatment group')\n\n# Plot histogram for 'cab'\nfiltered_df = df[df['cab'] &gt; 0]\nsns.histplot(data=filtered_df, x='cab', hue='group', binwidth=25, stat='density', common_norm=False, alpha=0.8, multiple='stack', palette='viridis', ax=axs[2, 0])\naxs[2, 0].set_xlim(0, 500)\naxs[2, 0].set_xlabel('Cabernet')\naxs[2, 0].set_ylabel('Customers')\naxs[2, 0].set_title('Distribution of Cabernet by treatment group')\n\n# Hide empty subplots\nfor ax in axs.flat[7:]:\n    ax.axis('off')\n\n# Adjust layout\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nFigure 2: Histograms of Baseline Variables\n\n\n\n\n\nRandomization checks out again!"
  },
  {
    "objectID": "posts/ab_testing1/1_ab_testing.html#t-tests",
    "href": "posts/ab_testing1/1_ab_testing.html#t-tests",
    "title": "A/B Testing Analysis",
    "section": "1.3. T-Tests",
    "text": "1.3. T-Tests\nNow that we’ve visualized differences between distributions, let’s move on to a more rigorous approach for assessing statistical significance. Visualization provides intuition by allowing us to visually inspect differences, but we also want to be able to quantify the significance of these differences.\nThe most common test is the student t-test. T-tests are generally used to compare means. In this case, we want to test whether the means of the days_since, visits and past_purch distribution are the same across two groups: Treated (Email A + Email B) vs. Control. This test statistic is given by:\n\nstat = \\frac{|\\bar x_1 - \\bar x_2|}{\\sqrt{s^2 / n }}\n\nWhere \\bar x is the sample mean and s is the sample standard deviation. Under mild conditions, the test statistic is asymptotically distributed as a student t distribution.\nWe use the ttest_ind function from scipy to perform the t-test. The function returns both the test statistic and the implied p-value.\n\nfrom scipy.stats import ttest_ind\n\n# Treated vs Control Categorical Variable\ndf['treat'] = df['group'].map({'email_A': 'treated', 'email_B': 'treated', 'ctrl': 'control'})\n\n## Days Since\ndays_since_treated = df.loc[df.treat=='treated', 'days_since'].values\ndays_since_ctrl = df.loc[df.treat=='control', 'days_since'].values\n\nstat, p_value = ttest_ind(days_since_treated, days_since_ctrl)\nprint(f\"days_since t-test: statistic={stat:.4f}, p-value={p_value:.4f}\")\n\ndays_since t-test: statistic=0.0154, p-value=0.9877\n\n\n\n\nvisits t-test: statistic=-0.2451, p-value=0.8064\npast_purch t-test: statistic=0.4345, p-value=0.6639\n\n\nThe p-values from these tests all are above 0.1, therefore we do not reject the null hypothesis of no difference in means across treatment and control groups.\nThe thing is, our example has more than 2 groups to compare. With multiple groups, the most popular test is the F-test. The F-test compares the variance of a variable across different groups. The F-test statistic is\n\n\\text{stat} = \\frac{\\text{between-group variance}}{\\text{within-group variance}} = \\frac{\\sum_{g} \\big( \\bar x_g - \\bar x \\big) / (G-1)}{\\sum_{g} \\sum_{i \\in g} \\big( \\bar x_i - \\bar x_g \\big) / (N-G)}\n\nWhere G is the number of groups, N is the number of observations, \\bar x is the overall mean and \\bar x_g is the mean within group g. Under the null hypothesis of group independence, the f-statistic is F-distributed.\n\nfrom scipy.stats import f_oneway\n\n# Days Since\ndays_since_A = df.loc[df.group=='email_A', 'days_since'].values\ndays_since_B = df.loc[df.group=='email_B', 'days_since'].values\ndays_since_ctrl = df.loc[df.group=='ctrl', 'days_since'].values\n\nstat, p_value = f_oneway(days_since_A, days_since_B, days_since_ctrl)\nprint(f\"days_since F-statistic={stat:.4f}, p-value={p_value:.4f}\")\n\ndays_since F-statistic=0.1642, p-value=0.8485\n\n\n\n\nvisits F-statistic=0.0336, p-value=0.9670\npast_purch F-statistic=0.2618, p-value=0.7697\n\n\nHere again, the p-values are telling us that these distributions across treatment are likely similar."
  },
  {
    "objectID": "posts/ab_testing1/1_ab_testing.html#standardized-mean-difference",
    "href": "posts/ab_testing1/1_ab_testing.html#standardized-mean-difference",
    "title": "A/B Testing Analysis",
    "section": "1.4. Standardized Mean Difference",
    "text": "1.4. Standardized Mean Difference\nIn general, it is good practice to always perform a test for difference in means on all variables across the treatment and control group, when we are running A/B tests. However, since the denominator of the t-test statistic depends on the sample size, the corresponding p-values are not directly comparable across studies.\nAs alternative, we can use the standardized mean difference (SMD), which is just a standardized difference, which can be computed as:\n\nSMD = \\frac{|\\bar x_1 - \\bar x_2|}{\\sqrt{(s^2_1 + s^2_2) / 2}}\n\nUsually a value below 0.1 is considered a “small” difference.\nIt is good practice to collect average values of all variables across treatment and control group and a measure of distance between the two — either the t-test or the SMD — into a table that is called balance table.\nLet’s use the create_table_one function from the causalml library to generate it.\n\n\nCode\nimport numpy as np\nimport pandas as pd\n\ndef smd(feature, treatment):\n    \"\"\"Calculate the standard mean difference (SMD) of a feature between the\n    treatment and control groups.\n\n    Args:\n        feature (pandas.Series): a column of a feature to calculate SMD for\n        treatment (pandas.Series): a column that indicate whether a row is in\n                                   the treatment group or not\n\n    Returns:\n        (float): The SMD of the feature\n    \"\"\"\n    t = feature[treatment == 1]\n    c = feature[treatment == 0]\n    return (t.mean() - c.mean()) / np.sqrt(0.5 * (t.var() + c.var()))\n\n\n\ndef create_table_one(data, treatment_col, features):\n    \"\"\"Report balance in input features between the treatment and control groups.\n\n    Args:\n        data (pandas.DataFrame): total or matched sample data\n        treatment_col (str): the column name for the treatment\n        features (list of str): the column names of features\n\n    Returns:\n        (pandas.DataFrame): A table with the means and standard deviations in\n            the treatment and control groups, and the SMD between two groups\n            for the features.\n    \"\"\"\n    t1 = pd.pivot_table(\n        data[features + [treatment_col]],\n        columns=treatment_col,\n        aggfunc=[lambda x: \"{:.2f} ({:.2f})\".format(x.mean(), x.std())],\n    )\n    t1.columns = t1.columns.droplevel(level=0)\n    t1[\"SMD\"] = data[features].apply(lambda x: smd(x, data[treatment_col])).round(4)\n\n    n_row = pd.pivot_table(\n        data[[features[0], treatment_col]], columns=treatment_col, aggfunc=[\"count\"]\n    )\n    n_row.columns = n_row.columns.droplevel(level=0)\n    n_row[\"SMD\"] = \"\"\n    n_row.index = [\"n\"]\n\n    t1 = pd.concat([n_row, t1], axis=0)\n    t1.columns.name = \"\"\n    t1.columns = [\"Control\", \"Treatment\", \"SMD\"]\n    t1.index.name = \"Variable\"\n\n    return t1\n\n\n\ndf['treat'] = df['group'].map({'email_A': 1, 'email_B': 1, 'ctrl': 0})\n\ncreate_table_one(df, 'treat', ['days_since', 'visits', 'past_purch', 'chard', 'sav_blanc', 'syrah', 'cab'])\n\n\n\nTable 3: Balance Table\n\n\n\n\n\n\n\n\n\n\n\nControl\nTreatment\nSMD\n\n\nVariable\n\n\n\n\n\n\n\nn\n41330\n82658\n\n\n\ncab\n16.52 (47.27)\n16.26 (46.67)\n-0.0053\n\n\nchard\n71.67 (199.39)\n74.14 (211.84)\n0.012\n\n\ndays_since\n89.98 (89.50)\n89.99 (89.95)\n0.0001\n\n\npast_purch\n188.27 (298.18)\n189.06 (303.33)\n0.0026\n\n\nsav_blanc\n73.63 (203.37)\n71.87 (197.25)\n-0.0088\n\n\nsyrah\n26.45 (73.91)\n26.79 (75.04)\n0.0045\n\n\nvisits\n5.95 (2.85)\n5.94 (2.86)\n-0.0015\n\n\n\n\n\n\n\n\n\n\n\nIn the first two columns of Table 3, we observe the average of the different variables across the treatment and control groups, with standard errors provided in parentheses. However, it’s in the last column where the values of the standardized mean difference (SMD) are presented. A standardized difference below |0.1| for all variables suggests that the two groups are likely similar."
  },
  {
    "objectID": "posts/ab_testing1/1_ab_testing.html#difference-in-means",
    "href": "posts/ab_testing1/1_ab_testing.html#difference-in-means",
    "title": "A/B Testing Analysis",
    "section": "2.1. Difference in Means",
    "text": "2.1. Difference in Means\nLet’s start with the first causal question (Treated vs Control). We can just compare the average outcome post-treatment Y_1 across control and treated units and randomization guarantees that this difference is, on average, due to the treatment alone:\n\n\\widehat {ATE}^{simple} = \\bar Y_{t=1, d=1} - \\bar Y_{t=1, d=0}\n\nIn our case, we compute the average purchases (purch) after the campaign in the treatment group, minus the average revenue post ad campaign in the control group:\n\n\nCode\nate = df.loc[df.treat == 'treated', 'purch'].mean() - df.loc[df.treat == 'control', 'purch'].mean()\nn_treat = df[df.treat == 'treated'].shape[0]\n\nprint( f'Average Treatment Effects: ${ate.round(2)}' )\nprint( f'Average Increased Revenues: ${round(ate*n_treat):,.0f}' )\n\n\nAverage Treatment Effects: $13.32\nAverage Increased Revenues: $1,101,358\n\n\nThis \\text{ATE} is telling us that, on average, each recipient of the email campaign resulted in an additional revenue of $13.32 compared to those who did not receive the campaign. In other words, the email campaign had a positive impact on increasing purchases.\nThis suggests that, overall, the email campaign resulted in an increased revenues of $1,101,358."
  },
  {
    "objectID": "posts/ab_testing1/1_ab_testing.html#linear-regression",
    "href": "posts/ab_testing1/1_ab_testing.html#linear-regression",
    "title": "A/B Testing Analysis",
    "section": "2.2. Linear Regression",
    "text": "2.2. Linear Regression\nWe can obtain the same estimate by regressing the post-treatment outcome purch on the treatment indicator treat:\n\nY_{i} = \\alpha + \\beta D_i + \\varepsilon_i\n\nwhere D_i is the treatment indicator (equal to 1 for treated units and 0 for control units), \\alpha is the intercept term and \\beta is our coefficient of interest representing the Average Treatment Effect.\n\nimport statsmodels.formula.api as smf\n\nsmf.ols('purch ~ treat', data=df).fit().summary().tables[1]\n\n\n\nTable 6: A/B Regression Results I\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n12.4203\n0.268\n46.360\n0.000\n11.895\n12.945\n\n\ntreat[T.treated]\n13.3243\n0.328\n40.608\n0.000\n12.681\n13.967\n\n\n\n\n\n\n\n\n\nThe advantage of running a linear regression is that we can directly test whether the difference in outcomes between the treatment and control groups is statistically significant, providing us with more robust information regarding the effectiveness of the campaign.\nTable 6 is not only confirming an \\text{ATE} of $13.32, but also telling us that this difference is statistically different from zero. This provides stronger evidence regarding the effectiveness of the email campaign in increasing purchases compared to the control group.\nWe can also compare both types of campaigns against our control group:\n\nsmf.ols('purch ~ group', data=df).fit().summary().tables[1]\n\n\n\nTable 7: A/B Regression Results II\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n12.4203\n0.268\n46.360\n0.000\n11.895\n12.945\n\n\ngroup[T.email_A]\n13.2026\n0.379\n34.846\n0.000\n12.460\n13.945\n\n\ngroup[T.email_B]\n13.4460\n0.379\n35.488\n0.000\n12.703\n14.189\n\n\n\n\n\n\n\n\n\nIt’s important to note that this regression results in Table 7 are not directly addressing our second causal question (Email A vs. Email B). Instead, the coefficients obtained from the regression are comparing each email campaign against the control group. In any case, the table suggests that, on average, Campaign B generates more purchases. It remains to be determined whether this difference is statistically significant."
  },
  {
    "objectID": "posts/ab_testing1/1_ab_testing.html#distribution-comparison",
    "href": "posts/ab_testing1/1_ab_testing.html#distribution-comparison",
    "title": "A/B Testing Analysis",
    "section": "2.3. Distribution Comparison",
    "text": "2.3. Distribution Comparison\nFigure 3 illustrates the purchase distribution by treatment, providing a comprehensive view of the data. However, it’s important to note that while we have established a statistically significant difference, our estimation of this distribution assumes a Normal Distribution, which may not always hold true. In future articles, we will delve deeper into alternative methods that do not rely on such assumptions, ensuring a more robust analysis.\n\n\nCode\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Control vs Treated\ndf_control = df[df['treat'] == 'control']\ndf_treated = df[df['treat'] == 'treated']\n\n# Calculate mean and standard error for email_A and email_B\nmean_open_A = df_control['purch'].mean()\nstd_error_A = df_control['purch'].std() / np.sqrt(len(df_control))\n\nmean_open_B = df_treated['purch'].mean()\nstd_error_B = df_treated['purch'].std() / np.sqrt(len(df_treated))\n\nnormal_data_A = np.random.normal(mean_open_A, std_error_A, 10000)\nnormal_data_B = np.random.normal(mean_open_B, std_error_B, 10000)\n\nfig, axs = plt.subplots(1, 1)\n\nsns.histplot(normal_data_A, kde=True, stat='density', label='Control', bins=20, palette='viridis')\nsns.histplot(normal_data_B, kde=True, stat='density', label='Treated', bins=20, palette='viridis')\n\nplt.xlabel('Purchases')\nplt.ylabel('Density')\nplt.title('Distribution of Purchases, by Treatment')\nplt.axvline(mean_open_A, color=sns.color_palette()[0], linestyle='--', label='Mean Control')\nplt.axvline(mean_open_B, color=sns.color_palette()[1], linestyle='--', label='Mean Treated')\nplt.errorbar(mean_open_A, 0.02, xerr=std_error_A, fmt='o', color=sns.color_palette()[0], label='95% CI Control')\nplt.errorbar(mean_open_B, 0.02, xerr=std_error_B, fmt='o', color=sns.color_palette()[1], label='95% CI Treated')\nplt.legend(ncol=3, bbox_to_anchor=(0.5, -0.15), loc='upper center') \n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nFigure 3: Purchase Distribution, by Treatment\n\n\n\n\n\nAgain, assuming Normality, we can also estimate confidence intervals:\n\n\nCode\nimport scipy.stats as stats\n\ndf_control = df[df['treat'] == 'control']\ndf_treated = df[df['treat'] == 'treated']\n        \n# Calculate mean and standard error for email_A and email_B\nmean_open_A = df_control['purch'].mean()\nstd_error_A = df_control['purch'].std() / np.sqrt(len(df_control))\n\nmean_open_B = df_treated['purch'].mean()\nstd_error_B = df_treated['purch'].std() / np.sqrt(len(df_treated))\n    \n# Calculate the confidence interval using the t-distribution\nconfidence_interval_A = stats.norm.interval(0.95, loc=mean_open_A, scale=std_error_A)\nconfidence_interval_B = stats.norm.interval(0.95, loc=mean_open_B, scale=std_error_B)\n\n\n\n\nMean Response Control: 12.4\nMean Response Treated: 25.7\n95% Confidence Interval for Control: [12.00, 12.84]\n95% Confidence Interval for Treated: [25.34, 26.15]"
  },
  {
    "objectID": "posts/ab_testing1/1_ab_testing.html#email-a-vs-email-b",
    "href": "posts/ab_testing1/1_ab_testing.html#email-a-vs-email-b",
    "title": "A/B Testing Analysis",
    "section": "2.4. Email A vs Email B",
    "text": "2.4. Email A vs Email B\nNow, let’s turn our attention to the second causal question: Which email design, Email A or Email B, is more effective in driving engagement and sales?\n\n\nCode\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef plot_variable_comparison(df):\n    variables = ['open', 'click', 'purch']\n\n    fig, axs = plt.subplots(1, 3, figsize=(18, 6))\n\n    for i, variable in enumerate(variables):\n        # Control vs Treated\n        df_control = df[df['group'] == 'email_A']\n        df_treated = df[df['group'] == 'email_B']\n\n        # Calculate mean and standard error for control and treated groups\n        mean_control = df_control[variable].mean()\n        std_error_control = df_control[variable].std() / np.sqrt(len(df_control))\n        mean_treated = df_treated[variable].mean()\n        std_error_treated = df_treated[variable].std() / np.sqrt(len(df_treated))\n\n        # Generate normal distribution data for control and treated groups\n        normal_data_control = np.random.normal(mean_control, std_error_control, 10000)\n        normal_data_treated = np.random.normal(mean_treated, std_error_treated, 10000)\n\n        # Plot histograms for control and treated groups\n        sns.histplot(normal_data_control, kde=True, stat='density', label='Control', bins=20, palette='viridis', ax=axs[i])\n        sns.histplot(normal_data_treated, kde=True, stat='density', label='Treated', bins=20, palette='viridis', ax=axs[i])\n\n        # Set labels and title\n        axs[i].set_xlabel(f'{variable.capitalize()}')\n        axs[i].set_ylabel('Density')\n        axs[i].set_title(f'Distribution of {variable.capitalize()}, by Treatment')\n\n        # Plot mean and error bars\n        axs[i].axvline(mean_control, color=sns.color_palette()[0], linestyle='--', label='Mean Control')\n        axs[i].axvline(mean_treated, color=sns.color_palette()[1], linestyle='--', label='Mean Treated')\n        axs[i].errorbar(mean_control, 0.02, xerr=std_error_control, fmt='o', color=sns.color_palette()[0], label='95% CI Control')\n        axs[i].errorbar(mean_treated, 0.02, xerr=std_error_treated, fmt='o', color=sns.color_palette()[1], label='95% CI Treated')\n\n        # Add legend\n        #axs[i].legend(ncol=3, bbox_to_anchor=(0.5, -0.15), loc='upper center') \n\n    #\n    handles, labels = axs[i].get_legend_handles_labels()\n    fig.legend(handles, labels, loc='upper center', ncol=3, bbox_to_anchor=(0.5, 0.0))\n    \n    # Adjust layout\n    plt.tight_layout()\n    plt.show()\n\n\n# Example usage:\nplot_variable_comparison(df)\n\n\n\n\n\n\n\n\nFigure 4: Outcome Distribution, by AB Treatment\n\n\n\n\n\n\n\nCode\nimport scipy.stats as stats\n\ndf_email_A = df[df['group'] == 'email_A']\ndf_email_B = df[df['group'] == 'email_B']\n        \n# Calculate mean and standard error for email_A and email_B\nmean_open_A = df_email_A['purch'].mean()\nstd_error_A = df_email_A['purch'].std() / np.sqrt(len(df_email_A))\n\nmean_open_B = df_email_B['purch'].mean()\nstd_error_B = df_email_B['purch'].std() / np.sqrt(len(df_email_B))\n    \n# Calculate the confidence interval using the t-distribution\nconfidence_interval_A = stats.norm.interval(0.95, loc=mean_open_A, scale=std_error_A)\nconfidence_interval_B = stats.norm.interval(0.95, loc=mean_open_B, scale=std_error_B)\n\n\n\n\nMean Response email_A: 25.6\nMean Response email_B: 25.9\n95% Confidence Interval for email_A: [25.06, 26.19]\n95% Confidence Interval for email_B: [25.29, 26.44]\n\n\nInterestingly enough, Email A appears to outperform Email B in terms of opens and clicks (Figure 4). However, this level of engagement does not necessarily translate to higher purchase rates. Purchases from Email B are higher on average, but the large overlap in their distribution is telling us that this difference might not be significant. While purchases from Email B are higher on average, the large overlap in their distribution suggests that this difference might not be significant.\nTo confirm this conclusion, we can conduct a linear regression comparing Email A and Email B purchases:\n\n\nCode\ndf_ab = df[df['group'] != 'ctrl']\nsmf.ols('purch ~ group', data=df_ab).fit().summary().tables[1]\n\n\n\n\nTable 8: A/B Regression Results III\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n25.6228\n0.291\n88.070\n0.000\n25.053\n26.193\n\n\ngroup[T.email_B]\n0.2435\n0.411\n0.592\n0.554\n-0.563\n1.050\n\n\n\n\n\n\n\n\n\nIndeed, despite observing higher purchases associated with Email B, the obtained p-value of 0.554 indicates that this difference is not statistically significant.\nThese findings show how important it is to look at different metrics and really dig into the data to figure out what’s working best for engaging customers and boosting sales."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "👋 Hey, I’m Daniel",
    "section": "",
    "text": "I am passionate about the intersection between Economics and Data Science. Experienced in Competition Economics and Quantitative Research. Currently working as a Junior Data Consultant at FactX, where I engage in diverse projects across different industries. Oriented toward analytical and interdisciplinary work, with high learning opportunities.\n \n  \n   \n  \n    \n     Twitter\n  \n  \n    \n     LinkedIn\n  \n  \n    \n     Github"
  },
  {
    "objectID": "index.html#latest-posts",
    "href": "index.html#latest-posts",
    "title": "👋 Hey, I’m Daniel",
    "section": "Latest Posts",
    "text": "Latest Posts\nClick here to check out more posts.\n\n\n\n\n\n\n\n\n\n\nRetail Gasoline Merger in Chile: An Ex-Post Merger Evaluation\n\n\n\n\n\n\nDaniel Redel\n\n\nApr 22, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA/B Testing Analysis\n\n\n\n\n\n\nDaniel Redel\n\n\nApr 5, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNonparametric & Semiparametric Binary-Choice Regressions\n\n\n\n\n\n\nDaniel Redel\n\n\nDec 11, 2022\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "I am passionate about the intersection between Economics and Data Science. Experienced in Competition Economics and Quantitative Research. Currently working as a Junior Data Consultant at FactX, where I engage in diverse projects across different industries. Oriented toward analytical and interdisciplinary work, with high learning opportunities.\n \n  \n   \n  \n    \n     Twitter\n  \n  \n    \n     LinkedIn\n  \n  \n    \n     Github"
  },
  {
    "objectID": "about.html#skills",
    "href": "about.html#skills",
    "title": "About Me",
    "section": "Skills",
    "text": "Skills"
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "About Me",
    "section": "Education",
    "text": "Education\n\n\nMSc. in Econometrics & Mathematical Economics\nTilburg University\n\n\nJan 2024\n\n\n\n\nMSc. in Applied Economics\nPUC Chile\n\n\n2021\n\n\n\n\nBSc. in Economics & Business Administration\nPUC Chile\n\n\n2020"
  },
  {
    "objectID": "about.html#experience",
    "href": "about.html#experience",
    "title": "About Me",
    "section": "Experience",
    "text": "Experience\n\n\nJunior Data Consultant\nFactX, Utrecht\n\n\nMarch 2024 – present\n\n\n\n\nThesis Internship\nCPB Netherlands Bureau for Economic Policy Analysis, The Hague\n\n\nMay 2023 - Oct 2023\n\n\n\n\nResearch Analyst\nCentroCompetencia, Santiago\n\n\nJul 2021 - Apr 2023"
  },
  {
    "objectID": "myposts.html",
    "href": "myposts.html",
    "title": "Daniel Redel",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n\n\nCausal Inference\n\n\nCompetition & Antitrust\n\n\nDiff-in-Diff\n\n\nEconometrics\n\n\nSpatial Analysis\n\n\nGasoline Industry\n\n\n\n\n\n\n\nDaniel Redel\n\n\nApr 22, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCausal Inference\n\n\nA/B Testing\n\n\nPython\n\n\n\n\n\n\n\nDaniel Redel\n\n\nApr 5, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNonparametric Regression\n\n\nSemiparametric Regression\n\n\nBinary Choice Models\n\n\nKernel Density Estimation\n\n\n\n\n\n\n\nDaniel Redel\n\n\nDec 11, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npython\n\n\ncode\n\n\nanalysis\n\n\n\nMy first Jupyter Notebook\n\n\n\nDaniel Redel\n\n\nMay 22, 2021\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "myposts.html#my-posts",
    "href": "myposts.html#my-posts",
    "title": "Daniel Redel",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n\n\nCausal Inference\n\n\nCompetition & Antitrust\n\n\nDiff-in-Diff\n\n\nEconometrics\n\n\nSpatial Analysis\n\n\nGasoline Industry\n\n\n\n\n\n\n\nDaniel Redel\n\n\nApr 22, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCausal Inference\n\n\nA/B Testing\n\n\nPython\n\n\n\n\n\n\n\nDaniel Redel\n\n\nApr 5, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNonparametric Regression\n\n\nSemiparametric Regression\n\n\nBinary Choice Models\n\n\nKernel Density Estimation\n\n\n\n\n\n\n\nDaniel Redel\n\n\nDec 11, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npython\n\n\ncode\n\n\nanalysis\n\n\n\nMy first Jupyter Notebook\n\n\n\nDaniel Redel\n\n\nMay 22, 2021\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/gasoline_merger_did/merger-report.html",
    "href": "posts/gasoline_merger_did/merger-report.html",
    "title": "Retail Gasoline Merger in Chile: An Ex-Post Merger Evaluation",
    "section": "",
    "text": "Ex-post assessments of merger effects in Chilean’s retail gasoline market."
  },
  {
    "objectID": "posts/gasoline_merger_did/merger-report.html#identification-strategy",
    "href": "posts/gasoline_merger_did/merger-report.html#identification-strategy",
    "title": "Retail Gasoline Merger in Chile: An Ex-Post Merger Evaluation",
    "section": "4.1. Identification Strategy",
    "text": "4.1. Identification Strategy\nA simple before-after comparison will lead to biased estimates of the merger effects, given that observed price changes might stem from changes in demand or costs. Therefore, we aim to compare price changes around the merger to a counterfactual scenario in which no merger took place. Based on ex-post merger evaluation literature (Dafny et al. [2012]; Ashenfelter et al. [2015]; Argentesi et al. [2021]), I compare the geographical market in which both the acquirer (COPEC) and the target (CGL) operated in the pre-merger period to control markets that did not experience a change in market concentration.\nMy identification strategy relies on the expectation that competitive effects of the merger are likely to be stronger in the so-called overlap area between the merging parties than in non-overlap areas where the parties did not compete with each other door to door, since only in overlap areas did the intensity of competition change because of the merger. We thus can compare areas that experienced a change in market concentration (treated group) to markets without a pre-merger overlap (control group). Finally, this causal effect can be identified by employing a Difference-in-Differences (DiD) methodology that compares overlap and non-overlap areas to get the Average Treatment Effects on the Treated at the local level.\nAnother assumption that is implicitly here is the assumption that in retail gasoline markets competition works at the local level. Any comparison between treated and control areas will be able to identify merger treatment effects only if competition is, at least to some extent, local. Although in this article we will not provide evidence to support this, previous literature tend to suggest that retail gasoline market is local rather than national (see here and here).\n\n4.1.1. Geographical Relevant Market\nThe impact of COPEC’s acquisition on the market structure around each gas station is measured as the change in the number of independent competing brands within a specific radius. However, determining the appropriate radius size is a critical consideration, not to be underestimated.\nThe Chilean competition authority (FNE), in its merger report, defined the relevant geographical market as the entire Castro County/Commune. According to their assessment, this determination was based on jurisprudence, which considers the local or communal market due to the difficulties and costs consumers face in traveling between gas stations in terms of time, convenience, and fuel expenses. Additionally, international jurisprudence has also noted that retail operators typically operate in local markets, monitoring competitor prices within a relatively limited radius, typically around 3 miles (approximately 5 kilometers), or based on travel isochrones of 10 minutes for urban areas and 20 minutes for rural areas (Case No. ME/3933/08). Consequently, I will adopt a 5 km radius to delineate the geographical relevant markets.\n\n\nCode\nimport geopandas as gpd\nfrom shapely.geometry import Point\n\n# Filter the DataFrame for the unit with Id='co1020101'\nunit_co1020101 = df[df['Id'] == 'co1020101']\n\n# Get the unique latitude and longitude values\nunique_latitudes = unit_co1020101['Latitud'].unique()\nunique_longitudes = unit_co1020101['Longitud'].unique()\n\n# Target coordinates (longitude, latitude)\ntarget_crs = (unique_longitudes, unique_latitudes)\n\n# Convert DataFrame to GeoDataFrame\ngrouped_df = df.groupby('Id').agg({'Latitud': 'first', 'Longitud': 'first'}).reset_index()\nnew_df = pd.DataFrame(grouped_df)\ngdf_units = gpd.GeoDataFrame(new_df, geometry=gpd.points_from_xy(new_df.Longitud, new_df.Latitud), crs=\"EPSG:4326\")\n\n# Create a Shapely Point object for the target coordinates\ntarget_point = gpd.GeoDataFrame({'geometry': [Point(target_crs)]}, crs=\"EPSG:4326\")\n# Buffer the point to create a circle with a radius of 5000 meters\nbuffer_radius_deg = 5000 / 111000  # 1 degree of latitude is approximately 111000 meters\n\nbuffer_circle = target_point.buffer(buffer_radius_deg)\nbuffer_area = buffer_circle.to_crs(gdf_units.crs)\n\n# Check if each unit's coordinates fall within the buffer circle\ngdf_units['treat'] = gdf_units.geometry.within(buffer_area.iloc[0])\n\n# Convert treat column to integer (1 for True, 0 for False)\ngdf_units['treat'] = gdf_units['treat'].astype(int)\ngdf_units['Treated'] = np.where(gdf_units['treat'] == 1, 'Treated', 'Control')\ndf = pd.merge(df, gdf_units[['Id', 'Treated', 'treat']], on='Id', how='left')\n\n# Count treated and control units\ntreated_count = gdf_units['treat'].value_counts()[1]\ncontrol_count = gdf_units['treat'].value_counts()[0]\n\nprint(\"Treated units:\", treated_count)\nprint(\"Control units:\", control_count)\n\n\nTreated units: 6\nControl units: 150\n\n\nThis selection process resulted in a total of 6 service stations falling within the defined overlap area, thus constituting the treated group. The remaining 150 stations outside this area will serve as the control group. Figure 2 provides a visual and interactive representation of the spatial distribution of these stations:\n\n\nCode\nimport folium\nfrom folium.plugins import MarkerCluster\n\n# Filter GeoDataFrame by Region=='Castro'\ndf_lagos = df[df['Region'] == 'Los Lagos']\ndf_lagos = df_lagos.dropna(subset=['Latitud', 'Longitud'])\nprice_id = df_lagos.groupby('Id').agg({'Precio': 'mean', 'Latitud': 'first', 'Longitud': 'first'}).reset_index()\ngdf = gpd.GeoDataFrame(price_id, geometry=gpd.points_from_xy(price_id.Longitud, price_id.Latitud), crs=\"EPSG:4326\")\n\n# Create a map centered around Los Lagos region\ntarget_crs = [-42.470071, -73.76461]\nm = folium.Map(location=target_crs, zoom_start=11)\n\n# Create a MarkerCluster to add all the points\nmarker_cluster = MarkerCluster().add_to(m)\n\n# # Iterate over each row in the GeoDataFrame and add a marker to the map\nfor idx, row in gdf.iterrows():\n    folium.Marker(location=[row['Latitud'], row['Longitud']], popup=row['Id'], \n                  ).add_to(marker_cluster)\n\nradius = 5000\nfolium.Circle(\n    location=target_crs,\n    radius=radius,\n    color=\"black\",\n    weight=1,\n    fill_opacity=0.1,\n    opacity=1,\n    fill_color=\"blue\",\n    fill=False,  # gets overridden by fill_color\n    popup=\"{} meters\".format(radius),\n    tooltip=\"I am in meters\",\n).add_to(m)\n\n# Show\nm\n\n\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\nFigure 2: Map with Treated Service Stations\n\n\n\n\n\n\n4.1.2. Treatment Period\nRegarding the treatment period, it’s important to acknowledge that certain treated service stations ceased price postings during the interval between the merger approval (December 2016) and the beginning of operations under the re-branded identity (around January 2018). Consequently, observations within this transitional period are excluded from analysis, aligning the last pre-merger period with November 2016 and the first post-merger period with February 2018.\n\n\nCode\nimport pandas as pd\n\n# Convert 'date' column to datetime\ndf['date'] = pd.to_datetime(df['date'])\ndf['year_month'] = df['date'].dt.to_period('M')\n\n# Define the date thresholds\npost_date_1 = pd.to_datetime(\"2016-12-02\")\npost_date_2 = pd.to_datetime(\"2017-11-01\")\n\n# Create 'Post' variable\ndf['Post'] = (df['date'] &gt;= post_date_1).astype(int)\n\n# Create 'post1' variable\ndf['post1'] = ((df['date'] &gt;= post_date_1) & (df['date'] &lt; post_date_2)).astype(int)\n\n# Create 'post2' variable\ndf['post2'] = (df['date'] &gt;= post_date_2).astype(int)\n\n\n# sh1020101 comes back at 2017-11-30 or 11\n# pe1020101 comes back at 2018-02-22 or 14\n\n# Filter the DataFrame for Balanced Panel\ndf = df[ (df['date'] &gt;= pd.to_datetime(\"2018-02-22\")) | (df['date'] &lt;= pd.to_datetime(\"2016-12-02\"))  ]\n\n# Get unique dates\nunique_dates = sorted(set(df['date']))\n\n# Create DataFrame from unique dates\nunique_dates_df = pd.DataFrame({'date': unique_dates})\n\n# Add a new column containing numbers from 1 to the number of rows\nunique_dates_df['event_date'] = range(1 - 25 , len(unique_dates_df) + 1 - 25)\n\n# Merge with original DataFrame based on 'date' column\ndf = pd.merge(df, unique_dates_df, on='date', how='left')\n\n\n\n\nCode\n# Count occurrences of each Id\nid_counts = df['Id'].value_counts()\n\n# Check if it's a balanced panel\nbalanced_panel = id_counts.nunique() == 1\n\n# If not balanced, identify Id with different counts\nif not balanced_panel:\n    unbalanced_ids = id_counts[id_counts != id_counts.iloc[0]].index\n    \n    # Remove rows corresponding to unbalanced Id\n    df = df[~df['Id'].isin(unbalanced_ids)]\n\n# Print the balanced panel status and the number of rows after removing unbalanced Ids\nprint(\"Is it a balanced panel?\", balanced_panel)\nprint(\"Number of rows after removing unbalanced Ids:\", len(df))\n\n# Check if 'sh1020101' and 'pe1020101' are in the filtered DataFrame\ntreated_ids = ['sh1020101', 'pe1020101']\nremoved_ids = [id for id in treated_ids if id not in df['Id'].unique()]\n\n# Print the result\nif removed_ids:\n    print(\"The following treated Ids were removed:\", removed_ids)\nelse:\n    print(\"All treated Ids are still present in the DataFrame.\")\n\n\nIs it a balanced panel? False\nNumber of rows after removing unbalanced Ids: 18423\nAll treated Ids are still present in the DataFrame.\n\n\nFurthermore, to ensure a balanced panel dataset, additional stations with irregular posting behavior during the relevant study period are excluded. Following these adjustments, the dataset comprises 18,423 observations."
  },
  {
    "objectID": "posts/gasoline_merger_did/merger-report.html#summary-statistics",
    "href": "posts/gasoline_merger_did/merger-report.html#summary-statistics",
    "title": "Retail Gasoline Merger in Chile: An Ex-Post Merger Evaluation",
    "section": "4.2. Summary Statistics",
    "text": "4.2. Summary Statistics\nTable 2 and Table 3 offer summary statistics on both the treated and non-treated units within the dataset.\n\n\nCode\n# Create a new column 'Brand' based on conditions\ndf['Brand'] = np.where(df['Nombre Distribuidor'] == 'COPEC', 'COPEC', 'Branded Rivals')\ndf.loc[df['Id'] == 'sh1020101', 'Brand'] = 'Non Branded'\n\n# Pivot the dataframe with 'treat' as columns\nsummary_stats = df.groupby(['treat', 'Brand'])['Id'].nunique().reset_index(name='Id')\n\npivot_summary_stats = summary_stats.pivot_table(index='Brand', columns='treat', values='Id', fill_value=0)\n\n# Print the pivot table\npivot_summary_stats\n\n\n\n\nTable 2: Pre-Merger Sample of Service Stations\n\n\n\n\n\n\n\n\n\n\ntreat\n0\n1\n\n\nBrand\n\n\n\n\n\n\nBranded Rivals\n46\n3\n\n\nCOPEC\n37\n2\n\n\nNon Branded\n0\n1\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Pivot the dataframe to have 'Combustible' values as columns\navg_summary = df.groupby(['treat', 'Post', 'Combustible'])['Precio'].mean().reset_index()\npivot_avg_summary = avg_summary.pivot_table(index=['Combustible', 'treat'], columns='Post', values='Precio')\npivot_avg_summary.round(1)\n\n\n\n\nTable 3: Preliminary Summary Statistics\n\n\n\n\n\n\n\n\n\n\n\nPost\n0\n1\n\n\nCombustible\ntreat\n\n\n\n\n\n\nGasolina 93\n0\n727.1\n844.8\n\n\n1\n735.4\n858.8\n\n\nGasolina 97\n0\n805.8\n902.9\n\n\n1\n815.4\n918.2\n\n\nPetroleo Diesel\n0\n501.1\n622.8\n\n\n1\n515.3\n639.4\n\n\n\n\n\n\n\n\n\n\n\nFigure 3 illustrates the comparative trajectory of average gasoline prices per type in the treated area against those in non-treated areas.\n\n\nCode\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Plot the average price over time for each treatment group\nfig, axes = plt.subplots(3, 2, figsize=(15, 12))\n\n# Filter DataFrame for Combustible=='Petroleo Diesel'\nfiltered_df = df[df['Combustible'] == 'Gasolina 93']\n\n# Group by 'Treated' and 'date' and calculate the mean price\navg_price = filtered_df.groupby(['Treated', 'event_date'])['Precio'].mean().reset_index()\n\n# First subplot\nfor Treated, data in avg_price.groupby('Treated'):\n    axes[0,0].plot(data['event_date'], data['Precio'], label=Treated)\n\n# Add vertical lines\naxes[0,0].axvline(-0.5, linestyle='--', color='grey')\n\n# Set labels and title\naxes[0,0].set_xlabel('Fecha')\naxes[0,0].set_ylabel('Precio')\naxes[0,0].set_title('Average Price of Gasolina 93 over Time')\naxes[0,0].legend(title='Treatment', loc='best')\n\n# Second subplot\n# Pivot the dataframe to have treated and control groups as columns\npivot_avg_price = avg_price.pivot(index='event_date', columns='Treated', values='Precio')\n\n# Calculate the difference between treated and control\ndifference = pivot_avg_price['Treated'] - pivot_avg_price['Control']\n\n# Plot the difference over time\naxes[0,1].plot(difference.index, difference.values, color='blue')\n\n# Add vertical lines\naxes[0,1].axvline(-0.5, linestyle='--', color='grey')\n\n# Set labels and title\naxes[0,1].set_xlabel('Fecha')\naxes[0,1].set_ylabel('Price Difference (Treated - Control)')\naxes[0,1].set_title('Difference in Average Price of Gasolina 93 between Treated and Control')\n\n\n\n# Filter DataFrame for Combustible=='Gasolina 97'\nfiltered_df = df[df['Combustible'] == 'Gasolina 97']\n\n# Group by 'Treated' and 'date' and calculate the mean price\navg_price = filtered_df.groupby(['Treated', 'event_date'])['Precio'].mean().reset_index()\n\n# First subplot\nfor Treated, data in avg_price.groupby('Treated'):\n    axes[1,0].plot(data['event_date'], data['Precio'], label=Treated)\n\n# Add vertical lines\naxes[1,0].axvline(-0.5, linestyle='--', color='grey')\n\n# Set labels and title\naxes[1,0].set_xlabel('Fecha')\naxes[1,0].set_ylabel('Precio')\naxes[1,0].set_title('Average Price of Gasolina 97 over Time')\naxes[1,0].legend(title='Treatment', loc='best')\n\n# Second subplot\n# Pivot the dataframe to have treated and control groups as columns\npivot_avg_price = avg_price.pivot(index='event_date', columns='Treated', values='Precio')\n\n# Calculate the difference between treated and control\ndifference = pivot_avg_price['Treated'] - pivot_avg_price['Control']\n\n# Plot the difference over time\naxes[1,1].plot(difference.index, difference.values, color='blue')\n\n# Add vertical lines\naxes[1,1].axvline(-0.5, linestyle='--', color='grey')\n\n# Set labels and title\naxes[1,1].set_xlabel('Fecha')\naxes[1,1].set_ylabel('Price Difference (Treated - Control)')\naxes[1,1].set_title('Difference in Average Price of Gasolina 97 between Treated and Control')\n\n\n\n# Filter DataFrame for Combustible=='Petroleo Diesel'\nfiltered_df = df[df['Combustible'] == 'Petroleo Diesel']\n\n# Group by 'Treated' and 'date' and calculate the mean price\navg_price = filtered_df.groupby(['Treated', 'event_date'])['Precio'].mean().reset_index()\n\n# First subplot\nfor Treated, data in avg_price.groupby('Treated'):\n    axes[2,0].plot(data['event_date'], data['Precio'], label=Treated)\n\n# Add vertical lines\naxes[2,0].axvline(-0.5, linestyle='--', color='grey')\n\n# Set labels and title\naxes[2,0].set_xlabel('Fecha')\naxes[2,0].set_ylabel('Precio')\naxes[2,0].set_title('Average Price of Diesel over Time')\naxes[2,0].legend(title='Treatment', loc='best')\n\n# Second subplot\n# Pivot the dataframe to have treated and control groups as columns\npivot_avg_price = avg_price.pivot(index='event_date', columns='Treated', values='Precio')\n\n# Calculate the difference between treated and control\ndifference = pivot_avg_price['Treated'] - pivot_avg_price['Control']\n\n# Plot the difference over time\naxes[2,1].plot(difference.index, difference.values, color='blue')\n\n# Add vertical lines\naxes[2,1].axvline(-0.5, linestyle='--', color='grey')\n\n# Set labels and title\naxes[2,1].set_xlabel('Fecha')\naxes[2,1].set_ylabel('Price Difference (Treated - Control)')\naxes[2,1].set_title('Difference in Average Price of Diesel between Treated and Control')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nFigure 3: Price Evolution in Combustibles, by Treatment\n\n\n\n\n\nNotably, the market’s high price transparency is reflected in closely aligned pre-merger prices between treated and non-treated stations. However, this coherence is less pronounced in the case of Diesel, where price trends in time appear to diverge.\nNevertheless, the graphical assessment suggests that the crucial assumption of the Difference-in-Differences (DiD) methodology —the parallel trend assumption— appears to be upheld within our sample, particularly evident for Gasoline 93 and 97. Our event-study framework aims to further elucidate and quantify these patterns."
  },
  {
    "objectID": "posts/gasoline_merger_did/merger-report.html#empirical-model",
    "href": "posts/gasoline_merger_did/merger-report.html#empirical-model",
    "title": "Retail Gasoline Merger in Chile: An Ex-Post Merger Evaluation",
    "section": "4.3. Empirical Model",
    "text": "4.3. Empirical Model\nI examined the impact of the merger on retail gasoline prices within affected local markets using a difference-in-differences (DiD) framework that compares the price changes in a selection of stations that were located in overlap areas with the change in the same outcome variable in other stations from the non-overlap areas before and after the merger. My baseline specification takes the following form:\n\nP_{it} = \\beta \\times Treat_i\\times \\textbf{1}[t-t^*_0\\geq0] + \\gamma_t + \\lambda_i + \\varepsilon_{it}\n\nwhere the outcome variable, P_{it}, denotes the price of service station i in year-month t. The treatment variable, Treat_i, takes the value of 1 if station i is located within the 5 kilometers of radius distance towards the target firm. The post-period indicator variable, \\textbf{1}[t-t^*_0\\geq0], equals 1 if period t belongs to the post-merger period (i.e., after January 2018) whose starting period is $t^*_0 $ , and 0 otherwise. Year-month and unit fixed effects are captured by \\gamma_t and \\lambda_i, respectively. The coefficient of interest is \\beta, which measures the average treatment effect of the merger. It identifies the additional variation in prices experienced by the stations in overlap areas compared to the control stations after the merger took place.\nUnder certain assumptions, parameter \\beta capture the causal effect of the treatment on the outcome. In our context, these effects are primarily identified by comparing units in the treated areas and units in no-treated areas, before and after the merger phase. The crucial identifying assumption is the so-called parallel trends assumption of a DiD model: that absent the merger, the treated service stations would have experienced the same outcome trend as the control stations.\nIn addition, I also report estimates from a more flexible specification that allows the coefficient to vary by the relative periods after the merger, by estimating the following equation:\n\nP_{it} = \\sum_{h=T_1}^{T_0}(\\beta_h \\times Treat_i\\times \\textbf{1}[t-t^*_s\\geq0]) + \\gamma_t + \\lambda_i + \\varepsilon_{it}\n\nwhere T_0 and T_1 are the lowest and highest number of lags and leads, respectively, to consider surrounding the treatment period. Because I normalize the coefficient on the periods just prior to the merger announcement to zero (i.e., \\beta_{-1}=0), each coefficient of \\beta_h can be interpreted as the price change in treated stations relative to no-treated stations after h periods of the merger, with all of the \\beta_h’s being estimated relative to the omitted year (i.e., h=-1).\nUsing this flexible model has two advantages. First, I can visually test the key identifying assumption, which is the parallel-trends assumption: abstent the acquisition, the prices between overlap and non-overlap areas would have evolved in parallel. Although this assumption is fundamentally untestable, plotting the \\beta_h’s of pre-periods (i.e., \\beta_{-8} to \\beta_{-2}) can provide visual evidence. Second, this event-study allows the assessment of the time-evolving effects of the merger. This flexible specification enables me to capture such time-varying effects."
  },
  {
    "objectID": "posts/gasoline_merger_did/merger-report.html#two-way-fixed-effects-results",
    "href": "posts/gasoline_merger_did/merger-report.html#two-way-fixed-effects-results",
    "title": "Retail Gasoline Merger in Chile: An Ex-Post Merger Evaluation",
    "section": "5.1. Two-Way Fixed Effects Results",
    "text": "5.1. Two-Way Fixed Effects Results\nTable 4, Table 5 and Table 6 report the baseline results from estimating the specification described in Equation 1 on Gasoline 93, 97 and Diesel prices, respectively:\n\n\nCode\nimport pyfixest as pf\n\n# Filter by Gasolina 93\ndf93 = df[(df['Combustible'] == 'Gasolina 93')]\n\n# TWFE Estimation\nfeols1 = pf.feols(\"Precio ~ treat*Post | Id + date\", data=df93, vcov={\"CRV1\": \"Id\"})\nfeols1.tidy().reset_index()\n\n\n\n\nTable 4: TWFE Results (Gasoline 93)\n\n\n\n\n            \n            \n            \n\n\n\n\n\n\n\n\n\n\nCoefficient\nEstimate\nStd. Error\nt value\nPr(&gt;|t|)\n2.5%\n97.5%\n\n\n\n\n0\ntreat:Post\n5.747088\n0.595788\n9.646201\n1.998401e-15\n4.563085\n6.931091\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nimport pyfixest as pf\n\n# Filter by Gasolina 97\ndf97 = df[(df['Combustible'] == 'Gasolina 97')]\n\n# TWFE Estimation\nfeols1 = pf.feols(\"Precio ~ treat*Post | Id + date\", data=df97, vcov={\"CRV1\": \"Id\"})\nfeols1.tidy().reset_index()\n\n\n\n\nTable 5: TWFE Results (Gasoline 97)\n\n\n\n\n\n\n\n\n\n\n\nCoefficient\nEstimate\nStd. Error\nt value\nPr(&gt;|t|)\n2.5%\n97.5%\n\n\n\n\n0\ntreat:Post\n5.689687\n0.62263\n9.138146\n2.131628e-14\n4.45234\n6.927033\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nimport pyfixest as pf\n\n# Filter by Diesel\ndfdiesel = df[(df['Combustible'] == 'Petroleo Diesel')]\n\n# TWFE Estimation\nfeols1 = pf.feols(\"Precio ~ treat*Post | Id + date\", data=dfdiesel, vcov={\"CRV1\": \"Id\"})\nfeols1.tidy().reset_index()\n\n\n\n\nTable 6: TWFE Results (Diesel)\n\n\n\n\n\n\n\n\n\n\n\nCoefficient\nEstimate\nStd. Error\nt value\nPr(&gt;|t|)\n2.5%\n97.5%\n\n\n\n\n0\ntreat:Post\n2.496932\n0.971307\n2.570692\n0.01183\n0.566663\n4.427201\n\n\n\n\n\n\n\n\n\n\n\nThe estimated average effect on all these cases suggests that the merger caused a statistically significant average increase in price in overlap areas by around $5-6 Chilean pesos for the Gasoline 93 and 97 and around $2-3 for Diesel. These price effects are all below 1%."
  },
  {
    "objectID": "posts/gasoline_merger_did/merger-report.html#event-study-results",
    "href": "posts/gasoline_merger_did/merger-report.html#event-study-results",
    "title": "Retail Gasoline Merger in Chile: An Ex-Post Merger Evaluation",
    "section": "5.2. Event-Study Results",
    "text": "5.2. Event-Study Results\nI now estimate the event-study specification outlined in Equation 2 with the period before the merger as the reference period. Figure 4, Figure 5 and Figure 6 shows the the estimated coefficients (and the corresponding 95% confidence intervals) after controlling for observed characteristics of the TWFE model.\n\n\n\n            \n            \n            \n\n\n\n\nCode\ndf93 = df[(df['Combustible'] == 'Gasolina 93')]\n\nfeols_event_study = pf.feols(\n    \"Precio ~ i(event_date, treat, ref=-1) | Id + event_date\",\n    data=df93,\n    vcov={\"CRV1\": \"Id\"}\n)\n\n\nfeols_event_study.iplot(\n    coord_flip=False,\n    #title=\"TWFE-Estimator\",\n    figsize=[900, 400],\n    xintercept=22.5,\n    yintercept=0\n) + theme(legend_position='none') + scale_color_viridis(option=\"magma\", end=0.5)\n\n\n\n\n   \n   \n\n\nFigure 4: Event Study Results - Gasoline 93\n\n\n\n\n\n\nCode\ndf97 = df[(df['Combustible'] == 'Gasolina 97')]\n\nfeols_event_study = pf.feols(\n    \"Precio ~ i(event_date, treat, ref=-1) | Id + event_date\",\n    data=df97,\n    vcov={\"CRV1\": \"Id\"}\n)\n\nfeols_event_study.iplot(\n    coord_flip=False,\n    title=\"TWFE-Estimator\",\n    figsize=[900, 400],\n    xintercept=22.5,\n    yintercept=0,\n) + theme(legend_position='none') + scale_color_viridis(option=\"magma\")\n\n\n\n\n   \n   \n\n\nFigure 5: Event Study Results - Gasoline 97\n\n\n\n\n\n\nCode\ndfdiesel = df[(df['Combustible'] == 'Petroleo Diesel')]\n\nfeols_event_study = pf.feols(\n    \"Precio ~ i(event_date, treat, ref=-1) | Id + event_date\",\n    data=dfdiesel,\n    vcov={\"CRV1\": \"Id\"}\n)\n\nfeols_event_study.iplot(\n    coord_flip=False,\n    #title=\"TWFE-Estimator\",\n    figsize=[900, 400],\n    xintercept=22.5,\n    yintercept=0,\n) + theme(legend_position='none') + scale_color_viridis(option=\"magma\", begin=0.7)\n\n\n\n\n   \n   \n\n\nFigure 6: Event Study Results - Diesel\n\n\n\n\nThe visual evidence support that the parallel trend assumption holds, which is the main identifying assumption to interpret the effects as causal. In all models, the estimated coefficients for the pre-treatment period are close to zero and statistically insignificant. After the merger, I found that service station prices increased significantly, confirming my baseline regression results.\nIt’s worth noting that the results for Gasoline 93 are particularly clear in Figure 4, with treatment effects exhibiting persistence or stability post-merger. Conversely, in the case of Gasoline 97, the treatment effects tend to diminish after approximately 20 months. Similar downward trends are observed for diesel prices."
  },
  {
    "objectID": "posts/nonparametrics/nonparametrics.html",
    "href": "posts/nonparametrics/nonparametrics.html",
    "title": "Nonparametric & Semiparametric Binary-Choice Regressions",
    "section": "",
    "text": "Nonparametric Kernel Density, Nonparametric Regression and Binary-Choice Models.\nThe data set used in this assignment comes from Pindyck and Rubinfeld (1998) “Econometric Models and Economic Forecasts” and contains the following variables: a dummy whether children attend private school (private), number of years the family has been at the present residence (years), log of property tax (logptax), log of income (loginc), and whether one voted for an increase in property taxes (vote). There are two dependent variables of interest -private and vote- which can be modeled individually by two univariate probit models or jointly by one bivariate probit model, for instance.\nLink to PDF HERE."
  },
  {
    "objectID": "posts/nonparametrics/nonparametrics.html#kernel-density-estimation",
    "href": "posts/nonparametrics/nonparametrics.html#kernel-density-estimation",
    "title": "Nonparametric & Semiparametric Binary-Choice Regressions",
    "section": "Kernel Density Estimation",
    "text": "Kernel Density Estimation\nBefore running a binary choice model to explain the voting choice, we plot different Kernel Density estimates of loginc to check whether it follows approximately a normal distribution. The idea is to test visually if the normal distribution falls within the 95% confidence interval area of each kernel density estimate:\n\n\n\n\n\n\nFigure 1: Kernel Density Estimation of Log(Income)\n\n\n\nAll these Kernel Density estimators in Figure 1 use an alternative Epanechnikov Kernel function (this is the default in the kdens command in STATA). The differences among the estimator are explained by the different methods of choosing the Optimal Bandwidth \\(h^*\\) that minimizes the Mean Integrated Squared Errors (MISE)1. Comparing the kernel densities of the log income against the normal distribution, we observe that the normal distribution seems to not quite fit inside the confidence intervals in none of the kernel estimators. While the case of an optimal bandwidth of \\(h^*=0.188\\) in Figure 1.b. seems to present some evidence about normality, you can still see how the purple line appears outside the boundaries around the 9 to 9.5 value.\nBoth Plug-in methods (Figure 1.a. and 1.c.) use a lower Optimal Bandwidth (0.11 and 0.12, respectively), thus, leading to less bias but more variance in their estimates. In these cases, the normal distribution line appears outside the 95% confidence boundaries in several places. Overall, the visual test seems to reject the hypothesis that log incomes follow a normal distribution."
  },
  {
    "objectID": "posts/nonparametrics/nonparametrics.html#binary-choice-model-probit",
    "href": "posts/nonparametrics/nonparametrics.html#binary-choice-model-probit",
    "title": "Nonparametric & Semiparametric Binary-Choice Regressions",
    "section": "Binary Choice Model (probit)",
    "text": "Binary Choice Model (probit)\nTable 1 reports the results of different Binary choice models to predict voting behavior using years, logptax and loginc as independent variables:\n\n\n\nTable 1: Binary Choice Model Results\n\n\n\n\n\n\nColumn 1 in Table 1 presents the results of the probit model. Here we see a positive and statistically significant relationship between the level of log income and the probability of voting for higher property taxes. Also, once again we observe that property taxes have a negative effect on voting in favor of higher taxes of this kind. Finally, the number of years in the residence did not report a significant result."
  },
  {
    "objectID": "posts/nonparametrics/nonparametrics.html#normality-assumption",
    "href": "posts/nonparametrics/nonparametrics.html#normality-assumption",
    "title": "Nonparametric & Semiparametric Binary-Choice Regressions",
    "section": "Normality Assumption",
    "text": "Normality Assumption\nWe can use a visual test to check the normality assumption of the probit model estimated in Column 1. Recall that in the probit model, the probability that \\(y_i\\) takes on the value 1 is modeled as a nonlinear function of a linear combination of a set of independent variables:\n\\[\n\\Pr(y_i=1|x_i)  = \\Phi(x_i'\\beta)\n\\]\nwhere \\(\\Phi(\\cdot)\\) is assumed to be the standard normal cumulative distribution function (CDF). To check if this assumption is reasonable, we performed nonparametric estimations of \\(\\Pr(y_i=1|x_{i'\\beta})\\). The first estimator is a Nadaraya-Watson Regression (or constant constant estimator) that assumes a constant \\(m(x)=b_0(x_0)\\) around some neighborhood of \\(x_0\\):\n\\[\n\\hat{m}_h(x_0)=\\frac{\\sum^N_{i=1} K_x\\left(\\frac{x_0-x_i}{h} \\right)}{\\sum^N_{i=1} K_x\\left(\\frac{x_0-x_j}{h} \\right)}y_i\n\\]\nThe Nadaraya-Watson estimator is essentially a weighted local average of the observations (weights defined by a Kernel function) at some neighborhood defined by the choice of bandwidth \\(h\\). The second estimator is a Local Linear Regression that, instead of assuming a constant average, lets \\(m(x)\\) be linear in the neighborhood of \\(x_0\\). More concretely, the local linear regression minimizes with respect to \\(b_0(x_0)\\) and \\(b_1(x_0)\\):\n\\[\n\\sum^N_{i=1} K_x\\left(\\frac{x_i-x_0}{h} \\right)[y_i-b_0(x_0)-b_1(x_0)(x_i-x_0)]^2\n\\]\nBoth nonparametric regressions use the Epanechnikov Kernel function. Regarding the choice of bandw\n\n\n\nTable 2: Binary Choice Model Results\n\n\n\n\n\n\nidth \\(h^*\\), I use the command npregress that performs the Leave-One-Out Cross Validation method (LOOCV) to estimate the Optimal Bandwidth size2. Figure 2 compares these regressions against the probit estimation:\n\n\n\n\n\n\nFigure 2: Nonparametric Estimation of Het Probit\n\n\n\nIn general, we see that the normality assumption about the error term \\(\\varepsilon_i\\) in the probit model is actually not rejected, as the probit curve of the \\(\\Pr(y_i=1|x_i'\\beta)\\) tends to be within the 95% confidence intervals of both the Local Constant and Local Linear nonparametric estimators. This means that the standard probit model does not suffer from misspecification."
  },
  {
    "objectID": "posts/nonparametrics/nonparametrics.html#heteroscedastic-probit-model",
    "href": "posts/nonparametrics/nonparametrics.html#heteroscedastic-probit-model",
    "title": "Nonparametric & Semiparametric Binary-Choice Regressions",
    "section": "Heteroscedastic Probit Model",
    "text": "Heteroscedastic Probit Model\nColumn 2 of Table 2 presents the results of the heteroscedastic probit model. This specification still assumes that the error term of the latent model follows a normal distribution, but is more flexible in the sense that it no longer assumes its variance to be 1, but that it can vary as a function of some set of explanatory variables. In this case, we only use the variable years to model the variance:\n\\[\nV(\\varepsilon_i|x_i)=\\sigma^2_i=[\\exp(\\text{years}_{i}'\\gamma)]^2\n\\]\nwhere \\(\\varepsilon_i\\sim N[0,\\exp(\\text{years}_{i}'\\gamma)]\\). The probability of success is now represented by:\n\\[\n\\Pr(y_i=1|x_i)  = \\Phi\\left( \\frac{x_i'\\beta}{\\sigma_i} \\right) = \\Phi\\left( \\frac{x_i'\\beta}{\\exp(\\text{years}_{i}'\\gamma)} \\right)\n\\]\nHere we are primarily interested in testing whether \\(\\gamma\\neq 0\\), which seems to be the case as we see in Table 2 a statistically significant coefficient. Additionally, the likelihood-ratio test of heteroskedasticity, which tests the full model with heteroskedasticity against the full model without, is significant as \\(\\chi^2(1) = 11.55\\). Regarding the coefficients, we now see that the number of years in the residence impacts negatively the probability of voting for higher property taxes. The remaining variables are still significant and with the same sign relative to the standard probit model, but the coefficients are more pronounced."
  },
  {
    "objectID": "posts/nonparametrics/nonparametrics.html#nonparametric-regression",
    "href": "posts/nonparametrics/nonparametrics.html#nonparametric-regression",
    "title": "Nonparametric & Semiparametric Binary-Choice Regressions",
    "section": "Nonparametric Regression",
    "text": "Nonparametric Regression\nWe now test again the normality assumption of the heteroscedastic probit model by visually comparing its CDF with the two nonparametric alternatives already discussed:\n\n\n\n\n\n\nFigure 3: Nonparametric Estimation of Heteroscedastic Probit\n\n\n\nClearly, the heteroscedastic alternative -that assumes normality- highly deviates from both nonparametric estimations. Hence, it seems that the normality assumption in the hetprobit model is rejected. Furthermore, the probit link function is no longer smooth when compared to the standard probit. Modeling the variance as dependent on the explanatory variable years makes the prediction less linear, leading to a more noisy curve than smoothed."
  },
  {
    "objectID": "posts/nonparametrics/nonparametrics.html#klein-spady-semiparametric-regression",
    "href": "posts/nonparametrics/nonparametrics.html#klein-spady-semiparametric-regression",
    "title": "Nonparametric & Semiparametric Binary-Choice Regressions",
    "section": "Klein & Spady Semiparametric Regression",
    "text": "Klein & Spady Semiparametric Regression\nSemiparametric alternatives do not rely on the parametric assumptions about the shape of the error term distribution \\(\\varepsilon_i\\). The Klein & Spady method is a Single Index model with a \\(\\Pr(y_i|x_i)= F(x_i'\\beta)\\) structure that only depends on \\(x_i\\) through a single linear combination, \\(x_i'\\beta\\), whereas the function \\(F(\\cdot)\\) is left unspecified. The estimation is based on some kind of Maximum Likelihood Estimation:\n\\[\nL(\\beta,h)=\\sum^N_{i=1}(1-y_i)\\ln[1-\\hat{F}_{-i,n}(x_i'\\beta)]+\\sum^N_{i=1}y_i\\ln[\\hat{F}_{-i,n}(x_i'\\beta)]\n\\]\nSince \\(F(\\cdot)\\) is unknown, Klein & Spady suggest replacing it with a Leave-One-Out Nadaraya-Watson estimator \\(\\hat{F}_{-i,n}(\\cdot)\\). In order to guarantee point identification, the coefficient of one continuous variable is normalized to 1, which in our case will be the variable loginc. Additionally, in single index models, the function \\(F(\\cdot)\\) will include any location and level shift, so the vector \\(x_i\\) cannot include an intercept. Column 3 of Table 2 shows the results of this semiparametric regression. But because we want to make some comparisons between the different models so far presented, we will also need to scale normalize the previous models, as in Table 2 they are not directly comparable. Table 3 shows the results of each model after scale normalization:\n\n\n\nTable 3: Binary Choice Models: Normalized Results\n\n\n\n\n\n\nHere we see that the direction of the signs in every specification is the same: years and logptax are negatively associated with the probability of voting in favor of the tax reform. The coefficients of the property tax variable are to some degree similar across models, but there are important differences regarding the coefficient linked to the number of years in residence. First, the probit model did not show an effect statistically distinct from zero, whereas the other two models did show significant and more pronounced coefficients.\nBetween the heteroscedastic probit and the Klein & Spady regression, the hetprobit seems to report a more negative effect of years on the probability of voting for higher taxes. In order to understand the magnitude of these differences between models, we will need to estimate marginal effects."
  },
  {
    "objectID": "posts/nonparametrics/nonparametrics.html#semiparametric-regression-visualization",
    "href": "posts/nonparametrics/nonparametrics.html#semiparametric-regression-visualization",
    "title": "Nonparametric & Semiparametric Binary-Choice Regressions",
    "section": "Semiparametric Regression Visualization",
    "text": "Semiparametric Regression Visualization\nFigure 4 visualize the Klein & Spady semiparametric estimate of \\(\\Pr(y_i=1|x_i'\\beta)\\) and compares it against the heteroskedastic estimation:\n\n\n\n\n\n\nFigure 4: Semiparametric Estimation of Probits\n\n\n\nThe semiparametric regression in Figure 4 this case is quite similar to the ``U’’ shape of the nonparametric (Nadara-Watson) regression from Figure 3.a. that is based on the predicted values \\(x_i'\\beta\\) of the heteroscedastic probit to estimate the probabilities. However, we also see that the (parametric) heteroscedastic probit falls outside the 95% confidence interval area at various points ( Figure 4 in green). Hence, while we cannot use the hetprobit model to draw conclusions on the probability of voting for higher property taxes, we do can use a nonparametric version of the predicted values of the heteroscedastic model for that purposes.\nMost of the behavior behind this strange shape of the link function -behavior that is mostly happening at \\(x_i'\\beta&lt;-5\\) in Figure 4 -, can be explained by the distribution of the years variable. From Figure 5 we can observe that this group consists of 5 observations that have an average number of years of 39, versus the 6.8 from the rest of the sample:\n\n\n\n\n\n\nFigure 5: Kernel Density Estimation of Years\n\n\n\nAs we have seen from the models, the number of years has in general a negative effect on the probability of voting for more taxes, but here we have that all these 5 observations have a high number of years in the residence and all of them have vote=1. Hence, this group of ``outliers’’ might be generating a countervailing effect on the true point estimates of years."
  },
  {
    "objectID": "posts/nonparametrics/nonparametrics.html#restricted-model",
    "href": "posts/nonparametrics/nonparametrics.html#restricted-model",
    "title": "Nonparametric & Semiparametric Binary-Choice Regressions",
    "section": "Restricted Model",
    "text": "Restricted Model\nWe now re-run the binary choice models so far discussed, but restricting the estimation for observations where the number of years in residence is smaller than 25. Table 4 shows the main results after normalization to make them comparable:\n\n\n\nTable 4: Binary Choice Models: Normalized Results (years &lt; 25)\n\n\n\n\n\n\nHere we see surprisingly similar conclusions on the point estimates across different specifications. In the standard probit model, we now see that years have statistically significant results, with a more intense negative effect on our dependent variable than before. The heteroscedastic probit now reports a more negative coefficient on the variable logptax, significant only at the 10% level, but for the number of years, we see a less pronounced effect and is no longer significant. Looking at \\(\\gamma\\), note that now we cannot reject the null hypothesis of homoscedastic errors so that, after filtering for the extreme values of years, there is no evidence of this model being better than the standard probit.\nFinally, the semiparametric model seems to be the model that changed the least. Both the direction of the signs and their significance remain as before, with years having a slightly less pronounced coefficient, and logptax reporting now a more negative effect on the probability of voting for higher taxes than before.\nWe want now to test the normality assumption, so we re-estimate the nonparametric regressions from previous exercises to make the visual comparison. In this case, I only perform the Nadaraya-Watson Estimator:\n\n\n\n\n\n\nFigure 6: Non and Semiparametric Estimation (years&lt;25)\n\n\n\nAgain, we see that all both the probit and hetprobit models falls within the boundaries of the confidence interval of the nonparametric estimates. Taking all this evidence together, we can conclude that, after filtering the extreme values of years, the three binary choice models are no longer much different from each other. Finally, because now there is no evidence of heteroscedasticity and we did not reject the normality assumption about the error terms \\(\\varepsilon_i\\), the standard probit model is the preferred model in my view."
  },
  {
    "objectID": "posts/nonparametrics/nonparametrics.html#nonparametric-and-semiparametric-regression-comparison",
    "href": "posts/nonparametrics/nonparametrics.html#nonparametric-and-semiparametric-regression-comparison",
    "title": "Nonparametric & Semiparametric Binary-Choice Regressions",
    "section": "Nonparametric and Semiparametric Regression Comparison",
    "text": "Nonparametric and Semiparametric Regression Comparison\nUsing the standard probit estimates, we want now to graphically compare the estimates between the Nadaraya-Watson (Local Constant), the Local Linear, and the Local Quadratic regression. The following figure summarizes the results:\n\n\n\n\n\n\nFigure 7: Nonparametric and Semiparametric Regression - Comparison\n\n\n\nAlthough the curves seem to be very similar, we can identify some differences at specific points. Generally speaking, the degree of bias of the Nadaraya-Watson highly depends on the curvature or second derivative \\(m''(x)\\) of the regression function, as well on the slope of the density of the regressors \\(f'(x)\\):\n\\[\n\\text{bias}[\\hat{m}(x)]=\\frac{h^2\\mu_2}{2}\\left\\{m''(x)+2\\frac{f'_x(x)m'(x)}{f_x(x)} \\right\\}+O(\\frac{1}{nh})+o(h^2)\n\\]\nNamely, the bias will be very large at a point where the density function curves a lot. In Figure 7 (and assuming the normality distribution is the true function), we can actually observe that the Local Constant estimator has a large and positive bias (i) between \\(-1&lt;x_i'\\beta&lt;0.5\\), which is the place where the density is curving positively and very quickly (positive curvature) and (ii) between \\(0.5&lt;x_i'\\beta&lt;1.55\\), marking the space where the density function is decreasing quickly (negative curvature). The local linear and local quadratic regressions also present this described behavior, but with less biasedness. The local linear is the more smoothed function among the three.\nOne reason that explains this difference may be due to the fact that the bias of the Local Linear (and Quadratic) regression does not depend on the density function of the regressors, also called design bias:\n\\[\\begin{align*}\n    \\text{bias}[\\hat{m}(x)]=\\frac{h^2\\mu_2}{2}m''(x)+o(h^2)\n\\end{align*}\\]\nIn that sense, the design bias adds an extra bias to the Nadaraya-Watson estimator. Finally, we observe more differences between the estimators on the extremes. On the bottom left (between -1.2 and -2), for example, the Nadaraya-Watson starts with higher bias, but then it shifts quickly to be equal to 0 towards the left, whereas the other two (which are very similar to each other) start with less bias, but then they end up with negative probabilities towards the left. On the other extreme we find the same behavior, where the local and quadratic estimators are not bounded to be within 0 and 1."
  },
  {
    "objectID": "posts/nonparametrics/nonparametrics.html#footnotes",
    "href": "posts/nonparametrics/nonparametrics.html#footnotes",
    "title": "Nonparametric & Semiparametric Binary-Choice Regressions",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAs the optimal bandwidth formula also depends on the curvature of the density \\(f''\\), which is unknown, there are different ways to deal with this issue. Some impose assumptions about the unknown distribution (Plug-in methods), and others are more data-driven such as cross-validation techniques.↩︎\nIn contrast, the command lpoly uses the Rule-of-Thumb method of bandwidth selection, which is a Plug-In estimator.↩︎"
  }
]