{"title":"A/B Testing Analysis","markdown":{"yaml":{"title":"A/B Testing Analysis","author":"Daniel Redel","date":"2024-04-05","categories":["Causal Inference","A/B Testing","Python"],"image":"abtest.png","toc":true,"format":{"html":{"toc":true,"code-fold":false,"html-math-method":"katex"}},"jupyter":"python3","freeze":true},"headingText":"Case Study: Online Wine Store's Email Campaign Experiment","containsRefs":false,"markdown":"\n\nA/B testing Analysis using Python.\n\nAs data scientists, we are frequently confronted with questions like: **“Does $X$ affects $Y$?”** Here, $Y$ is the outcome that we care about, while $X$ could be a new feature, product, policy or experience. For example, a website owner would like to ask if a new web page design leads to a higher click-through rate or sale. Similarly, [Spotify](https://confidence.spotify.com/blog/ab-tests-and-rollouts) may  to know whether their new interface leads to more minutes of music played, while [Zalando](https://engineering.zalando.com/posts/2019/02/effectiveness-online-marketing.html) aims to assess the impact of their marketing actions on purchases.\n\nBut how do we go about answering such **causal questions**? For the Platform Economy, the solution **lies in experimentation**. By randomly assigning some kind of treatment (new interface design, new product, etc.) to a subset of users (Group A) and comparing their behavior or outcomes (such as revenue, visits, clicks, etc.) to those who did not receive the treatment (Group B), we can effectively isolate the causal impact of the change. This is at the core of **A/B testing** (also known as Randomized Control Trials), which is considered the workhorse method of experimentation for platforms like [Spotify](https://confidence.spotify.com/blog/experiment-like-spotify), [Uber](https://www.uber.com/en-NL/blog/xp/), [Netflix](https://netflixtechblog.com/what-is-an-a-b-test-b08cc1b57962), an more.\n\nIn this article, we will delve into the analysis involved in A/B testing using **Python**, with a specific focus on Randomization Checks and various methods or statistical tools for studying Average Treatment Effects. Let's go!\n\n\nAs as study case, we are going to use an example taken from [Elea McDonnell](https://github.com/eleafeit/ab_test) (Drexel University). Imagine we are a _'Total Wine & More'_, a well established online wine store who wants to attract more customers and traffic to their website. Suppose our marketing team proposes launching an **email campaign** as a potential solution to drive more traffic and sales. However, before implementing the campaign, they want to conduct a test to determine which type of email design is more effective.\n\n![Source: Total Wine & More](attachment:wine_store.png)\n\nTo evaluate the effectiveness of this email campaign, we conducted an **A/B test**. In this experiment, users were **randomly assigned** to one of three groups: one group received Email A, another group received Email B, and the remaining users were assigned to the control group, which did not receive any special email treatment. What's interesting here is that the design of the test enables us to address two main questions:\n\n> 1. Does the email campaign lead to an increase in purchases compared to the control group (Treated vs. Control)?\n\n> 2. Which email design is more effective in driving engagement and sales (Email A vs. Email B)?\n\n@tbl-raw-dataset presents a sample of the dataset containing all the information we need to commence our analysis. The dataset can be found in [here](https://github.com/eleafeit/ab_test/blob/master/code/test_data.csv). \n\n\nA quick summary of what we have in this dataset:\n\n- **Treatments**: The dataset includes three groups defined by the `group` variable: email version A, email version B, and holdout (control group).\n- **Unit**: Each `user_id` unit in the dataset represents an email address.\n- **Reponse Variables**: We are interested in study causal effects on 3 response variables:\n  - `open`: Whether the recipient opened the test email (loaded images).\n  - `click`: Whether the recipient clicked on the test email to visit the website.\n  - `purch`: The amount ($) of purchases made within 30 days after the email was sent.\n- **Selection**: The dataset consists of data from all active customers.\n- **Assignment**: Participants were randomly assigned to one of the three groups (each group comprising one-third of the total sample).\n\n\n# 1. Randomization Checks\n\nBefore assessing causal effects, it's crucial to ensure that the randomization process was executed correctly. Randomization ensures that, on average, the only difference between the two groups is the treatment, allowing us to attribute outcome differences to the treatment effect.\n\nThe reality is that, despite randomization, the two groups are never identical. Therefore, it's essential, after randomization, to check whether all observed variables are balanced across groups and whether there are no systematic differences.\n\n**Randomization checks** serve to confirm **whether baseline variables are distributed similarly across the treatment and control groups**. In our case, referring back to @tbl-raw-dataset, the baseline variables of interest include:\n\n- `days_since`: Days since last activity\n- `visits`: Number of website visits\n- `past_purch`: Total past purchases ($)\n- Total _past_ purchases by category ($): `chard`, `sav_blanc`, `syrah`, `cab`\n\nLet's just compare their means by treatment group:\n\nJust by comparing and checking their means in @tbl-baseline1, we can already say that the randomization process was successful. On average, users across all treatment groups exhibit similar behavior and characteristics, making them comparable in terms of visits, past purchases, and other relevant factors. \n\nHowever, we want to go further than just comparing means and also **compare distributions across treatments**. This will ensure a balanced distribution of baseline variables among the treatment and control groups. We'll explore different methods to compare distributions and assess the magnitude and significance of their differences. We'll consider two approaches: **visual** and **statistical**. \n\n## 1.1. Violinplots\n\nA first visual approach is the [**violinplot**](https://en.wikipedia.org/wiki/Violin_plot). The violinplot plots depicts distributions of numeric data for one or more groups using [**density curves**](https://math.libretexts.org/Bookshelves/Applied_Mathematics/Math_For_Liberal_Art_Students_2e_(Diaz)/11%3A_Normal_Distribution/11.02%3A_The_Density_Curve_of_a_Normal_Distribution#:~:text=A%20density%20curve%20is%20an,the%20most%20useful%20to%20us.). These densities —usually smoothed by a [**kernel density estimator**](https://en.wikipedia.org/wiki/Kernel_density_estimation)— are displayed along the `y-axis` so that we can compare them. By default, the `.violintplot()` function from the `seaborn` library also adds a miniature boxplot inside. @fig-violinplots shows the violinplots of our baseline variables:\n\nThe shape of the distributions across `group` is quite similar, indicating comparable distributions of baseline variables. This similarity confirms that the randomization process was successful in achieving balance between the treatment and control groups.\n\n## 1.2. Histograms\n\nAnother intuitive way to plot a distribution is the **[histogram](https://en.wikipedia.org/wiki/Histogram)**. The histogram groups the data into equally wide **bins** and plots the number of observations within each bin.\n\nThe full distributions of baseline variables should also be the same between treatment groups in this case:\n\nRandomization checks out again!\n\n## 1.3. T-Tests\n\nNow that we've visualized differences between distributions, let's move on to a more rigorous approach for _assessing statistical significance_. Visualization provides **intuition** by allowing us to visually inspect differences, but we also want to be able to quantify the significance of these differences.\n\nThe most common test is the [student t-test](https://en.wikipedia.org/wiki/Student%27s_t-test). T-tests are generally used to **compare means**. In this case, we want to test whether the means of the `days_since`, `visits` and `past_purch` distribution are the same across two groups: Treated (Email A + Email B) vs. Control. This test statistic is given by:\n\n$$\nstat = \\frac{|\\bar x_1 - \\bar x_2|}{\\sqrt{s^2 / n }}\n$$\n\nWhere $\\bar x$ is the sample mean and $s$ is the sample standard deviation. Under mild conditions, the test statistic is asymptotically distributed as a [student t](https://en.wikipedia.org/wiki/Student%27s_t-distribution) distribution.\n\nWe use the `ttest_ind` function from `scipy` to perform the t-test. The function returns both the test statistic and the implied [p-value](https://en.wikipedia.org/wiki/P-value).\n\nThe p-values from these tests all are above 0.1, therefore we do **not reject** the null hypothesis of no difference in *means* across treatment and control groups.\n\nThe thing is, our example has m**ore than 2 groups to compare**. With multiple groups, the most popular test is the [**F-test**](https://en.wikipedia.org/wiki/F-test). The F-test compares the variance of a variable across different groups. The F-test statistic is\n\n$$\n\\text{stat} = \\frac{\\text{between-group variance}}{\\text{within-group variance}} = \\frac{\\sum_{g} \\big( \\bar x_g - \\bar x \\big) / (G-1)}{\\sum_{g} \\sum_{i \\in g} \\big( \\bar x_i - \\bar x_g \\big) / (N-G)}\n$$\n\nWhere $G$ is the number of groups, $N$ is the number of observations, $\\bar x$ is the overall mean and $\\bar x_g$ is the mean within group $g$. Under the null hypothesis of group independence, the f-statistic is [F-distributed](https://en.wikipedia.org/wiki/F-distribution).\n\nHere again, the p-values are telling us that these distributions across treatment are likely similar.\n\n## 1.4. Standardized Mean Difference\n\nIn general, it is good practice to always perform a test for difference in means on **all variables** across the treatment and control group, when we are running A/B tests. However, since the denominator of the t-test statistic depends on the sample size, the corresponding p-values are not directly comparable across studies. \n\nAs alternative, we can use the **standardized mean difference (SMD)**, which is just a standardized difference, which can be computed as:\n\n$$\nSMD = \\frac{|\\bar x_1 - \\bar x_2|}{\\sqrt{(s^2_1 + s^2_2) / 2}}\n$$\n\nUsually a value below $0.1$ is considered a \"small\" difference. \n\nIt is good practice to collect average values of all variables across treatment and control group and a measure of distance between the two — either the t-test or the SMD — into a table that is called **balance table**. \n\nLet's use the [`create_table_one`](https://causalml.readthedocs.io/en/latest/causalml.html#module-causalml.match) function from the [`causalml`](https://causalml.readthedocs.io/en/latest/about.html) library to generate it.\n\nIn the first two columns of @tbl-balancetable, we observe the average of the different variables across the treatment and control groups, with standard errors provided in parentheses. However, it's in the **last column** where the values of the standardized mean difference (SMD) are presented. A standardized difference below $|0.1|$ for all variables suggests that the two groups are likely similar.\n\n# 2. Average Treatment Effects\n\nNow, at last, we stand ready to estimate the effectiveness of our email campaign. Let's recall our main causal questions:\n\n> 1. Does the email campaign lead to an increase in purchases (`purch`) compared to the control group (Treated vs. Control)?\n\n\n> 2. Which email design is more effective in driving engagement (`open`, `click`) and sales (`purch`) (Email A vs. Email B)?\n\n## 2.1. Difference in Means\n\nLet's start with the first causal question (Treated vs Control). We can just compare the average outcome post-treatment $Y_1$ across control and treated units and randomization guarantees that this difference is, on average, due to the treatment alone:\n\n$$\n\\widehat {ATE}^{simple} = \\bar Y_{t=1, d=1} - \\bar Y_{t=1, d=0}\n$$\n\nIn our case, we compute the average purchases (`purch`) after the campaign in the treatment group, minus the average revenue post ad campaign in the control group:\n\nThis $\\text{ATE}$ is telling us that, on average, each recipient of the email campaign resulted in an **additional revenue of $13.32** compared to those who did not receive the campaign. In other words, the email campaign had a positive impact on increasing purchases.\n\nThis suggests that, overall, the email campaign resulted in an increased revenues of $1,101,358.\n\n## 2.2. Linear Regression\n\nWe can obtain the same estimate by regressing the post-treatment outcome `purch` on the treatment indicator `treat`:\n\n$$\nY_{i} = \\alpha + \\beta D_i + \\varepsilon_i\n$$\n\nwhere $D_i$ is the treatment indicator (equal to 1 for treated units and 0 for control units), $\\alpha$ is the intercept term and $\\beta$ is our coefficient of interest representing the **Average Treatment Effect**.\n\nThe advantage of running a linear regression is that we can directly test whether the difference in outcomes between the treatment and control groups is **statistically significant**, providing us with more robust information regarding the effectiveness of the campaign.\n\n@tbl-reg1 is not only confirming an $\\text{ATE}$ of $13.32, but also telling us that this difference is statistically different from zero. This provides stronger evidence regarding the effectiveness of the email campaign in increasing purchases compared to the control group.\n\nWe can also compare both types of campaigns against our control group:\n\nIt's important to note that this regression results in @tbl-reg2 are not directly addressing our second causal question (Email A vs. Email B). Instead, the coefficients obtained from the regression are comparing each email campaign against the control group. In any case, the table suggests that, on average, Campaign B generates more purchases. It remains to be determined whether this difference is statistically significant.\n\n## 2.3. Distribution Comparison\n\n@fig-ate-dist0 illustrates the purchase distribution by treatment, providing a comprehensive view of the data. However, it's important to note that while we have established a statistically significant difference, our estimation of this distribution **assumes a Normal Distribution**, which may not always hold true. In future articles, we will delve deeper into alternative methods that do not rely on such assumptions, ensuring a more robust analysis.\n\nAgain, assuming Normality, we can also estimate confidence intervals:\n\n## 2.4. Email A vs Email B\n\nNow, let's turn our attention to the second causal question: **Which email design, Email A or Email B, is more effective in driving engagement and sales?** \n\nInterestingly enough, Email A appears to outperform Email B in terms of **opens** and **clicks** (@fig-ate-dist2). However, **this level of engagement does not necessarily translate to higher purchase rates.** Purchases from Email B are higher on average, but the large overlap in their distribution is telling us that this difference might not be significant. While purchases from Email B are higher on average, the large overlap in their distribution suggests that this difference might not be significant. \n\nTo confirm this conclusion, we can conduct a linear regression comparing Email A and Email B purchases:\n\nIndeed, despite observing higher purchases associated with Email B, the obtained p-value of 0.554 indicates that this difference is **not statistically significant**.\n\nThese findings show how important it is to look at different metrics and really dig into the data to figure out what's working best for engaging customers and boosting sales.\n\n# 3. Conclusion\n\nIn this post, we've explored a variety of methods and tools used for **A/B testing analysis**, ranging from visual comparisons to statistical tests and regression analysis. Each approach offers cool insights about the quality of our randomization and also about the causal effect of our marketing campaigns.\n\nLooking ahead, in a next article on AB testing, I'll take a deep dive into **Bayesian methods** and **nonparametric estimation** of standard errors for uncertainty. These techniques are particularly valuable when we aim to avoid assumptions about the distribution behavior, offering robust alternatives for analyzing and interpreting A/B testing results in real-world scenarios.\n","srcMarkdownNoYaml":"\n\nA/B testing Analysis using Python.\n\nAs data scientists, we are frequently confronted with questions like: **“Does $X$ affects $Y$?”** Here, $Y$ is the outcome that we care about, while $X$ could be a new feature, product, policy or experience. For example, a website owner would like to ask if a new web page design leads to a higher click-through rate or sale. Similarly, [Spotify](https://confidence.spotify.com/blog/ab-tests-and-rollouts) may  to know whether their new interface leads to more minutes of music played, while [Zalando](https://engineering.zalando.com/posts/2019/02/effectiveness-online-marketing.html) aims to assess the impact of their marketing actions on purchases.\n\nBut how do we go about answering such **causal questions**? For the Platform Economy, the solution **lies in experimentation**. By randomly assigning some kind of treatment (new interface design, new product, etc.) to a subset of users (Group A) and comparing their behavior or outcomes (such as revenue, visits, clicks, etc.) to those who did not receive the treatment (Group B), we can effectively isolate the causal impact of the change. This is at the core of **A/B testing** (also known as Randomized Control Trials), which is considered the workhorse method of experimentation for platforms like [Spotify](https://confidence.spotify.com/blog/experiment-like-spotify), [Uber](https://www.uber.com/en-NL/blog/xp/), [Netflix](https://netflixtechblog.com/what-is-an-a-b-test-b08cc1b57962), an more.\n\nIn this article, we will delve into the analysis involved in A/B testing using **Python**, with a specific focus on Randomization Checks and various methods or statistical tools for studying Average Treatment Effects. Let's go!\n\n# Case Study: Online Wine Store's Email Campaign Experiment\n\nAs as study case, we are going to use an example taken from [Elea McDonnell](https://github.com/eleafeit/ab_test) (Drexel University). Imagine we are a _'Total Wine & More'_, a well established online wine store who wants to attract more customers and traffic to their website. Suppose our marketing team proposes launching an **email campaign** as a potential solution to drive more traffic and sales. However, before implementing the campaign, they want to conduct a test to determine which type of email design is more effective.\n\n![Source: Total Wine & More](attachment:wine_store.png)\n\nTo evaluate the effectiveness of this email campaign, we conducted an **A/B test**. In this experiment, users were **randomly assigned** to one of three groups: one group received Email A, another group received Email B, and the remaining users were assigned to the control group, which did not receive any special email treatment. What's interesting here is that the design of the test enables us to address two main questions:\n\n> 1. Does the email campaign lead to an increase in purchases compared to the control group (Treated vs. Control)?\n\n> 2. Which email design is more effective in driving engagement and sales (Email A vs. Email B)?\n\n@tbl-raw-dataset presents a sample of the dataset containing all the information we need to commence our analysis. The dataset can be found in [here](https://github.com/eleafeit/ab_test/blob/master/code/test_data.csv). \n\n\nA quick summary of what we have in this dataset:\n\n- **Treatments**: The dataset includes three groups defined by the `group` variable: email version A, email version B, and holdout (control group).\n- **Unit**: Each `user_id` unit in the dataset represents an email address.\n- **Reponse Variables**: We are interested in study causal effects on 3 response variables:\n  - `open`: Whether the recipient opened the test email (loaded images).\n  - `click`: Whether the recipient clicked on the test email to visit the website.\n  - `purch`: The amount ($) of purchases made within 30 days after the email was sent.\n- **Selection**: The dataset consists of data from all active customers.\n- **Assignment**: Participants were randomly assigned to one of the three groups (each group comprising one-third of the total sample).\n\n\n# 1. Randomization Checks\n\nBefore assessing causal effects, it's crucial to ensure that the randomization process was executed correctly. Randomization ensures that, on average, the only difference between the two groups is the treatment, allowing us to attribute outcome differences to the treatment effect.\n\nThe reality is that, despite randomization, the two groups are never identical. Therefore, it's essential, after randomization, to check whether all observed variables are balanced across groups and whether there are no systematic differences.\n\n**Randomization checks** serve to confirm **whether baseline variables are distributed similarly across the treatment and control groups**. In our case, referring back to @tbl-raw-dataset, the baseline variables of interest include:\n\n- `days_since`: Days since last activity\n- `visits`: Number of website visits\n- `past_purch`: Total past purchases ($)\n- Total _past_ purchases by category ($): `chard`, `sav_blanc`, `syrah`, `cab`\n\nLet's just compare their means by treatment group:\n\nJust by comparing and checking their means in @tbl-baseline1, we can already say that the randomization process was successful. On average, users across all treatment groups exhibit similar behavior and characteristics, making them comparable in terms of visits, past purchases, and other relevant factors. \n\nHowever, we want to go further than just comparing means and also **compare distributions across treatments**. This will ensure a balanced distribution of baseline variables among the treatment and control groups. We'll explore different methods to compare distributions and assess the magnitude and significance of their differences. We'll consider two approaches: **visual** and **statistical**. \n\n## 1.1. Violinplots\n\nA first visual approach is the [**violinplot**](https://en.wikipedia.org/wiki/Violin_plot). The violinplot plots depicts distributions of numeric data for one or more groups using [**density curves**](https://math.libretexts.org/Bookshelves/Applied_Mathematics/Math_For_Liberal_Art_Students_2e_(Diaz)/11%3A_Normal_Distribution/11.02%3A_The_Density_Curve_of_a_Normal_Distribution#:~:text=A%20density%20curve%20is%20an,the%20most%20useful%20to%20us.). These densities —usually smoothed by a [**kernel density estimator**](https://en.wikipedia.org/wiki/Kernel_density_estimation)— are displayed along the `y-axis` so that we can compare them. By default, the `.violintplot()` function from the `seaborn` library also adds a miniature boxplot inside. @fig-violinplots shows the violinplots of our baseline variables:\n\nThe shape of the distributions across `group` is quite similar, indicating comparable distributions of baseline variables. This similarity confirms that the randomization process was successful in achieving balance between the treatment and control groups.\n\n## 1.2. Histograms\n\nAnother intuitive way to plot a distribution is the **[histogram](https://en.wikipedia.org/wiki/Histogram)**. The histogram groups the data into equally wide **bins** and plots the number of observations within each bin.\n\nThe full distributions of baseline variables should also be the same between treatment groups in this case:\n\nRandomization checks out again!\n\n## 1.3. T-Tests\n\nNow that we've visualized differences between distributions, let's move on to a more rigorous approach for _assessing statistical significance_. Visualization provides **intuition** by allowing us to visually inspect differences, but we also want to be able to quantify the significance of these differences.\n\nThe most common test is the [student t-test](https://en.wikipedia.org/wiki/Student%27s_t-test). T-tests are generally used to **compare means**. In this case, we want to test whether the means of the `days_since`, `visits` and `past_purch` distribution are the same across two groups: Treated (Email A + Email B) vs. Control. This test statistic is given by:\n\n$$\nstat = \\frac{|\\bar x_1 - \\bar x_2|}{\\sqrt{s^2 / n }}\n$$\n\nWhere $\\bar x$ is the sample mean and $s$ is the sample standard deviation. Under mild conditions, the test statistic is asymptotically distributed as a [student t](https://en.wikipedia.org/wiki/Student%27s_t-distribution) distribution.\n\nWe use the `ttest_ind` function from `scipy` to perform the t-test. The function returns both the test statistic and the implied [p-value](https://en.wikipedia.org/wiki/P-value).\n\nThe p-values from these tests all are above 0.1, therefore we do **not reject** the null hypothesis of no difference in *means* across treatment and control groups.\n\nThe thing is, our example has m**ore than 2 groups to compare**. With multiple groups, the most popular test is the [**F-test**](https://en.wikipedia.org/wiki/F-test). The F-test compares the variance of a variable across different groups. The F-test statistic is\n\n$$\n\\text{stat} = \\frac{\\text{between-group variance}}{\\text{within-group variance}} = \\frac{\\sum_{g} \\big( \\bar x_g - \\bar x \\big) / (G-1)}{\\sum_{g} \\sum_{i \\in g} \\big( \\bar x_i - \\bar x_g \\big) / (N-G)}\n$$\n\nWhere $G$ is the number of groups, $N$ is the number of observations, $\\bar x$ is the overall mean and $\\bar x_g$ is the mean within group $g$. Under the null hypothesis of group independence, the f-statistic is [F-distributed](https://en.wikipedia.org/wiki/F-distribution).\n\nHere again, the p-values are telling us that these distributions across treatment are likely similar.\n\n## 1.4. Standardized Mean Difference\n\nIn general, it is good practice to always perform a test for difference in means on **all variables** across the treatment and control group, when we are running A/B tests. However, since the denominator of the t-test statistic depends on the sample size, the corresponding p-values are not directly comparable across studies. \n\nAs alternative, we can use the **standardized mean difference (SMD)**, which is just a standardized difference, which can be computed as:\n\n$$\nSMD = \\frac{|\\bar x_1 - \\bar x_2|}{\\sqrt{(s^2_1 + s^2_2) / 2}}\n$$\n\nUsually a value below $0.1$ is considered a \"small\" difference. \n\nIt is good practice to collect average values of all variables across treatment and control group and a measure of distance between the two — either the t-test or the SMD — into a table that is called **balance table**. \n\nLet's use the [`create_table_one`](https://causalml.readthedocs.io/en/latest/causalml.html#module-causalml.match) function from the [`causalml`](https://causalml.readthedocs.io/en/latest/about.html) library to generate it.\n\nIn the first two columns of @tbl-balancetable, we observe the average of the different variables across the treatment and control groups, with standard errors provided in parentheses. However, it's in the **last column** where the values of the standardized mean difference (SMD) are presented. A standardized difference below $|0.1|$ for all variables suggests that the two groups are likely similar.\n\n# 2. Average Treatment Effects\n\nNow, at last, we stand ready to estimate the effectiveness of our email campaign. Let's recall our main causal questions:\n\n> 1. Does the email campaign lead to an increase in purchases (`purch`) compared to the control group (Treated vs. Control)?\n\n\n> 2. Which email design is more effective in driving engagement (`open`, `click`) and sales (`purch`) (Email A vs. Email B)?\n\n## 2.1. Difference in Means\n\nLet's start with the first causal question (Treated vs Control). We can just compare the average outcome post-treatment $Y_1$ across control and treated units and randomization guarantees that this difference is, on average, due to the treatment alone:\n\n$$\n\\widehat {ATE}^{simple} = \\bar Y_{t=1, d=1} - \\bar Y_{t=1, d=0}\n$$\n\nIn our case, we compute the average purchases (`purch`) after the campaign in the treatment group, minus the average revenue post ad campaign in the control group:\n\nThis $\\text{ATE}$ is telling us that, on average, each recipient of the email campaign resulted in an **additional revenue of $13.32** compared to those who did not receive the campaign. In other words, the email campaign had a positive impact on increasing purchases.\n\nThis suggests that, overall, the email campaign resulted in an increased revenues of $1,101,358.\n\n## 2.2. Linear Regression\n\nWe can obtain the same estimate by regressing the post-treatment outcome `purch` on the treatment indicator `treat`:\n\n$$\nY_{i} = \\alpha + \\beta D_i + \\varepsilon_i\n$$\n\nwhere $D_i$ is the treatment indicator (equal to 1 for treated units and 0 for control units), $\\alpha$ is the intercept term and $\\beta$ is our coefficient of interest representing the **Average Treatment Effect**.\n\nThe advantage of running a linear regression is that we can directly test whether the difference in outcomes between the treatment and control groups is **statistically significant**, providing us with more robust information regarding the effectiveness of the campaign.\n\n@tbl-reg1 is not only confirming an $\\text{ATE}$ of $13.32, but also telling us that this difference is statistically different from zero. This provides stronger evidence regarding the effectiveness of the email campaign in increasing purchases compared to the control group.\n\nWe can also compare both types of campaigns against our control group:\n\nIt's important to note that this regression results in @tbl-reg2 are not directly addressing our second causal question (Email A vs. Email B). Instead, the coefficients obtained from the regression are comparing each email campaign against the control group. In any case, the table suggests that, on average, Campaign B generates more purchases. It remains to be determined whether this difference is statistically significant.\n\n## 2.3. Distribution Comparison\n\n@fig-ate-dist0 illustrates the purchase distribution by treatment, providing a comprehensive view of the data. However, it's important to note that while we have established a statistically significant difference, our estimation of this distribution **assumes a Normal Distribution**, which may not always hold true. In future articles, we will delve deeper into alternative methods that do not rely on such assumptions, ensuring a more robust analysis.\n\nAgain, assuming Normality, we can also estimate confidence intervals:\n\n## 2.4. Email A vs Email B\n\nNow, let's turn our attention to the second causal question: **Which email design, Email A or Email B, is more effective in driving engagement and sales?** \n\nInterestingly enough, Email A appears to outperform Email B in terms of **opens** and **clicks** (@fig-ate-dist2). However, **this level of engagement does not necessarily translate to higher purchase rates.** Purchases from Email B are higher on average, but the large overlap in their distribution is telling us that this difference might not be significant. While purchases from Email B are higher on average, the large overlap in their distribution suggests that this difference might not be significant. \n\nTo confirm this conclusion, we can conduct a linear regression comparing Email A and Email B purchases:\n\nIndeed, despite observing higher purchases associated with Email B, the obtained p-value of 0.554 indicates that this difference is **not statistically significant**.\n\nThese findings show how important it is to look at different metrics and really dig into the data to figure out what's working best for engaging customers and boosting sales.\n\n# 3. Conclusion\n\nIn this post, we've explored a variety of methods and tools used for **A/B testing analysis**, ranging from visual comparisons to statistical tests and regression analysis. Each approach offers cool insights about the quality of our randomization and also about the causal effect of our marketing campaigns.\n\nLooking ahead, in a next article on AB testing, I'll take a deep dive into **Bayesian methods** and **nonparametric estimation** of standard errors for uncertainty. These techniques are particularly valuable when we aim to avoid assumptions about the distribution behavior, offering robust alternatives for analyzing and interpreting A/B testing results in real-world scenarios.\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":false,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"jupyter"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":false,"code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../../styles.css"],"toc":true,"html-math-method":"katex","output-file":"1_ab_testing.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.4.550","theme":"flatly","title-block-banner":true,"title":"A/B Testing Analysis","author":"Daniel Redel","date":"2024-04-05","categories":["Causal Inference","A/B Testing","Python"],"image":"abtest.png","jupyter":"python3"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}