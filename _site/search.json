[
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n\n\n\n\n\n\n\n\n\n\nMachine Learning with Python: A Quick Guide\n\n\n\nMachine Learning\n\n\nPython\n\n\nscikit-learn\n\n\n\n\n30 March 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAmsterdam Airbnb Price Prediction: A Machine Learning Approach\n\n\n\nPython\n\n\nGeospatial Analysis\n\n\nMachine Learning\n\n\ngeopandas\n\n\nscikit-learn\n\n\n\n\n11 March 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImpact of Transit-Oriented Development on Dutch Commercial Real Estate Prices\n\n\n\nMaster Thesis\n\n\nEconometrics\n\n\nCausal Inference\n\n\nDiff-in-Diff\n\n\n\n\n02 January 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCustomer Analytics with R\n\n\n\nR & Rstudio\n\n\nA/B Testing\n\n\nMachine Learning\n\n\n\n\n01 December 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVoting Patterns of the Ministers of the Supreme Court in Antitrust Cases: Evidence from Chile\n\n\n\nArticle\n\n\nCompetition & Antitrust\n\n\nR & Rstudio\n\n\n\n\n18 May 2022\n\n\n\n\n\n\n\n\nNo matching items\n\n Back to top"
  },
  {
    "objectID": "posts/jupyter-first/hello.html",
    "href": "posts/jupyter-first/hello.html",
    "title": "Quarto Basics",
    "section": "",
    "text": "For a demonstration of a line plot on a polar axis, see Figure¬†1.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nr = np.arange(0, 2, 0.01)\ntheta = 2 * np.pi * r\nfig, ax = plt.subplots(\n  subplot_kw = {'projection': 'polar'} \n)\nax.plot(theta, r)\nax.set_rticks([0.5, 1, 1.5, 2])\nax.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\nFigure¬†1: A line plot on a polar axis"
  },
  {
    "objectID": "posts/jupyter-first/hello.html#polar-axis",
    "href": "posts/jupyter-first/hello.html#polar-axis",
    "title": "Quarto Basics",
    "section": "",
    "text": "For a demonstration of a line plot on a polar axis, see Figure¬†1.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nr = np.arange(0, 2, 0.01)\ntheta = 2 * np.pi * r\nfig, ax = plt.subplots(\n  subplot_kw = {'projection': 'polar'} \n)\nax.plot(theta, r)\nax.set_rticks([0.5, 1, 1.5, 2])\nax.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\nFigure¬†1: A line plot on a polar axis"
  },
  {
    "objectID": "myposts.html",
    "href": "myposts.html",
    "title": "Daniel Redel",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n\n\nCausal Inference\n\n\nA/B Testing\n\n\nPython\n\n\n\n\n\n\n\nDaniel Redel\n\n\nApr 5, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNonparametric Regression\n\n\nSemiparametric Regression\n\n\nBinary Choice Models\n\n\nKernel Density Estimation\n\n\n\n\n\n\n\nDaniel Redel\n\n\nDec 11, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npython\n\n\ncode\n\n\nanalysis\n\n\n\nMy first Jupyter Notebook\n\n\n\nDaniel Redel\n\n\nMay 22, 2021\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "myposts.html#my-posts",
    "href": "myposts.html#my-posts",
    "title": "Daniel Redel",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n\n\nCausal Inference\n\n\nA/B Testing\n\n\nPython\n\n\n\n\n\n\n\nDaniel Redel\n\n\nApr 5, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNonparametric Regression\n\n\nSemiparametric Regression\n\n\nBinary Choice Models\n\n\nKernel Density Estimation\n\n\n\n\n\n\n\nDaniel Redel\n\n\nDec 11, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npython\n\n\ncode\n\n\nanalysis\n\n\n\nMy first Jupyter Notebook\n\n\n\nDaniel Redel\n\n\nMay 22, 2021\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "I am passionate about the intersection between Economics and Data Science. Experienced in Competition Economics and Quantitative Research. Currently working as a Junior Data Consultant at FactX, where I engage in diverse projects across different industries. Oriented toward analytical and interdisciplinary work, with high learning opportunities.\n \n  \n   \n  \n    \n     Twitter\n  \n  \n    \n     LinkedIn\n  \n  \n    \n     Github"
  },
  {
    "objectID": "about.html#skills",
    "href": "about.html#skills",
    "title": "About Me",
    "section": "Skills",
    "text": "Skills"
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "About Me",
    "section": "Education",
    "text": "Education\n\n\nMSc. in Econometrics & Mathematical Economics\nTilburg University\n\n\nJan 2024\n\n\n\n\nMSc. in Applied Economics\nPUC Chile\n\n\n2021\n\n\n\n\nBSc. in Economics & Business Administration\nPUC Chile\n\n\n2020"
  },
  {
    "objectID": "about.html#experience",
    "href": "about.html#experience",
    "title": "About Me",
    "section": "Experience",
    "text": "Experience\n\n\nJunior Data Consultant\nFactX, Utrecht\n\n\nMarch 2024 ‚Äì present\n\n\n\n\nThesis Internship\nCPB Netherlands Bureau for Economic Policy Analysis, The Hague\n\n\nMay 2023 - Oct 2023\n\n\n\n\nResearch Analyst\nCentroCompetencia, Santiago\n\n\nJul 2021 - Apr 2023"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "üëã Hey, I‚Äôm Daniel",
    "section": "",
    "text": "I am passionate about the intersection between Economics and Data Science. Experienced in Competition Economics and Quantitative Research. Currently working as a Junior Data Consultant at FactX, where I engage in diverse projects across different industries. Oriented toward analytical and interdisciplinary work, with high learning opportunities.\n \n  \n   \n  \n    \n     Twitter\n  \n  \n    \n     LinkedIn\n  \n  \n    \n     Github"
  },
  {
    "objectID": "index.html#latest-posts",
    "href": "index.html#latest-posts",
    "title": "üëã Hey, I‚Äôm Daniel",
    "section": "Latest Posts",
    "text": "Latest Posts\nClick here to check out more posts.\n\n\n\n\n\n\n\n\n\n\nA/B Testing Analysis\n\n\n\n\n\n\nDaniel Redel\n\n\nApr 5, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNonparametric & Semiparametric Binary-Choice Regressions\n\n\n\n\n\n\nDaniel Redel\n\n\nDec 11, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nQuarto Basics\n\n\nMy first Jupyter Notebook\n\n\n\nDaniel Redel\n\n\nMay 22, 2021\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/ab_testing1/1_ab_testing.html",
    "href": "posts/ab_testing1/1_ab_testing.html",
    "title": "A/B Testing Analysis",
    "section": "",
    "text": "A/B testing Analysis using Python.\nAs data scientists, we are frequently confronted with questions like: ‚ÄúDoes X affects Y?‚Äù Here, Y is the outcome that we care about, while X could be a new feature, product, policy or experience. For example, a website owner would like to ask if a new web page design leads to a higher click-through rate or sale. Similarly, Spotify may to know whether their new interface leads to more minutes of music played, while Zalando aims to assess the impact of their marketing actions on purchases.\nBut how do we go about answering such causal questions? For the Platform Economy, the solution lies in experimentation. By randomly assigning some kind of treatment (new interface design, new product, etc.) to a subset of users (Group A) and comparing their behavior or outcomes (such as revenue, visits, clicks, etc.) to those who did not receive the treatment (Group B), we can effectively isolate the causal impact of the change. This is at the core of A/B testing (also known as Randomized Control Trials), which is considered the workhorse method of experimentation for platforms like Spotify, Uber, Netflix, an more.\nIn this article, we will delve into the analysis involved in A/B testing using Python, with a specific focus on Randomization Checks and various methods or statistical tools for studying Average Treatment Effects. Let‚Äôs go!"
  },
  {
    "objectID": "posts/ab_testing1/1_ab_testing.html#violinplots",
    "href": "posts/ab_testing1/1_ab_testing.html#violinplots",
    "title": "A/B Testing Analysis",
    "section": "1.1. Violinplots",
    "text": "1.1. Violinplots\nA first visual approach is the violinplot. The violinplot plots depicts distributions of numeric data for one or more groups using density curves. These densities ‚Äîusually smoothed by a kernel density estimator‚Äî are displayed along the y-axis so that we can compare them. By default, the .violintplot() function from the seaborn library also adds a miniature boxplot inside. Figure¬†1 shows the violinplots of our baseline variables:\n\n\nCode\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n\n# Create subplots\nfig, axs = plt.subplots(3, 3, figsize=(15, 12))\n\n# Plot histogram for 'days_since'\nfiltered_df = df[df['days_since'] &gt; 0]\nsns.violinplot(data=filtered_df, x='group', y='days_since', palette='viridis', ax=axs[0, 0])\naxs[0, 0].set_xlabel('Days Since Last Activity')\naxs[0, 0].set_ylabel('Customers')\naxs[0, 0].set_title('Violinplot of Days Since')\n\n# Plot histogram for 'visits'\nfiltered_df = df[df['visits'] &gt; 0]\nsns.violinplot(data=filtered_df, x='group', y='visits', palette='viridis', ax=axs[0, 1])\naxs[0, 1].set_xlabel('Visits')\naxs[0, 1].set_ylabel('Customers')\naxs[0, 1].set_title('Violinplot of Website Visits')\n\n# Plot histogram for 'past_purch'\nfiltered_df = df[(df['past_purch'] &gt; 0) & (df['past_purch'] &lt; 2000)]\nsns.violinplot(data=filtered_df, x='group', y='past_purch', palette='viridis', ax=axs[0, 2])\naxs[0, 2].set_xlabel('Past Purchases ($)')\naxs[0, 2].set_ylabel('Customers')\naxs[0, 2].set_title('Violinplot of Past Purchases')\n\n# Plot histogram for 'chard'\nfiltered_df = df[(df['chard'] &gt; 0) & (df['chard'] &lt; 2000)]\nsns.violinplot(data=filtered_df, x='group', y='chard', palette='viridis', ax=axs[1, 0])\naxs[1, 0].set_xlabel('Chardonnay')\naxs[1, 0].set_ylabel('Customers')\naxs[1, 0].set_title('Violinplot of Chardonnay Purchases')\n\n# Plot histogram for 'sav_blanc'\nfiltered_df = df[(df['sav_blanc'] &gt; 0) & (df['sav_blanc'] &lt; 2000)]\nsns.violinplot(data=filtered_df, x='group', y='sav_blanc', palette='viridis', ax=axs[1, 1])\naxs[1, 1].set_xlabel('Sauvignon Blanc')\naxs[1, 1].set_ylabel('Customers')\naxs[1, 1].set_title('Violinplot of Sauvignon Blanc Purchases')\n\n# Plot histogram for 'syrah'\nfiltered_df = df[(df['syrah'] &gt; 0) & (df['syrah'] &lt; 1000)]\nsns.violinplot(data=filtered_df, x='group', y='syrah', palette='viridis', ax=axs[1, 2])\naxs[1, 2].set_xlabel('Syrah')\naxs[1, 2].set_ylabel('Customers')\naxs[1, 2].set_title('Violinplot of Syrah Purchases')\n\n# Plot histogram for 'cab'\nfiltered_df = df[(df['cab'] &gt; 0) & (df['cab'] &lt; 500)]\nsns.violinplot(data=filtered_df, x='group', y='cab', palette='viridis', ax=axs[2, 0])\naxs[2, 0].set_xlabel('Cabernet')\naxs[2, 0].set_ylabel('Customers')\naxs[2, 0].set_title('Violinplot of Cabernet Purchases')\n\n# Hide the empty subplots\naxs[2, 1].axis('off')\naxs[2, 2].axis('off')\n\n# Adjust layout\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nFigure¬†1: Violinplot of Baseline Variables\n\n\n\n\n\nThe shape of the distributions across group is quite similar, indicating comparable distributions of baseline variables. This similarity confirms that the randomization process was successful in achieving balance between the treatment and control groups."
  },
  {
    "objectID": "posts/ab_testing1/1_ab_testing.html#histograms",
    "href": "posts/ab_testing1/1_ab_testing.html#histograms",
    "title": "A/B Testing Analysis",
    "section": "1.2. Histograms",
    "text": "1.2. Histograms\nAnother intuitive way to plot a distribution is the histogram. The histogram groups the data into equally wide bins and plots the number of observations within each bin.\nThe full distributions of baseline variables should also be the same between treatment groups in this case:\n\n\nCode\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Create subplots\nfig, axs = plt.subplots(3, 3, figsize=(18, 12))\n\n# Plot histogram for 'days_since'\nsns.histplot(data=df, x='days_since', hue='group', binwidth=25, stat='density', common_norm=False, alpha=0.8, multiple='stack', palette='viridis', ax=axs[0, 0])\naxs[0, 0].set_xlim(0, 700)\naxs[0, 0].set_xlabel('Days Since Last Activity')\naxs[0, 0].set_ylabel('Customers')\naxs[0, 0].set_title('Distribution of days_since by treatment group')\n\n# Plot histogram for 'visits'\nfiltered_df = df[df['visits'] &gt; 0]\nsns.histplot(data=filtered_df, x='visits', hue='group', binwidth=1, stat='density', common_norm=False, alpha=0.8, multiple='stack', palette='viridis', ax=axs[0, 1])\naxs[0, 1].set_xlim(0, 35)\naxs[0, 1].set_xlabel('Visits')\naxs[0, 1].set_ylabel('Customers')\naxs[0, 1].set_title('Distribution of website visits by treatment group')\n\n# Plot histogram for 'past_purch'\nfiltered_df = df[df['past_purch'] &gt; 0]\nsns.histplot(data=filtered_df, x='past_purch', hue='group', binwidth=25, stat='density', common_norm=False, alpha=0.8, multiple='stack', palette='viridis', ax=axs[0, 2])\naxs[0, 2].set_xlim(0, 2000)\naxs[0, 2].set_xlabel('Past Purchases ($)')\naxs[0, 2].set_ylabel('Customers')\naxs[0, 2].set_title('Distribution of past purchases by treatment group')\n\n# Plot histogram for 'chard'\nfiltered_df = df[df['chard'] &gt; 0]\nsns.histplot(data=filtered_df, x='chard', hue='group', binwidth=25, stat='density', common_norm=False, alpha=0.8, multiple='stack', palette='viridis', ax=axs[1, 0])\naxs[1, 0].set_xlim(0, 2000)\naxs[1, 0].set_xlabel('Chardonnay')\naxs[1, 0].set_ylabel('Customers')\naxs[1, 0].set_title('Distribution of Chardonnay by treatment group')\n\n# Plot histogram for 'sav_blanc'\nfiltered_df = df[df['sav_blanc'] &gt; 0]\nsns.histplot(data=filtered_df, x='sav_blanc', hue='group', binwidth=25, stat='density', common_norm=False, alpha=0.8, multiple='stack', palette='viridis', ax=axs[1, 1])\naxs[1, 1].set_xlim(0, 2000)\naxs[1, 1].set_xlabel('Sauvignon Blanc')\naxs[1, 1].set_ylabel('Customers')\naxs[1, 1].set_title('Distribution of Sauvignon Blanc by treatment group')\n\n# Plot histogram for 'syrah'\nfiltered_df = df[df['syrah'] &gt; 0]\nsns.histplot(data=filtered_df, x='syrah', hue='group', binwidth=25, stat='density', common_norm=False, alpha=0.8, multiple='stack', palette='viridis', ax=axs[1, 2])\naxs[1, 2].set_xlim(0, 1000)\naxs[1, 2].set_xlabel('Syrah')\naxs[1, 2].set_ylabel('Customers')\naxs[1, 2].set_title('Distribution of Syrah by treatment group')\n\n# Plot histogram for 'cab'\nfiltered_df = df[df['cab'] &gt; 0]\nsns.histplot(data=filtered_df, x='cab', hue='group', binwidth=25, stat='density', common_norm=False, alpha=0.8, multiple='stack', palette='viridis', ax=axs[2, 0])\naxs[2, 0].set_xlim(0, 500)\naxs[2, 0].set_xlabel('Cabernet')\naxs[2, 0].set_ylabel('Customers')\naxs[2, 0].set_title('Distribution of Cabernet by treatment group')\n\n# Hide empty subplots\nfor ax in axs.flat[7:]:\n    ax.axis('off')\n\n# Adjust layout\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nFigure¬†2: Histograms of Baseline Variables\n\n\n\n\n\nRandomization checks out again!"
  },
  {
    "objectID": "posts/ab_testing1/1_ab_testing.html#t-tests",
    "href": "posts/ab_testing1/1_ab_testing.html#t-tests",
    "title": "A/B Testing Analysis",
    "section": "1.3. T-Tests",
    "text": "1.3. T-Tests\nNow that we‚Äôve visualized differences between distributions, let‚Äôs move on to a more rigorous approach for assessing statistical significance. Visualization provides intuition by allowing us to visually inspect differences, but we also want to be able to quantify the significance of these differences.\nThe most common test is the student t-test. T-tests are generally used to compare means. In this case, we want to test whether the means of the days_since, visits and past_purch distribution are the same across two groups: Treated (Email A + Email B) vs.¬†Control. This test statistic is given by:\n\nstat = \\frac{|\\bar x_1 - \\bar x_2|}{\\sqrt{s^2 / n }}\n\nWhere \\bar x is the sample mean and s is the sample standard deviation. Under mild conditions, the test statistic is asymptotically distributed as a student t distribution.\nWe use the ttest_ind function from scipy to perform the t-test. The function returns both the test statistic and the implied p-value.\n\nfrom scipy.stats import ttest_ind\n\n# Treated vs Control Categorical Variable\ndf['treat'] = df['group'].map({'email_A': 'treated', 'email_B': 'treated', 'ctrl': 'control'})\n\n## Days Since\ndays_since_treated = df.loc[df.treat=='treated', 'days_since'].values\ndays_since_ctrl = df.loc[df.treat=='control', 'days_since'].values\n\nstat, p_value = ttest_ind(days_since_treated, days_since_ctrl)\nprint(f\"days_since t-test: statistic={stat:.4f}, p-value={p_value:.4f}\")\n\ndays_since t-test: statistic=0.0154, p-value=0.9877\n\n\n\n\nvisits t-test: statistic=-0.2451, p-value=0.8064\npast_purch t-test: statistic=0.4345, p-value=0.6639\n\n\nThe p-values from these tests all are above 0.1, therefore we do not reject the null hypothesis of no difference in means across treatment and control groups.\nThe thing is, our example has more than 2 groups to compare. With multiple groups, the most popular test is the F-test. The F-test compares the variance of a variable across different groups. The F-test statistic is\n\n\\text{stat} = \\frac{\\text{between-group variance}}{\\text{within-group variance}} = \\frac{\\sum_{g} \\big( \\bar x_g - \\bar x \\big) / (G-1)}{\\sum_{g} \\sum_{i \\in g} \\big( \\bar x_i - \\bar x_g \\big) / (N-G)}\n\nWhere G is the number of groups, N is the number of observations, \\bar x is the overall mean and \\bar x_g is the mean within group g. Under the null hypothesis of group independence, the f-statistic is F-distributed.\n\nfrom scipy.stats import f_oneway\n\n# Days Since\ndays_since_A = df.loc[df.group=='email_A', 'days_since'].values\ndays_since_B = df.loc[df.group=='email_B', 'days_since'].values\ndays_since_ctrl = df.loc[df.group=='ctrl', 'days_since'].values\n\nstat, p_value = f_oneway(days_since_A, days_since_B, days_since_ctrl)\nprint(f\"days_since F-statistic={stat:.4f}, p-value={p_value:.4f}\")\n\ndays_since F-statistic=0.1642, p-value=0.8485\n\n\n\n\nvisits F-statistic=0.0336, p-value=0.9670\npast_purch F-statistic=0.2618, p-value=0.7697\n\n\nHere again, the p-values are telling us that these distributions across treatment are likely similar."
  },
  {
    "objectID": "posts/ab_testing1/1_ab_testing.html#standardized-mean-difference",
    "href": "posts/ab_testing1/1_ab_testing.html#standardized-mean-difference",
    "title": "A/B Testing Analysis",
    "section": "1.4. Standardized Mean Difference",
    "text": "1.4. Standardized Mean Difference\nIn general, it is good practice to always perform a test for difference in means on all variables across the treatment and control group, when we are running A/B tests. However, since the denominator of the t-test statistic depends on the sample size, the corresponding p-values are not directly comparable across studies.\nAs alternative, we can use the standardized mean difference (SMD), which is just a standardized difference, which can be computed as:\n\nSMD = \\frac{|\\bar x_1 - \\bar x_2|}{\\sqrt{(s^2_1 + s^2_2) / 2}}\n\nUsually a value below 0.1 is considered a ‚Äúsmall‚Äù difference.\nIt is good practice to collect average values of all variables across treatment and control group and a measure of distance between the two ‚Äî either the t-test or the SMD ‚Äî into a table that is called balance table.\nLet‚Äôs use the create_table_one function from the causalml library to generate it.\n\n\nCode\nimport numpy as np\nimport pandas as pd\n\ndef smd(feature, treatment):\n    \"\"\"Calculate the standard mean difference (SMD) of a feature between the\n    treatment and control groups.\n\n    Args:\n        feature (pandas.Series): a column of a feature to calculate SMD for\n        treatment (pandas.Series): a column that indicate whether a row is in\n                                   the treatment group or not\n\n    Returns:\n        (float): The SMD of the feature\n    \"\"\"\n    t = feature[treatment == 1]\n    c = feature[treatment == 0]\n    return (t.mean() - c.mean()) / np.sqrt(0.5 * (t.var() + c.var()))\n\n\n\ndef create_table_one(data, treatment_col, features):\n    \"\"\"Report balance in input features between the treatment and control groups.\n\n    Args:\n        data (pandas.DataFrame): total or matched sample data\n        treatment_col (str): the column name for the treatment\n        features (list of str): the column names of features\n\n    Returns:\n        (pandas.DataFrame): A table with the means and standard deviations in\n            the treatment and control groups, and the SMD between two groups\n            for the features.\n    \"\"\"\n    t1 = pd.pivot_table(\n        data[features + [treatment_col]],\n        columns=treatment_col,\n        aggfunc=[lambda x: \"{:.2f} ({:.2f})\".format(x.mean(), x.std())],\n    )\n    t1.columns = t1.columns.droplevel(level=0)\n    t1[\"SMD\"] = data[features].apply(lambda x: smd(x, data[treatment_col])).round(4)\n\n    n_row = pd.pivot_table(\n        data[[features[0], treatment_col]], columns=treatment_col, aggfunc=[\"count\"]\n    )\n    n_row.columns = n_row.columns.droplevel(level=0)\n    n_row[\"SMD\"] = \"\"\n    n_row.index = [\"n\"]\n\n    t1 = pd.concat([n_row, t1], axis=0)\n    t1.columns.name = \"\"\n    t1.columns = [\"Control\", \"Treatment\", \"SMD\"]\n    t1.index.name = \"Variable\"\n\n    return t1\n\n\n\ndf['treat'] = df['group'].map({'email_A': 1, 'email_B': 1, 'ctrl': 0})\n\ncreate_table_one(df, 'treat', ['days_since', 'visits', 'past_purch', 'chard', 'sav_blanc', 'syrah', 'cab'])\n\n\n\nTable¬†3: Balance Table\n\n\n\n\n\n\n\n\n\n\n\nControl\nTreatment\nSMD\n\n\nVariable\n\n\n\n\n\n\n\nn\n41330\n82658\n\n\n\ncab\n16.52 (47.27)\n16.26 (46.67)\n-0.0053\n\n\nchard\n71.67 (199.39)\n74.14 (211.84)\n0.012\n\n\ndays_since\n89.98 (89.50)\n89.99 (89.95)\n0.0001\n\n\npast_purch\n188.27 (298.18)\n189.06 (303.33)\n0.0026\n\n\nsav_blanc\n73.63 (203.37)\n71.87 (197.25)\n-0.0088\n\n\nsyrah\n26.45 (73.91)\n26.79 (75.04)\n0.0045\n\n\nvisits\n5.95 (2.85)\n5.94 (2.86)\n-0.0015\n\n\n\n\n\n\n\n\n\n\n\nIn the first two columns of Table¬†3, we observe the average of the different variables across the treatment and control groups, with standard errors provided in parentheses. However, it‚Äôs in the last column where the values of the standardized mean difference (SMD) are presented. A standardized difference below |0.1| for all variables suggests that the two groups are likely similar."
  },
  {
    "objectID": "posts/ab_testing1/1_ab_testing.html#difference-in-means",
    "href": "posts/ab_testing1/1_ab_testing.html#difference-in-means",
    "title": "A/B Testing Analysis",
    "section": "2.1. Difference in Means",
    "text": "2.1. Difference in Means\nLet‚Äôs start with the first causal question (Treated vs Control). We can just compare the average outcome post-treatment Y_1 across control and treated units and randomization guarantees that this difference is, on average, due to the treatment alone:\n\n\\widehat {ATE}^{simple} = \\bar Y_{t=1, d=1} - \\bar Y_{t=1, d=0}\n\nIn our case, we compute the average purchases (purch) after the campaign in the treatment group, minus the average revenue post ad campaign in the control group:\n\n\nCode\nate = df.loc[df.treat == 'treated', 'purch'].mean() - df.loc[df.treat == 'control', 'purch'].mean()\nn_treat = df[df.treat == 'treated'].shape[0]\n\nprint( f'Average Treatment Effects: ${ate.round(2)}' )\nprint( f'Average Increased Revenues: ${round(ate*n_treat):,.0f}' )\n\n\nAverage Treatment Effects: $13.32\nAverage Increased Revenues: $1,101,358\n\n\nThis \\text{ATE} is telling us that, on average, each recipient of the email campaign resulted in an additional revenue of $13.32 compared to those who did not receive the campaign. In other words, the email campaign had a positive impact on increasing purchases.\nThis suggests that, overall, the email campaign resulted in an increased revenues of $1,101,358."
  },
  {
    "objectID": "posts/ab_testing1/1_ab_testing.html#linear-regression",
    "href": "posts/ab_testing1/1_ab_testing.html#linear-regression",
    "title": "A/B Testing Analysis",
    "section": "2.2. Linear Regression",
    "text": "2.2. Linear Regression\nWe can obtain the same estimate by regressing the post-treatment outcome purch on the treatment indicator treat:\n\nY_{i} = \\alpha + \\beta D_i + \\varepsilon_i\n\nwhere D_i is the treatment indicator (equal to 1 for treated units and 0 for control units), \\alpha is the intercept term and \\beta is our coefficient of interest representing the Average Treatment Effect.\n\nimport statsmodels.formula.api as smf\n\nsmf.ols('purch ~ treat', data=df).fit().summary().tables[1]\n\n\n\nTable¬†6: A/B Regression Results I\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n12.4203\n0.268\n46.360\n0.000\n11.895\n12.945\n\n\ntreat[T.treated]\n13.3243\n0.328\n40.608\n0.000\n12.681\n13.967\n\n\n\n\n\n\n\n\n\nThe advantage of running a linear regression is that we can directly test whether the difference in outcomes between the treatment and control groups is statistically significant, providing us with more robust information regarding the effectiveness of the campaign.\nTable¬†6 is not only confirming an \\text{ATE} of $13.32, but also telling us that this difference is statistically different from zero. This provides stronger evidence regarding the effectiveness of the email campaign in increasing purchases compared to the control group.\nWe can also compare both types of campaigns against our control group:\n\nsmf.ols('purch ~ group', data=df).fit().summary().tables[1]\n\n\n\nTable¬†7: A/B Regression Results II\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n12.4203\n0.268\n46.360\n0.000\n11.895\n12.945\n\n\ngroup[T.email_A]\n13.2026\n0.379\n34.846\n0.000\n12.460\n13.945\n\n\ngroup[T.email_B]\n13.4460\n0.379\n35.488\n0.000\n12.703\n14.189\n\n\n\n\n\n\n\n\n\nIt‚Äôs important to note that this regression results in Table¬†7 are not directly addressing our second causal question (Email A vs.¬†Email B). Instead, the coefficients obtained from the regression are comparing each email campaign against the control group. In any case, the table suggests that, on average, Campaign B generates more purchases. It remains to be determined whether this difference is statistically significant."
  },
  {
    "objectID": "posts/ab_testing1/1_ab_testing.html#distribution-comparison",
    "href": "posts/ab_testing1/1_ab_testing.html#distribution-comparison",
    "title": "A/B Testing Analysis",
    "section": "2.3. Distribution Comparison",
    "text": "2.3. Distribution Comparison\nFigure¬†3 illustrates the purchase distribution by treatment, providing a comprehensive view of the data. However, it‚Äôs important to note that while we have established a statistically significant difference, our estimation of this distribution assumes a Normal Distribution, which may not always hold true. In future articles, we will delve deeper into alternative methods that do not rely on such assumptions, ensuring a more robust analysis.\n\n\nCode\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Control vs Treated\ndf_control = df[df['treat'] == 'control']\ndf_treated = df[df['treat'] == 'treated']\n\n# Calculate mean and standard error for email_A and email_B\nmean_open_A = df_control['purch'].mean()\nstd_error_A = df_control['purch'].std() / np.sqrt(len(df_control))\n\nmean_open_B = df_treated['purch'].mean()\nstd_error_B = df_treated['purch'].std() / np.sqrt(len(df_treated))\n\nnormal_data_A = np.random.normal(mean_open_A, std_error_A, 10000)\nnormal_data_B = np.random.normal(mean_open_B, std_error_B, 10000)\n\nfig, axs = plt.subplots(1, 1)\n\nsns.histplot(normal_data_A, kde=True, stat='density', label='Control', bins=20, palette='viridis')\nsns.histplot(normal_data_B, kde=True, stat='density', label='Treated', bins=20, palette='viridis')\n\nplt.xlabel('Purchases')\nplt.ylabel('Density')\nplt.title('Distribution of Purchases, by Treatment')\nplt.axvline(mean_open_A, color=sns.color_palette()[0], linestyle='--', label='Mean Control')\nplt.axvline(mean_open_B, color=sns.color_palette()[1], linestyle='--', label='Mean Treated')\nplt.errorbar(mean_open_A, 0.02, xerr=std_error_A, fmt='o', color=sns.color_palette()[0], label='95% CI Control')\nplt.errorbar(mean_open_B, 0.02, xerr=std_error_B, fmt='o', color=sns.color_palette()[1], label='95% CI Treated')\nplt.legend(ncol=3, bbox_to_anchor=(0.5, -0.15), loc='upper center') \n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nFigure¬†3: Purchase Distribution, by Treatment\n\n\n\n\n\nAgain, assuming Normality, we can also estimate confidence intervals:\n\n\nCode\nimport scipy.stats as stats\n\ndf_control = df[df['treat'] == 'control']\ndf_treated = df[df['treat'] == 'treated']\n        \n# Calculate mean and standard error for email_A and email_B\nmean_open_A = df_control['purch'].mean()\nstd_error_A = df_control['purch'].std() / np.sqrt(len(df_control))\n\nmean_open_B = df_treated['purch'].mean()\nstd_error_B = df_treated['purch'].std() / np.sqrt(len(df_treated))\n    \n# Calculate the confidence interval using the t-distribution\nconfidence_interval_A = stats.norm.interval(0.95, loc=mean_open_A, scale=std_error_A)\nconfidence_interval_B = stats.norm.interval(0.95, loc=mean_open_B, scale=std_error_B)\n\n\n\n\nMean Response Control: 12.4\nMean Response Treated: 25.7\n95% Confidence Interval for Control: [12.00, 12.84]\n95% Confidence Interval for Treated: [25.34, 26.15]"
  },
  {
    "objectID": "posts/ab_testing1/1_ab_testing.html#email-a-vs-email-b",
    "href": "posts/ab_testing1/1_ab_testing.html#email-a-vs-email-b",
    "title": "A/B Testing Analysis",
    "section": "2.4. Email A vs Email B",
    "text": "2.4. Email A vs Email B\nNow, let‚Äôs turn our attention to the second causal question: Which email design, Email A or Email B, is more effective in driving engagement and sales?\n\n\nCode\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef plot_variable_comparison(df):\n    variables = ['open', 'click', 'purch']\n\n    fig, axs = plt.subplots(1, 3, figsize=(18, 6))\n\n    for i, variable in enumerate(variables):\n        # Control vs Treated\n        df_control = df[df['group'] == 'email_A']\n        df_treated = df[df['group'] == 'email_B']\n\n        # Calculate mean and standard error for control and treated groups\n        mean_control = df_control[variable].mean()\n        std_error_control = df_control[variable].std() / np.sqrt(len(df_control))\n        mean_treated = df_treated[variable].mean()\n        std_error_treated = df_treated[variable].std() / np.sqrt(len(df_treated))\n\n        # Generate normal distribution data for control and treated groups\n        normal_data_control = np.random.normal(mean_control, std_error_control, 10000)\n        normal_data_treated = np.random.normal(mean_treated, std_error_treated, 10000)\n\n        # Plot histograms for control and treated groups\n        sns.histplot(normal_data_control, kde=True, stat='density', label='Control', bins=20, palette='viridis', ax=axs[i])\n        sns.histplot(normal_data_treated, kde=True, stat='density', label='Treated', bins=20, palette='viridis', ax=axs[i])\n\n        # Set labels and title\n        axs[i].set_xlabel(f'{variable.capitalize()}')\n        axs[i].set_ylabel('Density')\n        axs[i].set_title(f'Distribution of {variable.capitalize()}, by Treatment')\n\n        # Plot mean and error bars\n        axs[i].axvline(mean_control, color=sns.color_palette()[0], linestyle='--', label='Mean Control')\n        axs[i].axvline(mean_treated, color=sns.color_palette()[1], linestyle='--', label='Mean Treated')\n        axs[i].errorbar(mean_control, 0.02, xerr=std_error_control, fmt='o', color=sns.color_palette()[0], label='95% CI Control')\n        axs[i].errorbar(mean_treated, 0.02, xerr=std_error_treated, fmt='o', color=sns.color_palette()[1], label='95% CI Treated')\n\n        # Add legend\n        #axs[i].legend(ncol=3, bbox_to_anchor=(0.5, -0.15), loc='upper center') \n\n    #\n    handles, labels = axs[i].get_legend_handles_labels()\n    fig.legend(handles, labels, loc='upper center', ncol=3, bbox_to_anchor=(0.5, 0.0))\n    \n    # Adjust layout\n    plt.tight_layout()\n    plt.show()\n\n\n# Example usage:\nplot_variable_comparison(df)\n\n\n\n\n\n\n\n\nFigure¬†4: Outcome Distribution, by AB Treatment\n\n\n\n\n\n\n\nCode\nimport scipy.stats as stats\n\ndf_email_A = df[df['group'] == 'email_A']\ndf_email_B = df[df['group'] == 'email_B']\n        \n# Calculate mean and standard error for email_A and email_B\nmean_open_A = df_email_A['purch'].mean()\nstd_error_A = df_email_A['purch'].std() / np.sqrt(len(df_email_A))\n\nmean_open_B = df_email_B['purch'].mean()\nstd_error_B = df_email_B['purch'].std() / np.sqrt(len(df_email_B))\n    \n# Calculate the confidence interval using the t-distribution\nconfidence_interval_A = stats.norm.interval(0.95, loc=mean_open_A, scale=std_error_A)\nconfidence_interval_B = stats.norm.interval(0.95, loc=mean_open_B, scale=std_error_B)\n\n\n\n\nMean Response email_A: 25.6\nMean Response email_B: 25.9\n95% Confidence Interval for email_A: [25.06, 26.19]\n95% Confidence Interval for email_B: [25.29, 26.44]\n\n\nInterestingly enough, Email A appears to outperform Email B in terms of opens and clicks (Figure¬†4). However, this level of engagement does not necessarily translate to higher purchase rates. Purchases from Email B are higher on average, but the large overlap in their distribution is telling us that this difference might not be significant. While purchases from Email B are higher on average, the large overlap in their distribution suggests that this difference might not be significant.\nTo confirm this conclusion, we can conduct a linear regression comparing Email A and Email B purchases:\n\n\nCode\ndf_ab = df[df['group'] != 'ctrl']\nsmf.ols('purch ~ group', data=df_ab).fit().summary().tables[1]\n\n\n\n\nTable¬†8: A/B Regression Results III\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n25.6228\n0.291\n88.070\n0.000\n25.053\n26.193\n\n\ngroup[T.email_B]\n0.2435\n0.411\n0.592\n0.554\n-0.563\n1.050\n\n\n\n\n\n\n\n\n\nIndeed, despite observing higher purchases associated with Email B, the obtained p-value of 0.554 indicates that this difference is not statistically significant.\nThese findings show how important it is to look at different metrics and really dig into the data to figure out what‚Äôs working best for engaging customers and boosting sales."
  },
  {
    "objectID": "posts/nonparametrics/nonparametrics.html",
    "href": "posts/nonparametrics/nonparametrics.html",
    "title": "Nonparametric & Semiparametric Binary-Choice Regressions",
    "section": "",
    "text": "Nonparametric Kernel Density, Nonparametric Regression and Binary-Choice Models.\nThe data set used in this assignment comes from Pindyck and Rubinfeld (1998) ‚ÄúEconometric Models and Economic Forecasts‚Äù and contains the following variables: a dummy whether children attend private school (private), number of years the family has been at the present residence (years), log of property tax (logptax), log of income (loginc), and whether one voted for an increase in property taxes (vote). There are two dependent variables of interest -private and vote- which can be modeled individually by two univariate probit models or jointly by one bivariate probit model, for instance.\nLink to PDF HERE."
  },
  {
    "objectID": "posts/nonparametrics/nonparametrics.html#kernel-density-estimation",
    "href": "posts/nonparametrics/nonparametrics.html#kernel-density-estimation",
    "title": "Nonparametric & Semiparametric Binary-Choice Regressions",
    "section": "Kernel Density Estimation",
    "text": "Kernel Density Estimation\nBefore running a binary choice model to explain the voting choice, we plot different Kernel Density estimates of loginc to check whether it follows approximately a normal distribution. The idea is to test visually if the normal distribution falls within the 95% confidence interval area of each kernel density estimate:\n\n\n\n\n\n\nFigure¬†1: Kernel Density Estimation of Log(Income)\n\n\n\nAll these Kernel Density estimators in Figure¬†1 use an alternative Epanechnikov Kernel function (this is the default in the kdens command in STATA). The differences among the estimator are explained by the different methods of choosing the Optimal Bandwidth \\(h^*\\) that minimizes the Mean Integrated Squared Errors (MISE)1. Comparing the kernel densities of the log income against the normal distribution, we observe that the normal distribution seems to not quite fit inside the confidence intervals in none of the kernel estimators. While the case of an optimal bandwidth of \\(h^*=0.188\\) in Figure 1.b. seems to present some evidence about normality, you can still see how the purple line appears outside the boundaries around the 9 to 9.5 value.\nBoth Plug-in methods (Figure 1.a. and 1.c.) use a lower Optimal Bandwidth (0.11 and 0.12, respectively), thus, leading to less bias but more variance in their estimates. In these cases, the normal distribution line appears outside the 95% confidence boundaries in several places. Overall, the visual test seems to reject the hypothesis that log incomes follow a normal distribution."
  },
  {
    "objectID": "posts/nonparametrics/nonparametrics.html#binary-choice-model-probit",
    "href": "posts/nonparametrics/nonparametrics.html#binary-choice-model-probit",
    "title": "Nonparametric & Semiparametric Binary-Choice Regressions",
    "section": "Binary Choice Model (probit)",
    "text": "Binary Choice Model (probit)\nTable 1 reports the results of different Binary choice models to predict voting behavior using years, logptax and loginc as independent variables:\n\n\n\nTable¬†1: Binary Choice Model Results\n\n\n\n\n\n\nColumn 1 in Table¬†1 presents the results of the probit model. Here we see a positive and statistically significant relationship between the level of log income and the probability of voting for higher property taxes. Also, once again we observe that property taxes have a negative effect on voting in favor of higher taxes of this kind. Finally, the number of years in the residence did not report a significant result."
  },
  {
    "objectID": "posts/nonparametrics/nonparametrics.html#normality-assumption",
    "href": "posts/nonparametrics/nonparametrics.html#normality-assumption",
    "title": "Nonparametric & Semiparametric Binary-Choice Regressions",
    "section": "Normality Assumption",
    "text": "Normality Assumption\nWe can use a visual test to check the normality assumption of the probit model estimated in Column 1. Recall that in the probit model, the probability that \\(y_i\\) takes on the value 1 is modeled as a nonlinear function of a linear combination of a set of independent variables:\n\\[\n\\Pr(y_i=1|x_i)  = \\Phi(x_i'\\beta)\n\\]\nwhere \\(\\Phi(\\cdot)\\) is assumed to be the standard normal cumulative distribution function (CDF). To check if this assumption is reasonable, we performed nonparametric estimations of \\(\\Pr(y_i=1|x_{i'\\beta})\\). The first estimator is a Nadaraya-Watson Regression (or constant constant estimator) that assumes a constant \\(m(x)=b_0(x_0)\\) around some neighborhood of \\(x_0\\):\n\\[\n\\hat{m}_h(x_0)=\\frac{\\sum^N_{i=1} K_x\\left(\\frac{x_0-x_i}{h} \\right)}{\\sum^N_{i=1} K_x\\left(\\frac{x_0-x_j}{h} \\right)}y_i\n\\]\nThe Nadaraya-Watson estimator is essentially a weighted local average of the observations (weights defined by a Kernel function) at some neighborhood defined by the choice of bandwidth \\(h\\). The second estimator is a Local Linear Regression that, instead of assuming a constant average, lets \\(m(x)\\) be linear in the neighborhood of \\(x_0\\). More concretely, the local linear regression minimizes with respect to \\(b_0(x_0)\\) and \\(b_1(x_0)\\):\n\\[\n\\sum^N_{i=1} K_x\\left(\\frac{x_i-x_0}{h} \\right)[y_i-b_0(x_0)-b_1(x_0)(x_i-x_0)]^2\n\\]\nBoth nonparametric regressions use the Epanechnikov Kernel function. Regarding the choice of bandw\n\n\n\nTable¬†2: Binary Choice Model Results\n\n\n\n\n\n\nidth \\(h^*\\), I use the command npregress that performs the Leave-One-Out Cross Validation method (LOOCV) to estimate the Optimal Bandwidth size2. Figure¬†2 compares these regressions against the probit estimation:\n\n\n\n\n\n\nFigure¬†2: Nonparametric Estimation of Het Probit\n\n\n\nIn general, we see that the normality assumption about the error term \\(\\varepsilon_i\\) in the probit model is actually not rejected, as the probit curve of the \\(\\Pr(y_i=1|x_i'\\beta)\\) tends to be within the 95% confidence intervals of both the Local Constant and Local Linear nonparametric estimators. This means that the standard probit model does not suffer from misspecification."
  },
  {
    "objectID": "posts/nonparametrics/nonparametrics.html#heteroscedastic-probit-model",
    "href": "posts/nonparametrics/nonparametrics.html#heteroscedastic-probit-model",
    "title": "Nonparametric & Semiparametric Binary-Choice Regressions",
    "section": "Heteroscedastic Probit Model",
    "text": "Heteroscedastic Probit Model\nColumn 2 of Table¬†2 presents the results of the heteroscedastic probit model. This specification still assumes that the error term of the latent model follows a normal distribution, but is more flexible in the sense that it no longer assumes its variance to be 1, but that it can vary as a function of some set of explanatory variables. In this case, we only use the variable years to model the variance:\n\\[\nV(\\varepsilon_i|x_i)=\\sigma^2_i=[\\exp(\\text{years}_{i}'\\gamma)]^2\n\\]\nwhere \\(\\varepsilon_i\\sim N[0,\\exp(\\text{years}_{i}'\\gamma)]\\). The probability of success is now represented by:\n\\[\n\\Pr(y_i=1|x_i)  = \\Phi\\left( \\frac{x_i'\\beta}{\\sigma_i} \\right) = \\Phi\\left( \\frac{x_i'\\beta}{\\exp(\\text{years}_{i}'\\gamma)} \\right)\n\\]\nHere we are primarily interested in testing whether \\(\\gamma\\neq 0\\), which seems to be the case as we see in Table¬†2 a statistically significant coefficient. Additionally, the likelihood-ratio test of heteroskedasticity, which tests the full model with heteroskedasticity against the full model without, is significant as \\(\\chi^2(1) = 11.55\\). Regarding the coefficients, we now see that the number of years in the residence impacts negatively the probability of voting for higher property taxes. The remaining variables are still significant and with the same sign relative to the standard probit model, but the coefficients are more pronounced."
  },
  {
    "objectID": "posts/nonparametrics/nonparametrics.html#nonparametric-regression",
    "href": "posts/nonparametrics/nonparametrics.html#nonparametric-regression",
    "title": "Nonparametric & Semiparametric Binary-Choice Regressions",
    "section": "Nonparametric Regression",
    "text": "Nonparametric Regression\nWe now test again the normality assumption of the heteroscedastic probit model by visually comparing its CDF with the two nonparametric alternatives already discussed:\n\n\n\n\n\n\nFigure¬†3: Nonparametric Estimation of Heteroscedastic Probit\n\n\n\nClearly, the heteroscedastic alternative -that assumes normality- highly deviates from both nonparametric estimations. Hence, it seems that the normality assumption in the hetprobit model is rejected. Furthermore, the probit link function is no longer smooth when compared to the standard probit. Modeling the variance as dependent on the explanatory variable years makes the prediction less linear, leading to a more noisy curve than smoothed."
  },
  {
    "objectID": "posts/nonparametrics/nonparametrics.html#klein-spady-semiparametric-regression",
    "href": "posts/nonparametrics/nonparametrics.html#klein-spady-semiparametric-regression",
    "title": "Nonparametric & Semiparametric Binary-Choice Regressions",
    "section": "Klein & Spady Semiparametric Regression",
    "text": "Klein & Spady Semiparametric Regression\nSemiparametric alternatives do not rely on the parametric assumptions about the shape of the error term distribution \\(\\varepsilon_i\\). The Klein & Spady method is a Single Index model with a \\(\\Pr(y_i|x_i)= F(x_i'\\beta)\\) structure that only depends on \\(x_i\\) through a single linear combination, \\(x_i'\\beta\\), whereas the function \\(F(\\cdot)\\) is left unspecified. The estimation is based on some kind of Maximum Likelihood Estimation:\n\\[\nL(\\beta,h)=\\sum^N_{i=1}(1-y_i)\\ln[1-\\hat{F}_{-i,n}(x_i'\\beta)]+\\sum^N_{i=1}y_i\\ln[\\hat{F}_{-i,n}(x_i'\\beta)]\n\\]\nSince \\(F(\\cdot)\\) is unknown, Klein & Spady suggest replacing it with a Leave-One-Out Nadaraya-Watson estimator \\(\\hat{F}_{-i,n}(\\cdot)\\). In order to guarantee point identification, the coefficient of one continuous variable is normalized to 1, which in our case will be the variable loginc. Additionally, in single index models, the function \\(F(\\cdot)\\) will include any location and level shift, so the vector \\(x_i\\) cannot include an intercept. Column 3 of Table¬†2 shows the results of this semiparametric regression. But because we want to make some comparisons between the different models so far presented, we will also need to scale normalize the previous models, as in Table¬†2 they are not directly comparable. Table¬†3 shows the results of each model after scale normalization:\n\n\n\nTable¬†3: Binary Choice Models: Normalized Results\n\n\n\n\n\n\nHere we see that the direction of the signs in every specification is the same: years and logptax are negatively associated with the probability of voting in favor of the tax reform. The coefficients of the property tax variable are to some degree similar across models, but there are important differences regarding the coefficient linked to the number of years in residence. First, the probit model did not show an effect statistically distinct from zero, whereas the other two models did show significant and more pronounced coefficients.\nBetween the heteroscedastic probit and the Klein & Spady regression, the hetprobit seems to report a more negative effect of years on the probability of voting for higher taxes. In order to understand the magnitude of these differences between models, we will need to estimate marginal effects."
  },
  {
    "objectID": "posts/nonparametrics/nonparametrics.html#semiparametric-regression-visualization",
    "href": "posts/nonparametrics/nonparametrics.html#semiparametric-regression-visualization",
    "title": "Nonparametric & Semiparametric Binary-Choice Regressions",
    "section": "Semiparametric Regression Visualization",
    "text": "Semiparametric Regression Visualization\nFigure¬†4 visualize the Klein & Spady semiparametric estimate of \\(\\Pr(y_i=1|x_i'\\beta)\\) and compares it against the heteroskedastic estimation:\n\n\n\n\n\n\nFigure¬†4: Semiparametric Estimation of Probits\n\n\n\nThe semiparametric regression in Figure¬†4 this case is quite similar to the ``U‚Äô‚Äô shape of the nonparametric (Nadara-Watson) regression from Figure 3.a. that is based on the predicted values \\(x_i'\\beta\\) of the heteroscedastic probit to estimate the probabilities. However, we also see that the (parametric) heteroscedastic probit falls outside the 95% confidence interval area at various points ( Figure¬†4 in green). Hence, while we cannot use the hetprobit model to draw conclusions on the probability of voting for higher property taxes, we do can use a nonparametric version of the predicted values of the heteroscedastic model for that purposes.\nMost of the behavior behind this strange shape of the link function -behavior that is mostly happening at \\(x_i'\\beta&lt;-5\\) in Figure¬†4 -, can be explained by the distribution of the years variable. From Figure¬†5 we can observe that this group consists of 5 observations that have an average number of years of 39, versus the 6.8 from the rest of the sample:\n\n\n\n\n\n\nFigure¬†5: Kernel Density Estimation of Years\n\n\n\nAs we have seen from the models, the number of years has in general a negative effect on the probability of voting for more taxes, but here we have that all these 5 observations have a high number of years in the residence and all of them have vote=1. Hence, this group of ``outliers‚Äô‚Äô might be generating a countervailing effect on the true point estimates of years."
  },
  {
    "objectID": "posts/nonparametrics/nonparametrics.html#restricted-model",
    "href": "posts/nonparametrics/nonparametrics.html#restricted-model",
    "title": "Nonparametric & Semiparametric Binary-Choice Regressions",
    "section": "Restricted Model",
    "text": "Restricted Model\nWe now re-run the binary choice models so far discussed, but restricting the estimation for observations where the number of years in residence is smaller than 25. Table¬†4 shows the main results after normalization to make them comparable:\n\n\n\nTable¬†4: Binary Choice Models: Normalized Results (years &lt; 25)\n\n\n\n\n\n\nHere we see surprisingly similar conclusions on the point estimates across different specifications. In the standard probit model, we now see that years have statistically significant results, with a more intense negative effect on our dependent variable than before. The heteroscedastic probit now reports a more negative coefficient on the variable logptax, significant only at the 10% level, but for the number of years, we see a less pronounced effect and is no longer significant. Looking at \\(\\gamma\\), note that now we cannot reject the null hypothesis of homoscedastic errors so that, after filtering for the extreme values of years, there is no evidence of this model being better than the standard probit.\nFinally, the semiparametric model seems to be the model that changed the least. Both the direction of the signs and their significance remain as before, with years having a slightly less pronounced coefficient, and logptax reporting now a more negative effect on the probability of voting for higher taxes than before.\nWe want now to test the normality assumption, so we re-estimate the nonparametric regressions from previous exercises to make the visual comparison. In this case, I only perform the Nadaraya-Watson Estimator:\n\n\n\n\n\n\nFigure¬†6: Non and Semiparametric Estimation (years&lt;25)\n\n\n\nAgain, we see that all both the probit and hetprobit models falls within the boundaries of the confidence interval of the nonparametric estimates. Taking all this evidence together, we can conclude that, after filtering the extreme values of years, the three binary choice models are no longer much different from each other. Finally, because now there is no evidence of heteroscedasticity and we did not reject the normality assumption about the error terms \\(\\varepsilon_i\\), the standard probit model is the preferred model in my view."
  },
  {
    "objectID": "posts/nonparametrics/nonparametrics.html#nonparametric-and-semiparametric-regression-comparison",
    "href": "posts/nonparametrics/nonparametrics.html#nonparametric-and-semiparametric-regression-comparison",
    "title": "Nonparametric & Semiparametric Binary-Choice Regressions",
    "section": "Nonparametric and Semiparametric Regression Comparison",
    "text": "Nonparametric and Semiparametric Regression Comparison\nUsing the standard probit estimates, we want now to graphically compare the estimates between the Nadaraya-Watson (Local Constant), the Local Linear, and the Local Quadratic regression. The following figure summarizes the results:\n\n\n\n\n\n\nFigure¬†7: Nonparametric and Semiparametric Regression - Comparison\n\n\n\nAlthough the curves seem to be very similar, we can identify some differences at specific points. Generally speaking, the degree of bias of the Nadaraya-Watson highly depends on the curvature or second derivative \\(m''(x)\\) of the regression function, as well on the slope of the density of the regressors \\(f'(x)\\):\n\\[\n\\text{bias}[\\hat{m}(x)]=\\frac{h^2\\mu_2}{2}\\left\\{m''(x)+2\\frac{f'_x(x)m'(x)}{f_x(x)} \\right\\}+O(\\frac{1}{nh})+o(h^2)\n\\]\nNamely, the bias will be very large at a point where the density function curves a lot. In Figure¬†7 (and assuming the normality distribution is the true function), we can actually observe that the Local Constant estimator has a large and positive bias (i) between \\(-1&lt;x_i'\\beta&lt;0.5\\), which is the place where the density is curving positively and very quickly (positive curvature) and (ii) between \\(0.5&lt;x_i'\\beta&lt;1.55\\), marking the space where the density function is decreasing quickly (negative curvature). The local linear and local quadratic regressions also present this described behavior, but with less biasedness. The local linear is the more smoothed function among the three.\nOne reason that explains this difference may be due to the fact that the bias of the Local Linear (and Quadratic) regression does not depend on the density function of the regressors, also called design bias:\n\\[\\begin{align*}\n    \\text{bias}[\\hat{m}(x)]=\\frac{h^2\\mu_2}{2}m''(x)+o(h^2)\n\\end{align*}\\]\nIn that sense, the design bias adds an extra bias to the Nadaraya-Watson estimator. Finally, we observe more differences between the estimators on the extremes. On the bottom left (between -1.2 and -2), for example, the Nadaraya-Watson starts with higher bias, but then it shifts quickly to be equal to 0 towards the left, whereas the other two (which are very similar to each other) start with less bias, but then they end up with negative probabilities towards the left. On the other extreme we find the same behavior, where the local and quadratic estimators are not bounded to be within 0 and 1."
  },
  {
    "objectID": "posts/nonparametrics/nonparametrics.html#footnotes",
    "href": "posts/nonparametrics/nonparametrics.html#footnotes",
    "title": "Nonparametric & Semiparametric Binary-Choice Regressions",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAs the optimal bandwidth formula also depends on the curvature of the density \\(f''\\), which is unknown, there are different ways to deal with this issue. Some impose assumptions about the unknown distribution (Plug-in methods), and others are more data-driven such as cross-validation techniques.‚Ü©Ô∏é\nIn contrast, the command lpoly uses the Rule-of-Thumb method of bandwidth selection, which is a Plug-In estimator.‚Ü©Ô∏é"
  }
]