[
  {
    "objectID": "research.html",
    "href": "research.html",
    "title": "Research",
    "section": "",
    "text": "Copec-CGL Merger: An Ex-Post Assessment of the FNE Decision\n\n\nPublished analysis evaluating the competitive effects of a retail gasoline merger in Chile using difference-in-differences and spatial econometrics.\n\n\n\n2024\n\n\n\n\n\n\n\n\n\n\n\n\n\nImpact of Transit-Oriented Development on Dutch Commercial Real Estate Prices\n\n\nMaster thesis. Econometric analysis using difference-in-differences to assess the causal impact of transit infrastructure on commercial real estate values in the Netherlands.\n\n\n\n2024\n\n\n\n\n\n\n\n\n\n\n\n\n\nVoting Patterns of the Ministers of the Supreme Court in Antitrust Cases: Evidence from Chile\n\n\nPublished article analyzing judicial voting behavior in competition law cases using statistical methods.\n\n\n\n2022\n\n\n\n\n\n\nNo matching items\n Back to top"
  },
  {
    "objectID": "posts/synthetic_did/synthetic_did.html",
    "href": "posts/synthetic_did/synthetic_did.html",
    "title": "DiD vs Synthetic Control vs Synthetic DiD",
    "section": "",
    "text": "A unified comparison of Difference-in-Differences, Synthetic Control, and Synthetic Difference-in-Differences — when each method works, when it fails, and why SynthDiD offers the best of both worlds.\nIn policy evaluation, we often want to estimate the causal effect of an intervention (a law, a program, a shock) on a treated unit (a state, a firm, a country) by comparing its post-treatment trajectory to a suitable counterfactual: what would have happened without the treatment?\nThree leading methods construct this counterfactual in fundamentally different ways:\nArkhangelsky et al. (2021) showed that all three methods can be written as special cases of a single weighted regression — they differ only in their choice of weights. This elegant unification reveals exactly when and why each method succeeds or fails.\nIn this post, we:\nWe use the diff-diff Python package for DiD and SynthDiD estimation, and implement pure Synthetic Control manually."
  },
  {
    "objectID": "posts/synthetic_did/synthetic_did.html#difference-in-differences",
    "href": "posts/synthetic_did/synthetic_did.html#difference-in-differences",
    "title": "DiD vs Synthetic Control vs Synthetic DiD",
    "section": "1.1 Difference-in-Differences",
    "text": "1.1 Difference-in-Differences\nDiD uses uniform weights on all control units and all pre-treatment periods:\n\n\\hat{\\omega}_i^{\\text{did}} = \\frac{1}{N_{\\text{co}}} \\quad \\text{for all controls}, \\qquad \\hat{\\lambda}_t^{\\text{did}} = \\frac{1}{T_{\\text{pre}}} \\quad \\text{for all pre-periods}\n\nThis yields the familiar 2×2 DiD estimator:\n\n\\hat{\\tau}^{\\text{did}} = \\left(\\bar{Y}_{\\text{tr, post}} - \\bar{Y}_{\\text{tr, pre}}\\right) - \\left(\\bar{Y}_{\\text{co, post}} - \\bar{Y}_{\\text{co, pre}}\\right)\n\nKey assumption — Parallel trends: In the absence of treatment, treated and control units would have followed the same trajectory on average. Formally: \\mathbb{E}[Y_{it}(0) \\mid \\text{treated}] - \\mathbb{E}[Y_{it}(0) \\mid \\text{control}] is constant over t.\nStrength: Allows arbitrary level differences between treated and control units (absorbed by \\alpha_i).\nWeakness: Fails when units follow different trends (interactive fixed effects)."
  },
  {
    "objectID": "posts/synthetic_did/synthetic_did.html#synthetic-control",
    "href": "posts/synthetic_did/synthetic_did.html#synthetic-control",
    "title": "DiD vs Synthetic Control vs Synthetic DiD",
    "section": "1.2 Synthetic Control",
    "text": "1.2 Synthetic Control\nSC (Abadie, Diamond & Hainmueller, 2010) uses data-driven unit weights that match the treated unit’s pre-treatment outcomes:\n\n\\hat{\\omega}^{\\text{sc}} = \\arg\\min_{\\omega \\in \\Delta^{N_{\\text{co}}}} \\left\\| \\sum_{i=1}^{N_{\\text{co}}} \\omega_i \\, \\mathbf{Y}_{i, \\text{pre}} - \\mathbf{Y}_{\\text{tr, pre}} \\right\\|^2\n\nwhere \\Delta^{N_{\\text{co}}} = \\{\\omega \\geq 0 : \\sum_i \\omega_i = 1\\} is the simplex. Time weights are uniform. Crucially, SC uses no intercept — it constructs the counterfactual purely from weighted control outcomes:\n\n\\hat{\\tau}^{\\text{sc}} = \\bar{Y}_{\\text{tr, post}} - \\sum_{i=1}^{N_{\\text{co}}} \\hat{\\omega}_i^{\\text{sc}} \\, \\bar{Y}_{i, \\text{post}}\n\nKey assumption — Convex hull: The treated unit’s untreated potential outcomes must lie in the convex hull of control outcomes, both in levels and in factor loadings.\nStrength: Handles interactive fixed effects — different units can follow different trends, as long as the treated unit’s trajectory can be matched by a weighted combination of controls.\nWeakness: Fails when the treated unit’s level is outside the convex hull of controls. Since there is no intercept, SC tries to match levels through the weights, which can distort the factor loading match."
  },
  {
    "objectID": "posts/synthetic_did/synthetic_did.html#synthetic-difference-in-differences",
    "href": "posts/synthetic_did/synthetic_did.html#synthetic-difference-in-differences",
    "title": "DiD vs Synthetic Control vs Synthetic DiD",
    "section": "1.3 Synthetic Difference-in-Differences",
    "text": "1.3 Synthetic Difference-in-Differences\nSynthDiD (Arkhangelsky et al., 2021) combines the best of both methods:\n\nData-driven unit weights \\hat{\\omega}^{\\text{sdid}} (like SC): match the pre-treatment trajectory of the treated unit\nData-driven time weights \\hat{\\lambda}^{\\text{sdid}} (unique to SynthDiD): focus on the most informative pre-treatment periods\nIntercept adjustment (like DiD): unit fixed effects \\alpha_i absorb level differences\n\nThe unit weights solve:\n\n\\hat{\\omega}^{\\text{sdid}} = \\arg\\min_{\\omega \\in \\Delta^{N_{\\text{co}}}} \\left\\| \\sum_{i=1}^{N_{\\text{co}}} \\omega_i \\, \\mathbf{Y}_{i, \\text{pre}} - \\mathbf{Y}_{\\text{tr, pre}} \\right\\|^2 + \\zeta_{\\omega}^2 T_{\\text{pre}} \\|\\omega\\|_2^2\n\nThe time weights solve an analogous problem across pre-treatment periods:\n\n\\hat{\\lambda}^{\\text{sdid}} = \\arg\\min_{\\lambda \\in \\Delta^{T_{\\text{pre}}}} \\left\\| \\sum_{t=1}^{T_{\\text{pre}}} \\lambda_t \\, \\mathbf{Y}_{\\text{co}, t} - \\mathbf{Y}_{\\text{co, post}} \\right\\|^2 + \\zeta_{\\lambda}^2 N_{\\text{co}} \\|\\lambda\\|_2^2\n\nThese weights are then plugged into the unified TWFE regression (with intercept), yielding:\n\n\\hat{\\tau}^{\\text{sdid}} = \\left(\\bar{Y}_{\\text{tr, post}} - \\hat{\\omega}^{\\text{sdid}'} \\mathbf{\\bar{Y}}_{\\text{co, post}}\\right) - \\left( \\hat{\\lambda}^{\\text{sdid}'} \\mathbf{Y}_{\\text{tr, pre}} - \\hat{\\omega}^{\\text{sdid}'} \\hat{\\lambda}^{\\text{sdid}'} \\mathbf{Y}_{\\text{co, pre}} \\right)\n\nKey insight: The intercept (unit FE) absorbs level shifts that would break SC, while the data-driven unit weights match factor loadings that would violate parallel trends for DiD. The time weights add robustness by down-weighting pre-treatment periods that are less informative about the post-treatment counterfactual."
  },
  {
    "objectID": "posts/synthetic_did/synthetic_did.html#summary",
    "href": "posts/synthetic_did/synthetic_did.html#summary",
    "title": "DiD vs Synthetic Control vs Synthetic DiD",
    "section": "1.4 Summary",
    "text": "1.4 Summary\nThe following table summarizes the key differences:\n\n\n\n\n\n\n\n\n\n\nDiD\nSC\nSynthDiD\n\n\n\n\nUnit weights \\hat{\\omega}_i\nUniform\nData-driven (match pre-treatment)\nData-driven + regularized\n\n\nTime weights \\hat{\\lambda}_t\nUniform\nUniform\nData-driven + regularized\n\n\nIntercept / Unit FE\nYes\nNo\nYes\n\n\nHandles level shifts\nYes\nNo\nYes\n\n\nHandles non-parallel trends\nNo\nYes (if in convex hull)\nYes\n\n\nKey assumption\nParallel trends\nConvex hull (levels + factors)\nConvex hull (factors only)"
  },
  {
    "objectID": "posts/synthetic_did/synthetic_did.html#setup",
    "href": "posts/synthetic_did/synthetic_did.html#setup",
    "title": "DiD vs Synthetic Control vs Synthetic DiD",
    "section": "2.1 Setup",
    "text": "2.1 Setup\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\nfrom scipy.optimize import minimize\n\nfrom diff_diff import TwoWayFixedEffects, SyntheticDiD\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nnp.random.seed(42)\n\nc:\\Users\\danny\\anaconda3\\Lib\\site-packages\\pandas\\core\\computation\\expressions.py:22: UserWarning: Pandas requires version '2.10.2' or newer of 'numexpr' (version '2.8.7' currently installed).\n  from pandas.core.computation.check import NUMEXPR_INSTALLED\nc:\\Users\\danny\\anaconda3\\Lib\\site-packages\\pandas\\core\\arrays\\masked.py:56: UserWarning: Pandas requires version '1.4.2' or newer of 'bottleneck' (version '1.3.7' currently installed).\n  from pandas.core import (\n\n\n\ndef generate_panel(N_co=30, T_pre=20, T_post=10, tau=10.0,\n                   alpha_co=None, alpha_tr=3.0,\n                   gamma_co=None, gamma_tr=0.0,\n                   sigma=0.5, seed=42):\n    \"\"\"\n    Generate panel data from a latent factor model.\n    \n    Y_it(0) = delta_t + alpha_i + gamma_i * f_t + eps_it\n    Y_it(1) = Y_it(0) + tau  (for treated, post-treatment only)\n    \n    Returns: (df, info_dict)\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    N = N_co + 1  # 1 treated unit\n    T = T_pre + T_post\n    t_idx = np.arange(T)\n    \n    # Common time effects: smooth trend\n    delta_t = 2 + 0.3 * t_idx\n    \n    # Factor: periodic with period T_pre (post mirrors early pre)\n    f_t = 5 * np.sin(2 * np.pi * t_idx / T_pre)\n    \n    # Unit fixed effects\n    if alpha_co is None:\n        alpha_co = rng.uniform(1, 5, N_co)\n    alpha = np.append(alpha_co, alpha_tr)\n    \n    # Factor loadings\n    if gamma_co is None:\n        gamma_co = np.zeros(N_co)\n    gamma = np.append(gamma_co, gamma_tr)\n    \n    # Generate potential outcomes Y(0)\n    Y = np.zeros((N, T))\n    for i in range(N):\n        Y[i, :] = delta_t + alpha[i] + gamma[i] * f_t + rng.normal(0, sigma, T)\n    \n    # Store counterfactual for treated unit\n    Y_cf = Y[-1, :].copy()\n    \n    # Add treatment effect to treated unit in post-period\n    Y[-1, T_pre:] += tau\n    \n    # Build long-format DataFrame\n    rows = []\n    for i in range(N):\n        for t in range(T):\n            rows.append({\n                'unit': i,\n                'period': t,\n                'y': Y[i, t],\n                'treated': int(i == N - 1),\n                'post': int(t &gt;= T_pre),\n            })\n    df = pd.DataFrame(rows)\n    \n    info = {\n        'Y': Y, 'Y_cf': Y_cf, 'alpha': alpha, 'gamma': gamma,\n        'delta_t': delta_t, 'f_t': f_t, 'T_pre': T_pre, 'T_post': T_post,\n        'N_co': N_co, 'tau': tau, 't_idx': t_idx,\n    }\n    return df, info\n\n\ndef estimate_did_manual(Y, N_co, T_pre):\n    \"\"\"Manual 2x2 DiD: (Y_tr_post - Y_tr_pre) - (Y_co_post - Y_co_pre).\"\"\"\n    Y_co = Y[:N_co, :]\n    Y_tr = Y[N_co:, :]\n    \n    co_pre = Y_co[:, :T_pre].mean()\n    co_post = Y_co[:, T_pre:].mean()\n    tr_pre = Y_tr[:, :T_pre].mean()\n    tr_post = Y_tr[:, T_pre:].mean()\n    \n    tau_did = (tr_post - tr_pre) - (co_post - co_pre)\n    \n    # DiD counterfactual for the treated unit (each period)\n    co_change = Y_co[:, :].mean(axis=0) - co_pre\n    cf_did = tr_pre + co_change\n    \n    return tau_did, cf_did\n\n\ndef estimate_sc_manual(Y, N_co, T_pre):\n    \"\"\"Pure Synthetic Control via constrained optimization.\"\"\"\n    Y_co_pre = Y[:N_co, :T_pre].T   # (T_pre, N_co)\n    Y_tr_pre = Y[N_co, :T_pre]      # (T_pre,)\n    Y_tr_post = Y[N_co, T_pre:]     # (T_post,)\n    \n    N = N_co\n    \n    # Objective: ||Y_co_pre @ w - Y_tr_pre||^2\n    def objective(w):\n        return np.sum((Y_co_pre @ w - Y_tr_pre) ** 2)\n    \n    # Constraints: w &gt;= 0, sum(w) = 1\n    constraints = [{'type': 'eq', 'fun': lambda w: np.sum(w) - 1}]\n    bounds = [(0, 1)] * N\n    w0 = np.ones(N) / N\n    \n    res = minimize(objective, w0, method='SLSQP', bounds=bounds, constraints=constraints)\n    w_sc = res.x\n    \n    # SC counterfactual (all periods): Y_co.T is (T, N_co), w_sc is (N_co,)\n    cf_sc = Y[:N_co, :].T @ w_sc  # (T,)\n    \n    # Treatment effect: mean post-treatment gap\n    # Y_co_post is (T_post, N_co), w_sc is (N_co,) -&gt; (T_post,)\n    Y_co_post = Y[:N_co, T_pre:].T\n    tau_sc = np.mean(Y_tr_post - Y_co_post @ w_sc)\n    \n    return tau_sc, cf_sc, w_sc\n\n\ndef estimate_all(df, info):\n    \"\"\"Run DiD, SC, and SynthDiD on a simulated panel. Returns dict of results.\"\"\"\n    Y = info['Y']\n    N_co = info['N_co']\n    T_pre = info['T_pre']\n    T_post = info['T_post']\n    \n    results = {}\n    \n    # 1. Manual DiD\n    tau_did, cf_did = estimate_did_manual(Y, N_co, T_pre)\n    results['DiD'] = {'tau': tau_did, 'cf': cf_did}\n    \n    # 2. Manual SC\n    tau_sc, cf_sc, w_sc = estimate_sc_manual(Y, N_co, T_pre)\n    results['SC'] = {'tau': tau_sc, 'cf': cf_sc, 'weights': w_sc}\n    \n    # 3. SynthDiD via diff-diff\n    post_periods = list(range(T_pre, T_pre + T_post))\n    sdid = SyntheticDiD(variance_method=\"placebo\", seed=42)\n    sdid_res = sdid.fit(\n        df, outcome='y', treatment='treated',\n        unit='unit', time='period', post_periods=post_periods,\n    )\n    results['SynthDiD'] = {'tau': sdid_res.att, 'se': sdid_res.se, 'result_obj': sdid_res}\n    \n    return results\n\n\ndef plot_scenario(info, results, title, figsize=(14, 5)):\n    \"\"\"\n    Two-panel figure:\n    Left: Time series with treated, control avg, counterfactuals.\n    Right: Bar chart comparing estimates to true tau.\n    \"\"\"\n    Y = info['Y']\n    Y_cf = info['Y_cf']\n    N_co = info['N_co']\n    T_pre = info['T_pre']\n    tau = info['tau']\n    t = info['t_idx']\n    \n    fig, axes = plt.subplots(1, 2, figsize=figsize, gridspec_kw={'width_ratios': [2.2, 1]})\n    \n    # --- Left panel: Time series ---\n    ax = axes[0]\n    \n    # Individual control units (faint)\n    for i in range(N_co):\n        ax.plot(t, Y[i, :], color='gray', alpha=0.1, linewidth=0.5)\n    \n    # Control average\n    co_avg = Y[:N_co, :].mean(axis=0)\n    ax.plot(t, co_avg, color='gray', linewidth=2, linestyle='--', label='Control Average', alpha=0.7)\n    \n    # True counterfactual\n    ax.plot(t, Y_cf, color='black', linewidth=1.5, linestyle=':', label='True Counterfactual', alpha=0.6)\n    \n    # Treated unit (observed)\n    ax.plot(t, Y[-1, :], color='#440154', linewidth=2.5, label='Treated (observed)')\n    \n    # SC counterfactual\n    if 'SC' in results and 'cf' in results['SC']:\n        ax.plot(t, results['SC']['cf'], color='#35b779', linewidth=2, linestyle='-.', \n                label=f\"SC Counterfactual ($\\\\hat{{\\\\tau}}$={results['SC']['tau']:.2f})\")\n    \n    # Treatment period shading\n    ax.axvline(T_pre - 0.5, color='red', linestyle='-', linewidth=1.5, alpha=0.7)\n    ax.axvspan(T_pre - 0.5, t[-1], alpha=0.05, color='red')\n    ax.text(T_pre + 0.5, ax.get_ylim()[0] + 0.5, 'Post-treatment', fontsize=10, color='red', alpha=0.7)\n    \n    ax.set_xlabel('Period', fontsize=12)\n    ax.set_ylabel('$Y_{it}$', fontsize=12)\n    ax.set_title(title, fontsize=14)\n    ax.legend(fontsize=9, loc='upper left')\n    ax.grid(True, alpha=0.2)\n    \n    # --- Right panel: Estimates bar chart ---\n    ax2 = axes[1]\n    methods = ['DiD', 'SC', 'SynthDiD']\n    estimates = [results[m]['tau'] for m in methods]\n    colors = ['#440154', '#35b779', '#fde725']\n    \n    bars = ax2.bar(methods, estimates, color=colors, edgecolor='white', width=0.6)\n    ax2.axhline(tau, color='black', linestyle='--', linewidth=1.5, label=f'True $\\\\tau$ = {tau}')\n    \n    # Add value labels on bars\n    for bar, est in zip(bars, estimates):\n        ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.2,\n                f'{est:.2f}', ha='center', va='bottom', fontsize=11, fontweight='bold')\n    \n    ax2.set_ylabel('Estimated $\\\\hat{\\\\tau}$', fontsize=12)\n    ax2.set_title('Estimates vs. Truth', fontsize=14)\n    ax2.legend(fontsize=10)\n    ax2.grid(True, alpha=0.2, axis='y')\n    \n    plt.tight_layout()\n    plt.show()"
  },
  {
    "objectID": "posts/synthetic_did/synthetic_did.html#scenario-1-parallel-trends-holds",
    "href": "posts/synthetic_did/synthetic_did.html#scenario-1-parallel-trends-holds",
    "title": "DiD vs Synthetic Control vs Synthetic DiD",
    "section": "2.2 Scenario 1: Parallel Trends Holds",
    "text": "2.2 Scenario 1: Parallel Trends Holds\nIn the baseline scenario, there are no interactive fixed effects (\\gamma_i = 0 for all units). The data-generating process is:\n\nY_{it}(0) = \\delta_t + \\alpha_i + \\varepsilon_{it}\n\nThis is the textbook DiD setting: all units share the same time trend \\delta_t, differing only in their levels \\alpha_i. Parallel trends holds by construction, so DiD should perform well. SC and SynthDiD should also work, since the treated unit’s trajectory is easily matched by any convex combination of controls.\n\n# Scenario 1: Parallel trends (gamma = 0 for all units)\ndf1, info1 = generate_panel(\n    N_co=30, T_pre=20, T_post=10, tau=10.0,\n    alpha_tr=3.0,          # within the control range [1, 5]\n    gamma_co=None,          # defaults to zeros\n    gamma_tr=0.0,           # no interactive FE\n    sigma=0.5, seed=42,\n)\n\nresults1 = estimate_all(df1, info1)\n\nprint(\"=\" * 50)\nprint(\"Scenario 1: Parallel Trends Holds\")\nprint(\"=\" * 50)\nprint(f\"True tau:     {info1['tau']:.2f}\")\nprint(f\"DiD:          {results1['DiD']['tau']:.2f}\")\nprint(f\"SC:           {results1['SC']['tau']:.2f}\")\nprint(f\"SynthDiD:     {results1['SynthDiD']['tau']:.2f} (SE: {results1['SynthDiD']['se']:.2f})\")\n\n==================================================\nScenario 1: Parallel Trends Holds\n==================================================\nTrue tau:     10.00\nDiD:          10.03\nSC:           10.09\nSynthDiD:     9.91 (SE: 0.18)\n\n\n\n\nCode\nplot_scenario(info1, results1, 'Scenario 1: Parallel Trends Holds')\n\n\n\n\n\n\n\n\nFigure 1: Scenario 1: When parallel trends holds, all three methods closely recover the true treatment effect.\n\n\n\n\n\nAs expected, when parallel trends holds, all three estimators closely recover the true treatment effect \\tau = 10. The left panel of Figure 1 shows that the treated unit’s pre-treatment trajectory runs parallel to the control average — exactly the setting DiD was designed for. The SC counterfactual (green dash-dot line) tracks the true counterfactual (black dotted) closely as well.\nThe right panel confirms that all three estimates cluster tightly around the truth. This is the baseline: when your assumptions are satisfied, simpler methods work just fine."
  },
  {
    "objectID": "posts/synthetic_did/synthetic_did.html#scenario-2-parallel-trends-fails-interactive-fixed-effects",
    "href": "posts/synthetic_did/synthetic_did.html#scenario-2-parallel-trends-fails-interactive-fixed-effects",
    "title": "DiD vs Synthetic Control vs Synthetic DiD",
    "section": "2.3 Scenario 2: Parallel Trends Fails (Interactive Fixed Effects)",
    "text": "2.3 Scenario 2: Parallel Trends Fails (Interactive Fixed Effects)\nNow we introduce interactive fixed effects: each unit has its own loading \\gamma_i on the common factor f_t:\n\nY_{it}(0) = \\delta_t + \\alpha_i + \\gamma_i \\cdot f_t + \\varepsilon_{it}\n\nCrucially, the treated unit’s level \\alpha_{\\text{tr}} = 3 remains within the convex hull of controls (\\alpha_{\\text{co}} \\sim U[1, 5]), and its factor loading \\gamma_{\\text{tr}} = 2.5 is within the range of control loadings (\\gamma_{\\text{co}} \\sim U[0, 3]).\nSince different units now follow different trends (depending on \\gamma_i), parallel trends is violated. DiD should be biased. However, SC can construct a synthetic control that matches both the level and the factor loading of the treated unit, so it should recover the true effect.\n\n# Scenario 2: Interactive FE, treated unit within convex hull\nrng2 = np.random.default_rng(42)\ngamma_co_2 = rng2.uniform(0, 3, 30)  # control factor loadings\n\ndf2, info2 = generate_panel(\n    N_co=30, T_pre=20, T_post=10, tau=10.0,\n    alpha_tr=3.0,            # within control range\n    gamma_co=gamma_co_2,     # heterogeneous factor loadings\n    gamma_tr=2.5,            # within control range\n    sigma=0.5, seed=42,\n)\n\nresults2 = estimate_all(df2, info2)\n\nprint(\"=\" * 50)\nprint(\"Scenario 2: Interactive Fixed Effects\")\nprint(\"=\" * 50)\nprint(f\"True tau:     {info2['tau']:.2f}\")\nprint(f\"DiD:          {results2['DiD']['tau']:.2f}  (bias: {results2['DiD']['tau'] - info2['tau']:+.2f})\")\nprint(f\"SC:           {results2['SC']['tau']:.2f}  (bias: {results2['SC']['tau'] - info2['tau']:+.2f})\")\nprint(f\"SynthDiD:     {results2['SynthDiD']['tau']:.2f}  (bias: {results2['SynthDiD']['tau'] - info2['tau']:+.2f})\")\n\n==================================================\nScenario 2: Interactive Fixed Effects\n==================================================\nTrue tau:     10.00\nDiD:          12.63  (bias: +2.63)\nSC:           8.91  (bias: -1.09)\nSynthDiD:     9.78  (bias: -0.22)\n\n\n\n\nCode\nplot_scenario(info2, results2, 'Scenario 2: Interactive Fixed Effects')\n\n\n\n\n\n\n\n\nFigure 2: Scenario 2: When parallel trends fails due to interactive fixed effects, DiD is biased while SC and SynthDiD recover the true effect.\n\n\n\n\n\nFigure 2 tells a very different story from Scenario 1. The left panel shows that the treated unit (purple) no longer runs parallel to the control average (gray dashed) in the pre-treatment period — the trajectories visibly diverge due to the interactive fixed effects. The DiD counterfactual, which projects the control average’s change onto the treated unit, is therefore biased.\nThe SC counterfactual (green), by contrast, closely tracks the treated unit’s true counterfactual in both the pre- and post-treatment periods. This is because SC found a weighted combination of controls that matches the treated unit’s specific trajectory, including its factor loading.\nThe right panel confirms the key takeaway: DiD is substantially biased, while both SC and SynthDiD are close to the true \\tau = 10. This is exactly the scenario that motivated synthetic control methods — when units follow different trends, equal-weighting all controls (as DiD does) gives the wrong answer."
  },
  {
    "objectID": "posts/synthetic_did/synthetic_did.html#scenario-3-interactive-fe-level-shift",
    "href": "posts/synthetic_did/synthetic_did.html#scenario-3-interactive-fe-level-shift",
    "title": "DiD vs Synthetic Control vs Synthetic DiD",
    "section": "2.4 Scenario 3: Interactive FE + Level Shift",
    "text": "2.4 Scenario 3: Interactive FE + Level Shift\nThe final scenario combines interactive fixed effects with a level shift: the treated unit’s fixed effect \\alpha_{\\text{tr}} = 20 is far above the range of any control unit (\\alpha_{\\text{co}} \\sim U[1, 5]):\n\nY_{it}(0) = \\delta_t + \\alpha_i + \\gamma_i \\cdot f_t + \\varepsilon_{it}, \\quad \\alpha_{\\text{tr}} = 20 \\gg \\max_i \\alpha_i^{\\text{co}}\n\nThis creates a two-fold challenge:\n\nDiD fails because parallel trends is violated (interactive FE).\nSC fails because the treated unit’s level is outside the convex hull of controls. With weights constrained to the simplex (\\omega \\geq 0, \\sum \\omega = 1), SC cannot produce a synthetic control at level 20 from units at levels 1–5. It will try to match the level by putting all weight on the highest-level control, which may not have the right factor loading.\n\nSynthDiD should handle both: its unit FE \\alpha_i absorbs the level shift (like DiD), while the data-driven weights match the factor loading pattern (like SC).\n\n# Scenario 3: Interactive FE + level shift\nrng3 = np.random.default_rng(42)\ngamma_co_3 = rng3.uniform(0, 3, 30)\n\ndf3, info3 = generate_panel(\n    N_co=30, T_pre=20, T_post=10, tau=10.0,\n    alpha_tr=20.0,           # FAR outside control range [1, 5]\n    gamma_co=gamma_co_3,     # heterogeneous factor loadings\n    gamma_tr=2.5,            # within control range\n    sigma=0.5, seed=42,\n)\n\nresults3 = estimate_all(df3, info3)\n\nprint(\"=\" * 50)\nprint(\"Scenario 3: Interactive FE + Level Shift\")\nprint(\"=\" * 50)\nprint(f\"True tau:     {info3['tau']:.2f}\")\nprint(f\"DiD:          {results3['DiD']['tau']:.2f}  (bias: {results3['DiD']['tau'] - info3['tau']:+.2f})\")\nprint(f\"SC:           {results3['SC']['tau']:.2f}  (bias: {results3['SC']['tau'] - info3['tau']:+.2f})\")\nprint(f\"SynthDiD:     {results3['SynthDiD']['tau']:.2f}  (bias: {results3['SynthDiD']['tau'] - info3['tau']:+.2f})\")\n\n==================================================\nScenario 3: Interactive FE + Level Shift\n==================================================\nTrue tau:     10.00\nDiD:          12.63  (bias: +2.63)\nSC:           23.94  (bias: +13.94)\nSynthDiD:     9.78  (bias: -0.22)\n\n\n\n\nCode\nplot_scenario(info3, results3, 'Scenario 3: Interactive FE + Level Shift')\n\n\n\n\n\n\n\n\nFigure 3: Scenario 3: With both interactive fixed effects and a level shift, only SynthDiD recovers the true treatment effect.\n\n\n\n\n\nFigure 3 illustrates the scenario where SynthDiD shines. The left panel shows a dramatic level gap between the treated unit (purple, at ~Y \\approx 25) and the control units (gray, clustered around Y \\approx 5–12). The SC counterfactual (green) cannot reach the treated unit’s level — it is forced to stay within the convex hull of control outcomes.\nThe right panel makes the failure modes clear:\n\nDiD is biased by the interactive fixed effects (same issue as Scenario 2).\nSC is biased because the level shift distorts the weights: instead of matching the factor loading \\gamma_{\\text{tr}} = 2, SC puts all weight on the control unit with the highest level, which may have a very different \\gamma_i.\nSynthDiD absorbs the level shift via its intercept (unit FE) and matches the factor loading via data-driven weights, recovering a much more accurate estimate.\n\nThis is the core insight of Arkhangelsky et al. (2021): by combining DiD’s intercept adjustment with SC’s data-driven weighting, SynthDiD is robust to both level shifts and non-parallel trends."
  },
  {
    "objectID": "posts/synthetic_did/synthetic_did.html#monte-carlo-comparison",
    "href": "posts/synthetic_did/synthetic_did.html#monte-carlo-comparison",
    "title": "DiD vs Synthetic Control vs Synthetic DiD",
    "section": "2.5 Monte Carlo Comparison",
    "text": "2.5 Monte Carlo Comparison\nThe single realizations above are illustrative, but they depend on a particular draw of \\varepsilon_{it}. To confirm that the patterns are robust, we run a small Monte Carlo experiment: 200 replications of each scenario, varying only the noise draws.\n\nn_mc = 200\ntau_true = 10.0\n\n# Pre-draw fixed control loadings (same across MC runs, only noise varies)\nrng_mc = np.random.default_rng(0)\ngamma_co_mc = rng_mc.uniform(0, 3, 30)\n\nscenarios = {\n    'S1: Parallel Trends': {\n        'alpha_tr': 3.0, 'gamma_co': np.zeros(30), 'gamma_tr': 0.0,\n    },\n    'S2: Interactive FE': {\n        'alpha_tr': 3.0, 'gamma_co': gamma_co_mc, 'gamma_tr': 2.5,\n    },\n    'S3: Interactive FE\\n+ Level Shift': {\n        'alpha_tr': 20.0, 'gamma_co': gamma_co_mc, 'gamma_tr': 2.5,\n    },\n}\n\nmc_results = {s: {m: [] for m in ['DiD', 'SC', 'SynthDiD']} for s in scenarios}\n\nfor s_name, s_params in scenarios.items():\n    print(f\"Running MC for {s_name}...\", end=\" \")\n    for i in range(n_mc):\n        df_mc, info_mc = generate_panel(\n            N_co=30, T_pre=20, T_post=10, tau=tau_true,\n            alpha_tr=s_params['alpha_tr'],\n            gamma_co=s_params['gamma_co'],\n            gamma_tr=s_params['gamma_tr'],\n            sigma=0.5, seed=1000 + i,\n        )\n        \n        Y_mc = info_mc['Y']\n        N_co_mc = info_mc['N_co']\n        T_pre_mc = info_mc['T_pre']\n        \n        # DiD\n        tau_did, _ = estimate_did_manual(Y_mc, N_co_mc, T_pre_mc)\n        mc_results[s_name]['DiD'].append(tau_did)\n        \n        # SC\n        tau_sc, _, _ = estimate_sc_manual(Y_mc, N_co_mc, T_pre_mc)\n        mc_results[s_name]['SC'].append(tau_sc)\n        \n        # SynthDiD\n        post_periods = list(range(T_pre_mc, T_pre_mc + info_mc['T_post']))\n        try:\n            sdid = SyntheticDiD(seed=i)\n            sdid_res = sdid.fit(\n                df_mc, outcome='y', treatment='treated',\n                unit='unit', time='period', post_periods=post_periods,\n            )\n            mc_results[s_name]['SynthDiD'].append(sdid_res.att)\n        except Exception:\n            mc_results[s_name]['SynthDiD'].append(np.nan)\n    \n    print(\"done.\")\n\nprint(\"\\nMonte Carlo complete.\")\n\nRunning MC for S1: Parallel Trends... done.\nRunning MC for S2: Interactive FE... done.\nRunning MC for S3: Interactive FE\n+ Level Shift... done.\n\nMonte Carlo complete.\n\n\n\n\nCode\nfig, axes = plt.subplots(1, 3, figsize=(16, 5), sharey=True)\nmethods = ['DiD', 'SC', 'SynthDiD']\ncolors = {'DiD': '#440154', 'SC': '#35b779', 'SynthDiD': '#fde725'}\n\nfor ax, (s_name, s_data) in zip(axes, mc_results.items()):\n    for method in methods:\n        vals = np.array(s_data[method])\n        vals = vals[~np.isnan(vals)]\n        ax.hist(vals, bins=30, alpha=0.5, color=colors[method], label=method, density=True)\n    \n    ax.axvline(tau_true, color='black', linestyle='--', linewidth=2, label=f'True $\\\\tau$={tau_true}')\n    ax.set_xlabel('$\\\\hat{\\\\tau}$', fontsize=12)\n    ax.set_title(s_name, fontsize=13)\n    ax.legend(fontsize=9)\n    ax.grid(True, alpha=0.2)\n\naxes[0].set_ylabel('Density', fontsize=12)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nFigure 4: Monte Carlo distributions of treatment effect estimates across 200 replications. The dashed black line marks the true \\tau = 10.\n\n\n\n\n\n\nrows = []\nfor s_name, s_data in mc_results.items():\n    for method in ['DiD', 'SC', 'SynthDiD']:\n        vals = np.array(s_data[method])\n        vals = vals[~np.isnan(vals)]\n        bias = np.mean(vals) - tau_true\n        rmse = np.sqrt(np.mean((vals - tau_true)**2))\n        rows.append({\n            'Scenario': s_name.replace('\\n', ' '),\n            'Method': method,\n            'Mean $\\\\hat{\\\\tau}$': round(np.mean(vals), 3),\n            'Bias': round(bias, 3),\n            'RMSE': round(rmse, 3),\n        })\n\nmc_table = pd.DataFrame(rows)\nmc_table = mc_table.set_index(['Scenario', 'Method'])\nmc_table\n\n\n\nTable 1: Monte Carlo results: Bias, RMSE, and coverage across 200 replications.\n\n\n\n\n\n\n\n\n\n\n\nMean $\\hat{\\tau}$\nBias\nRMSE\n\n\nScenario\nMethod\n\n\n\n\n\n\n\nS1: Parallel Trends\nDiD\n9.993\n-0.007\n0.190\n\n\nSC\n9.999\n-0.001\n0.208\n\n\nSynthDiD\n10.004\n0.004\n0.232\n\n\nS2: Interactive FE\nDiD\n12.824\n2.824\n2.831\n\n\nSC\n10.019\n0.019\n0.230\n\n\nSynthDiD\n10.005\n0.005\n0.255\n\n\nS3: Interactive FE + Level Shift\nDiD\n12.824\n2.824\n2.831\n\n\nSC\n25.966\n15.966\n15.987\n\n\nSynthDiD\n10.005\n0.005\n0.255\n\n\n\n\n\n\n\n\n\n\nThe Monte Carlo results in Figure 4 and Table 1 confirm the patterns from the single realizations:\n\nScenario 1 (Parallel Trends): All three methods are approximately unbiased. DiD has the lowest RMSE, reflecting its efficiency advantage when its assumptions hold.\nScenario 2 (Interactive FE): DiD shows substantial bias, while SC and SynthDiD remain well-centered at \\tau = 10. SC has slightly lower RMSE because it doesn’t need to estimate time weights.\nScenario 3 (Interactive FE + Level Shift): Both DiD and SC are biased — DiD due to non-parallel trends, SC due to the level shift breaking the convex hull condition. Only SynthDiD remains approximately unbiased, demonstrating its robustness.\n\nThe key takeaway: SynthDiD is the only method that performs well across all three scenarios. It is never the single best performer (DiD wins in Scenario 1, SC ties it in Scenario 2), but it is never badly biased either. This “minimax” property — avoiding the worst case rather than optimizing for the best case — is precisely the motivation of Arkhangelsky et al. (2021)."
  },
  {
    "objectID": "posts/nonparametrics/nonparametrics.html",
    "href": "posts/nonparametrics/nonparametrics.html",
    "title": "Nonparametric & Semiparametric Binary-Choice Regressions",
    "section": "",
    "text": "Nonparametric Kernel Density, Nonparametric Regression and Binary-Choice Models.\nThe data set used in this assignment comes from Pindyck and Rubinfeld (1998) “Econometric Models and Economic Forecasts” and contains the following variables: a dummy whether children attend private school (private), number of years the family has been at the present residence (years), log of property tax (logptax), log of income (loginc), and whether one voted for an increase in property taxes (vote). There are two dependent variables of interest -private and vote- which can be modeled individually by two univariate probit models or jointly by one bivariate probit model, for instance.\nLink to PDF HERE."
  },
  {
    "objectID": "posts/nonparametrics/nonparametrics.html#kernel-density-estimation",
    "href": "posts/nonparametrics/nonparametrics.html#kernel-density-estimation",
    "title": "Nonparametric & Semiparametric Binary-Choice Regressions",
    "section": "Kernel Density Estimation",
    "text": "Kernel Density Estimation\nBefore running a binary choice model to explain the voting choice, we plot different Kernel Density estimates of loginc to check whether it follows approximately a normal distribution. The idea is to test visually if the normal distribution falls within the 95% confidence interval area of each kernel density estimate:\n\n\n\n\n\n\nFigure 1: Kernel Density Estimation of Log(Income)\n\n\n\nAll these Kernel Density estimators in Figure 1 use an alternative Epanechnikov Kernel function (this is the default in the kdens command in STATA). The differences among the estimator are explained by the different methods of choosing the Optimal Bandwidth \\(h^*\\) that minimizes the Mean Integrated Squared Errors (MISE)1. Comparing the kernel densities of the log income against the normal distribution, we observe that the normal distribution seems to not quite fit inside the confidence intervals in none of the kernel estimators. While the case of an optimal bandwidth of \\(h^*=0.188\\) in Figure 1.b. seems to present some evidence about normality, you can still see how the purple line appears outside the boundaries around the 9 to 9.5 value.\nBoth Plug-in methods (Figure 1.a. and 1.c.) use a lower Optimal Bandwidth (0.11 and 0.12, respectively), thus, leading to less bias but more variance in their estimates. In these cases, the normal distribution line appears outside the 95% confidence boundaries in several places. Overall, the visual test seems to reject the hypothesis that log incomes follow a normal distribution."
  },
  {
    "objectID": "posts/nonparametrics/nonparametrics.html#binary-choice-model-probit",
    "href": "posts/nonparametrics/nonparametrics.html#binary-choice-model-probit",
    "title": "Nonparametric & Semiparametric Binary-Choice Regressions",
    "section": "Binary Choice Model (probit)",
    "text": "Binary Choice Model (probit)\nTable 1 reports the results of different Binary choice models to predict voting behavior using years, logptax and loginc as independent variables:\n\n\n\nTable 1: Binary Choice Model Results\n\n\n\n\n\n\nColumn 1 in Table 1 presents the results of the probit model. Here we see a positive and statistically significant relationship between the level of log income and the probability of voting for higher property taxes. Also, once again we observe that property taxes have a negative effect on voting in favor of higher taxes of this kind. Finally, the number of years in the residence did not report a significant result."
  },
  {
    "objectID": "posts/nonparametrics/nonparametrics.html#normality-assumption",
    "href": "posts/nonparametrics/nonparametrics.html#normality-assumption",
    "title": "Nonparametric & Semiparametric Binary-Choice Regressions",
    "section": "Normality Assumption",
    "text": "Normality Assumption\nWe can use a visual test to check the normality assumption of the probit model estimated in Column 1. Recall that in the probit model, the probability that \\(y_i\\) takes on the value 1 is modeled as a nonlinear function of a linear combination of a set of independent variables:\n\\[\n\\Pr(y_i=1|x_i)  = \\Phi(x_i'\\beta)\n\\]\nwhere \\(\\Phi(\\cdot)\\) is assumed to be the standard normal cumulative distribution function (CDF). To check if this assumption is reasonable, we performed nonparametric estimations of \\(\\Pr(y_i=1|x_{i'\\beta})\\). The first estimator is a Nadaraya-Watson Regression (or constant constant estimator) that assumes a constant \\(m(x)=b_0(x_0)\\) around some neighborhood of \\(x_0\\):\n\\[\n\\hat{m}_h(x_0)=\\frac{\\sum^N_{i=1} K_x\\left(\\frac{x_0-x_i}{h} \\right)}{\\sum^N_{i=1} K_x\\left(\\frac{x_0-x_j}{h} \\right)}y_i\n\\]\nThe Nadaraya-Watson estimator is essentially a weighted local average of the observations (weights defined by a Kernel function) at some neighborhood defined by the choice of bandwidth \\(h\\). The second estimator is a Local Linear Regression that, instead of assuming a constant average, lets \\(m(x)\\) be linear in the neighborhood of \\(x_0\\). More concretely, the local linear regression minimizes with respect to \\(b_0(x_0)\\) and \\(b_1(x_0)\\):\n\\[\n\\sum^N_{i=1} K_x\\left(\\frac{x_i-x_0}{h} \\right)[y_i-b_0(x_0)-b_1(x_0)(x_i-x_0)]^2\n\\]\nBoth nonparametric regressions use the Epanechnikov Kernel function. Regarding the choice of bandw\n\n\n\nTable 2: Binary Choice Model Results\n\n\n\n\n\n\nidth \\(h^*\\), I use the command npregress that performs the Leave-One-Out Cross Validation method (LOOCV) to estimate the Optimal Bandwidth size2. Figure 2 compares these regressions against the probit estimation:\n\n\n\n\n\n\nFigure 2: Nonparametric Estimation of Het Probit\n\n\n\nIn general, we see that the normality assumption about the error term \\(\\varepsilon_i\\) in the probit model is actually not rejected, as the probit curve of the \\(\\Pr(y_i=1|x_i'\\beta)\\) tends to be within the 95% confidence intervals of both the Local Constant and Local Linear nonparametric estimators. This means that the standard probit model does not suffer from misspecification."
  },
  {
    "objectID": "posts/nonparametrics/nonparametrics.html#heteroscedastic-probit-model",
    "href": "posts/nonparametrics/nonparametrics.html#heteroscedastic-probit-model",
    "title": "Nonparametric & Semiparametric Binary-Choice Regressions",
    "section": "Heteroscedastic Probit Model",
    "text": "Heteroscedastic Probit Model\nColumn 2 of Table 2 presents the results of the heteroscedastic probit model. This specification still assumes that the error term of the latent model follows a normal distribution, but is more flexible in the sense that it no longer assumes its variance to be 1, but that it can vary as a function of some set of explanatory variables. In this case, we only use the variable years to model the variance:\n\\[\nV(\\varepsilon_i|x_i)=\\sigma^2_i=[\\exp(\\text{years}_{i}'\\gamma)]^2\n\\]\nwhere \\(\\varepsilon_i\\sim N[0,\\exp(\\text{years}_{i}'\\gamma)]\\). The probability of success is now represented by:\n\\[\n\\Pr(y_i=1|x_i)  = \\Phi\\left( \\frac{x_i'\\beta}{\\sigma_i} \\right) = \\Phi\\left( \\frac{x_i'\\beta}{\\exp(\\text{years}_{i}'\\gamma)} \\right)\n\\]\nHere we are primarily interested in testing whether \\(\\gamma\\neq 0\\), which seems to be the case as we see in Table 2 a statistically significant coefficient. Additionally, the likelihood-ratio test of heteroskedasticity, which tests the full model with heteroskedasticity against the full model without, is significant as \\(\\chi^2(1) = 11.55\\). Regarding the coefficients, we now see that the number of years in the residence impacts negatively the probability of voting for higher property taxes. The remaining variables are still significant and with the same sign relative to the standard probit model, but the coefficients are more pronounced."
  },
  {
    "objectID": "posts/nonparametrics/nonparametrics.html#nonparametric-regression",
    "href": "posts/nonparametrics/nonparametrics.html#nonparametric-regression",
    "title": "Nonparametric & Semiparametric Binary-Choice Regressions",
    "section": "Nonparametric Regression",
    "text": "Nonparametric Regression\nWe now test again the normality assumption of the heteroscedastic probit model by visually comparing its CDF with the two nonparametric alternatives already discussed:\n\n\n\n\n\n\nFigure 3: Nonparametric Estimation of Heteroscedastic Probit\n\n\n\nClearly, the heteroscedastic alternative -that assumes normality- highly deviates from both nonparametric estimations. Hence, it seems that the normality assumption in the hetprobit model is rejected. Furthermore, the probit link function is no longer smooth when compared to the standard probit. Modeling the variance as dependent on the explanatory variable years makes the prediction less linear, leading to a more noisy curve than smoothed."
  },
  {
    "objectID": "posts/nonparametrics/nonparametrics.html#klein-spady-semiparametric-regression",
    "href": "posts/nonparametrics/nonparametrics.html#klein-spady-semiparametric-regression",
    "title": "Nonparametric & Semiparametric Binary-Choice Regressions",
    "section": "Klein & Spady Semiparametric Regression",
    "text": "Klein & Spady Semiparametric Regression\nSemiparametric alternatives do not rely on the parametric assumptions about the shape of the error term distribution \\(\\varepsilon_i\\). The Klein & Spady method is a Single Index model with a \\(\\Pr(y_i|x_i)= F(x_i'\\beta)\\) structure that only depends on \\(x_i\\) through a single linear combination, \\(x_i'\\beta\\), whereas the function \\(F(\\cdot)\\) is left unspecified. The estimation is based on some kind of Maximum Likelihood Estimation:\n\\[\nL(\\beta,h)=\\sum^N_{i=1}(1-y_i)\\ln[1-\\hat{F}_{-i,n}(x_i'\\beta)]+\\sum^N_{i=1}y_i\\ln[\\hat{F}_{-i,n}(x_i'\\beta)]\n\\]\nSince \\(F(\\cdot)\\) is unknown, Klein & Spady suggest replacing it with a Leave-One-Out Nadaraya-Watson estimator \\(\\hat{F}_{-i,n}(\\cdot)\\). In order to guarantee point identification, the coefficient of one continuous variable is normalized to 1, which in our case will be the variable loginc. Additionally, in single index models, the function \\(F(\\cdot)\\) will include any location and level shift, so the vector \\(x_i\\) cannot include an intercept. Column 3 of Table 2 shows the results of this semiparametric regression. But because we want to make some comparisons between the different models so far presented, we will also need to scale normalize the previous models, as in Table 2 they are not directly comparable. Table 3 shows the results of each model after scale normalization:\n\n\n\nTable 3: Binary Choice Models: Normalized Results\n\n\n\n\n\n\nHere we see that the direction of the signs in every specification is the same: years and logptax are negatively associated with the probability of voting in favor of the tax reform. The coefficients of the property tax variable are to some degree similar across models, but there are important differences regarding the coefficient linked to the number of years in residence. First, the probit model did not show an effect statistically distinct from zero, whereas the other two models did show significant and more pronounced coefficients.\nBetween the heteroscedastic probit and the Klein & Spady regression, the hetprobit seems to report a more negative effect of years on the probability of voting for higher taxes. In order to understand the magnitude of these differences between models, we will need to estimate marginal effects."
  },
  {
    "objectID": "posts/nonparametrics/nonparametrics.html#semiparametric-regression-visualization",
    "href": "posts/nonparametrics/nonparametrics.html#semiparametric-regression-visualization",
    "title": "Nonparametric & Semiparametric Binary-Choice Regressions",
    "section": "Semiparametric Regression Visualization",
    "text": "Semiparametric Regression Visualization\nFigure 4 visualize the Klein & Spady semiparametric estimate of \\(\\Pr(y_i=1|x_i'\\beta)\\) and compares it against the heteroskedastic estimation:\n\n\n\n\n\n\nFigure 4: Semiparametric Estimation of Probits\n\n\n\nThe semiparametric regression in Figure 4 this case is quite similar to the ``U’’ shape of the nonparametric (Nadara-Watson) regression from Figure 3.a. that is based on the predicted values \\(x_i'\\beta\\) of the heteroscedastic probit to estimate the probabilities. However, we also see that the (parametric) heteroscedastic probit falls outside the 95% confidence interval area at various points ( Figure 4 in green). Hence, while we cannot use the hetprobit model to draw conclusions on the probability of voting for higher property taxes, we do can use a nonparametric version of the predicted values of the heteroscedastic model for that purposes.\nMost of the behavior behind this strange shape of the link function -behavior that is mostly happening at \\(x_i'\\beta&lt;-5\\) in Figure 4 -, can be explained by the distribution of the years variable. From Figure 5 we can observe that this group consists of 5 observations that have an average number of years of 39, versus the 6.8 from the rest of the sample:\n\n\n\n\n\n\nFigure 5: Kernel Density Estimation of Years\n\n\n\nAs we have seen from the models, the number of years has in general a negative effect on the probability of voting for more taxes, but here we have that all these 5 observations have a high number of years in the residence and all of them have vote=1. Hence, this group of ``outliers’’ might be generating a countervailing effect on the true point estimates of years."
  },
  {
    "objectID": "posts/nonparametrics/nonparametrics.html#restricted-model",
    "href": "posts/nonparametrics/nonparametrics.html#restricted-model",
    "title": "Nonparametric & Semiparametric Binary-Choice Regressions",
    "section": "Restricted Model",
    "text": "Restricted Model\nWe now re-run the binary choice models so far discussed, but restricting the estimation for observations where the number of years in residence is smaller than 25. Table 4 shows the main results after normalization to make them comparable:\n\n\n\nTable 4: Binary Choice Models: Normalized Results (years &lt; 25)\n\n\n\n\n\n\nHere we see surprisingly similar conclusions on the point estimates across different specifications. In the standard probit model, we now see that years have statistically significant results, with a more intense negative effect on our dependent variable than before. The heteroscedastic probit now reports a more negative coefficient on the variable logptax, significant only at the 10% level, but for the number of years, we see a less pronounced effect and is no longer significant. Looking at \\(\\gamma\\), note that now we cannot reject the null hypothesis of homoscedastic errors so that, after filtering for the extreme values of years, there is no evidence of this model being better than the standard probit.\nFinally, the semiparametric model seems to be the model that changed the least. Both the direction of the signs and their significance remain as before, with years having a slightly less pronounced coefficient, and logptax reporting now a more negative effect on the probability of voting for higher taxes than before.\nWe want now to test the normality assumption, so we re-estimate the nonparametric regressions from previous exercises to make the visual comparison. In this case, I only perform the Nadaraya-Watson Estimator:\n\n\n\n\n\n\nFigure 6: Non and Semiparametric Estimation (years&lt;25)\n\n\n\nAgain, we see that all both the probit and hetprobit models falls within the boundaries of the confidence interval of the nonparametric estimates. Taking all this evidence together, we can conclude that, after filtering the extreme values of years, the three binary choice models are no longer much different from each other. Finally, because now there is no evidence of heteroscedasticity and we did not reject the normality assumption about the error terms \\(\\varepsilon_i\\), the standard probit model is the preferred model in my view."
  },
  {
    "objectID": "posts/nonparametrics/nonparametrics.html#nonparametric-and-semiparametric-regression-comparison",
    "href": "posts/nonparametrics/nonparametrics.html#nonparametric-and-semiparametric-regression-comparison",
    "title": "Nonparametric & Semiparametric Binary-Choice Regressions",
    "section": "Nonparametric and Semiparametric Regression Comparison",
    "text": "Nonparametric and Semiparametric Regression Comparison\nUsing the standard probit estimates, we want now to graphically compare the estimates between the Nadaraya-Watson (Local Constant), the Local Linear, and the Local Quadratic regression. The following figure summarizes the results:\n\n\n\n\n\n\nFigure 7: Nonparametric and Semiparametric Regression - Comparison\n\n\n\nAlthough the curves seem to be very similar, we can identify some differences at specific points. Generally speaking, the degree of bias of the Nadaraya-Watson highly depends on the curvature or second derivative \\(m''(x)\\) of the regression function, as well on the slope of the density of the regressors \\(f'(x)\\):\n\\[\n\\text{bias}[\\hat{m}(x)]=\\frac{h^2\\mu_2}{2}\\left\\{m''(x)+2\\frac{f'_x(x)m'(x)}{f_x(x)} \\right\\}+O(\\frac{1}{nh})+o(h^2)\n\\]\nNamely, the bias will be very large at a point where the density function curves a lot. In Figure 7 (and assuming the normality distribution is the true function), we can actually observe that the Local Constant estimator has a large and positive bias (i) between \\(-1&lt;x_i'\\beta&lt;0.5\\), which is the place where the density is curving positively and very quickly (positive curvature) and (ii) between \\(0.5&lt;x_i'\\beta&lt;1.55\\), marking the space where the density function is decreasing quickly (negative curvature). The local linear and local quadratic regressions also present this described behavior, but with less biasedness. The local linear is the more smoothed function among the three.\nOne reason that explains this difference may be due to the fact that the bias of the Local Linear (and Quadratic) regression does not depend on the density function of the regressors, also called design bias:\n\\[\\begin{align*}\n    \\text{bias}[\\hat{m}(x)]=\\frac{h^2\\mu_2}{2}m''(x)+o(h^2)\n\\end{align*}\\]\nIn that sense, the design bias adds an extra bias to the Nadaraya-Watson estimator. Finally, we observe more differences between the estimators on the extremes. On the bottom left (between -1.2 and -2), for example, the Nadaraya-Watson starts with higher bias, but then it shifts quickly to be equal to 0 towards the left, whereas the other two (which are very similar to each other) start with less bias, but then they end up with negative probabilities towards the left. On the other extreme we find the same behavior, where the local and quadratic estimators are not bounded to be within 0 and 1."
  },
  {
    "objectID": "posts/nonparametrics/nonparametrics.html#footnotes",
    "href": "posts/nonparametrics/nonparametrics.html#footnotes",
    "title": "Nonparametric & Semiparametric Binary-Choice Regressions",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAs the optimal bandwidth formula also depends on the curvature of the density \\(f''\\), which is unknown, there are different ways to deal with this issue. Some impose assumptions about the unknown distribution (Plug-in methods), and others are more data-driven such as cross-validation techniques.↩︎\nIn contrast, the command lpoly uses the Rule-of-Thumb method of bandwidth selection, which is a Plug-In estimator.↩︎"
  },
  {
    "objectID": "posts/gasoline_merger_did/merger-report.html",
    "href": "posts/gasoline_merger_did/merger-report.html",
    "title": "Retail Gasoline Merger in Chile: An Ex-Post Merger Evaluation",
    "section": "",
    "text": "Ex-post assessments of merger effects in Chilean’s retail gasoline market."
  },
  {
    "objectID": "posts/gasoline_merger_did/merger-report.html#identification-strategy",
    "href": "posts/gasoline_merger_did/merger-report.html#identification-strategy",
    "title": "Retail Gasoline Merger in Chile: An Ex-Post Merger Evaluation",
    "section": "4.1. Identification Strategy",
    "text": "4.1. Identification Strategy\nA simple before-after comparison will lead to biased estimates of the merger effects, given that observed price changes might stem from changes in demand or costs. Therefore, we aim to compare price changes around the merger to a counterfactual scenario in which no merger took place. Based on ex-post merger evaluation literature (Dafny et al. [2012]; Ashenfelter et al. [2015]; Argentesi et al. [2021]), I compare the geographical market in which both the acquirer (COPEC) and the target (CGL) operated in the pre-merger period to control markets that did not experience a change in market concentration.\nMy identification strategy relies on the expectation that competitive effects of the merger are likely to be stronger in the so-called overlap area between the merging parties than in non-overlap areas where the parties did not compete with each other door to door, since only in overlap areas did the intensity of competition change because of the merger. We thus can compare areas that experienced a change in market concentration (treated group) to markets without a pre-merger overlap (control group). Finally, this causal effect can be identified by employing a Difference-in-Differences (DiD) methodology that compares overlap and non-overlap areas to get the Average Treatment Effects on the Treated at the local level.\nAnother assumption that is implicitly here is the assumption that in retail gasoline markets competition works at the local level. Any comparison between treated and control areas will be able to identify merger treatment effects only if competition is, at least to some extent, local. Although in this article we will not provide evidence to support this, previous literature tend to suggest that retail gasoline market is local rather than national (see here and here).\n\n4.1.1. Geographical Relevant Market\nThe impact of COPEC’s acquisition on the market structure around each gas station is measured as the change in the number of independent competing brands within a specific radius. However, determining the appropriate radius size is a critical consideration, not to be underestimated.\nThe Chilean competition authority (FNE), in its merger report, defined the relevant geographical market as the entire Castro County/Commune. According to their assessment, this determination was based on jurisprudence, which considers the local or communal market due to the difficulties and costs consumers face in traveling between gas stations in terms of time, convenience, and fuel expenses. Additionally, international jurisprudence has also noted that retail operators typically operate in local markets, monitoring competitor prices within a relatively limited radius, typically around 3 miles (approximately 5 kilometers), or based on travel isochrones of 10 minutes for urban areas and 20 minutes for rural areas (Case No. ME/3933/08). Consequently, I will adopt a 5 km radius to delineate the geographical relevant markets.\n\n\nCode\nimport geopandas as gpd\nfrom shapely.geometry import Point\n\n# Filter the DataFrame for the unit with Id='co1020101'\nunit_co1020101 = df[df['Id'] == 'co1020101']\n\n# Get the unique latitude and longitude values\nunique_latitudes = unit_co1020101['Latitud'].unique()\nunique_longitudes = unit_co1020101['Longitud'].unique()\n\n# Target coordinates (longitude, latitude)\ntarget_crs = (unique_longitudes, unique_latitudes)\n\n# Convert DataFrame to GeoDataFrame\ngrouped_df = df.groupby('Id').agg({'Latitud': 'first', 'Longitud': 'first'}).reset_index()\nnew_df = pd.DataFrame(grouped_df)\ngdf_units = gpd.GeoDataFrame(new_df, geometry=gpd.points_from_xy(new_df.Longitud, new_df.Latitud), crs=\"EPSG:4326\")\n\n# Create a Shapely Point object for the target coordinates\ntarget_point = gpd.GeoDataFrame({'geometry': [Point(target_crs)]}, crs=\"EPSG:4326\")\n# Buffer the point to create a circle with a radius of 5000 meters\nbuffer_radius_deg = 5000 / 111000  # 1 degree of latitude is approximately 111000 meters\n\nbuffer_circle = target_point.buffer(buffer_radius_deg)\nbuffer_area = buffer_circle.to_crs(gdf_units.crs)\n\n# Check if each unit's coordinates fall within the buffer circle\ngdf_units['treat'] = gdf_units.geometry.within(buffer_area.iloc[0])\n\n# Convert treat column to integer (1 for True, 0 for False)\ngdf_units['treat'] = gdf_units['treat'].astype(int)\ngdf_units['Treated'] = np.where(gdf_units['treat'] == 1, 'Treated', 'Control')\ndf = pd.merge(df, gdf_units[['Id', 'Treated', 'treat']], on='Id', how='left')\n\n# Count treated and control units\ntreated_count = gdf_units['treat'].value_counts()[1]\ncontrol_count = gdf_units['treat'].value_counts()[0]\n\nprint(\"Treated units:\", treated_count)\nprint(\"Control units:\", control_count)\n\n\nTreated units: 6\nControl units: 150\n\n\nThis selection process resulted in a total of 6 service stations falling within the defined overlap area, thus constituting the treated group. The remaining 150 stations outside this area will serve as the control group. Figure 2 provides a visual and interactive representation of the spatial distribution of these stations:\n\n\nCode\nimport folium\nfrom folium.plugins import MarkerCluster\n\n# Filter GeoDataFrame by Region=='Castro'\ndf_lagos = df[df['Region'] == 'Los Lagos']\ndf_lagos = df_lagos.dropna(subset=['Latitud', 'Longitud'])\nprice_id = df_lagos.groupby('Id').agg({'Precio': 'mean', 'Latitud': 'first', 'Longitud': 'first'}).reset_index()\ngdf = gpd.GeoDataFrame(price_id, geometry=gpd.points_from_xy(price_id.Longitud, price_id.Latitud), crs=\"EPSG:4326\")\n\n# Create a map centered around Los Lagos region\ntarget_crs = [-42.470071, -73.76461]\nm = folium.Map(location=target_crs, zoom_start=11)\n\n# Create a MarkerCluster to add all the points\nmarker_cluster = MarkerCluster().add_to(m)\n\n# # Iterate over each row in the GeoDataFrame and add a marker to the map\nfor idx, row in gdf.iterrows():\n    folium.Marker(location=[row['Latitud'], row['Longitud']], popup=row['Id'], \n                  ).add_to(marker_cluster)\n\nradius = 5000\nfolium.Circle(\n    location=target_crs,\n    radius=radius,\n    color=\"black\",\n    weight=1,\n    fill_opacity=0.1,\n    opacity=1,\n    fill_color=\"blue\",\n    fill=False,  # gets overridden by fill_color\n    popup=\"{} meters\".format(radius),\n    tooltip=\"I am in meters\",\n).add_to(m)\n\n# Show\nm\n\n\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\nFigure 2: Map with Treated Service Stations\n\n\n\n\n\n\n4.1.2. Treatment Period\nRegarding the treatment period, it’s important to acknowledge that certain treated service stations ceased price postings during the interval between the merger approval (December 2016) and the beginning of operations under the re-branded identity (around January 2018). Consequently, observations within this transitional period are excluded from analysis, aligning the last pre-merger period with November 2016 and the first post-merger period with February 2018.\n\n\nCode\nimport pandas as pd\n\n# Convert 'date' column to datetime\ndf['date'] = pd.to_datetime(df['date'])\ndf['year_month'] = df['date'].dt.to_period('M')\n\n# Define the date thresholds\npost_date_1 = pd.to_datetime(\"2016-12-02\")\npost_date_2 = pd.to_datetime(\"2017-11-01\")\n\n# Create 'Post' variable\ndf['Post'] = (df['date'] &gt;= post_date_1).astype(int)\n\n# Create 'post1' variable\ndf['post1'] = ((df['date'] &gt;= post_date_1) & (df['date'] &lt; post_date_2)).astype(int)\n\n# Create 'post2' variable\ndf['post2'] = (df['date'] &gt;= post_date_2).astype(int)\n\n\n# sh1020101 comes back at 2017-11-30 or 11\n# pe1020101 comes back at 2018-02-22 or 14\n\n# Filter the DataFrame for Balanced Panel\ndf = df[ (df['date'] &gt;= pd.to_datetime(\"2018-02-22\")) | (df['date'] &lt;= pd.to_datetime(\"2016-12-02\"))  ]\n\n# Get unique dates\nunique_dates = sorted(set(df['date']))\n\n# Create DataFrame from unique dates\nunique_dates_df = pd.DataFrame({'date': unique_dates})\n\n# Add a new column containing numbers from 1 to the number of rows\nunique_dates_df['event_date'] = range(1 - 25 , len(unique_dates_df) + 1 - 25)\n\n# Merge with original DataFrame based on 'date' column\ndf = pd.merge(df, unique_dates_df, on='date', how='left')\n\n\n\n\nCode\n# Count occurrences of each Id\nid_counts = df['Id'].value_counts()\n\n# Check if it's a balanced panel\nbalanced_panel = id_counts.nunique() == 1\n\n# If not balanced, identify Id with different counts\nif not balanced_panel:\n    unbalanced_ids = id_counts[id_counts != id_counts.iloc[0]].index\n    \n    # Remove rows corresponding to unbalanced Id\n    df = df[~df['Id'].isin(unbalanced_ids)]\n\n# Print the balanced panel status and the number of rows after removing unbalanced Ids\nprint(\"Is it a balanced panel?\", balanced_panel)\nprint(\"Number of rows after removing unbalanced Ids:\", len(df))\n\n# Check if 'sh1020101' and 'pe1020101' are in the filtered DataFrame\ntreated_ids = ['sh1020101', 'pe1020101']\nremoved_ids = [id for id in treated_ids if id not in df['Id'].unique()]\n\n# Print the result\nif removed_ids:\n    print(\"The following treated Ids were removed:\", removed_ids)\nelse:\n    print(\"All treated Ids are still present in the DataFrame.\")\n\n\nIs it a balanced panel? False\nNumber of rows after removing unbalanced Ids: 18423\nAll treated Ids are still present in the DataFrame.\n\n\nFurthermore, to ensure a balanced panel dataset, additional stations with irregular posting behavior during the relevant study period are excluded. Following these adjustments, the dataset comprises 18,423 observations."
  },
  {
    "objectID": "posts/gasoline_merger_did/merger-report.html#summary-statistics",
    "href": "posts/gasoline_merger_did/merger-report.html#summary-statistics",
    "title": "Retail Gasoline Merger in Chile: An Ex-Post Merger Evaluation",
    "section": "4.2. Summary Statistics",
    "text": "4.2. Summary Statistics\nTable 2 and Table 3 offer summary statistics on both the treated and non-treated units within the dataset.\n\n\nCode\n# Create a new column 'Brand' based on conditions\ndf['Brand'] = np.where(df['Nombre Distribuidor'] == 'COPEC', 'COPEC', 'Branded Rivals')\ndf.loc[df['Id'] == 'sh1020101', 'Brand'] = 'Non Branded'\n\n# Pivot the dataframe with 'treat' as columns\nsummary_stats = df.groupby(['treat', 'Brand'])['Id'].nunique().reset_index(name='Id')\n\npivot_summary_stats = summary_stats.pivot_table(index='Brand', columns='treat', values='Id', fill_value=0)\n\n# Print the pivot table\npivot_summary_stats\n\n\n\n\nTable 2: Pre-Merger Sample of Service Stations\n\n\n\n\n\n\n\n\n\ntreat\n0\n1\n\n\nBrand\n\n\n\n\n\n\nBranded Rivals\n46\n3\n\n\nCOPEC\n37\n2\n\n\nNon Branded\n0\n1\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Pivot the dataframe to have 'Combustible' values as columns\navg_summary = df.groupby(['treat', 'Post', 'Combustible'])['Precio'].mean().reset_index()\npivot_avg_summary = avg_summary.pivot_table(index=['Combustible', 'treat'], columns='Post', values='Precio')\npivot_avg_summary.round(1)\n\n\n\n\nTable 3: Preliminary Summary Statistics\n\n\n\n\n\n\n\n\n\n\nPost\n0\n1\n\n\nCombustible\ntreat\n\n\n\n\n\n\nGasolina 93\n0\n727.1\n844.8\n\n\n1\n735.4\n858.8\n\n\nGasolina 97\n0\n805.8\n902.9\n\n\n1\n815.4\n918.2\n\n\nPetroleo Diesel\n0\n501.1\n622.8\n\n\n1\n515.3\n639.4\n\n\n\n\n\n\n\n\n\n\nFigure 3 illustrates the comparative trajectory of average gasoline prices per type in the treated area against those in non-treated areas.\n\n\nCode\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Plot the average price over time for each treatment group\nfig, axes = plt.subplots(3, 2, figsize=(15, 12))\n\n# Filter DataFrame for Combustible=='Petroleo Diesel'\nfiltered_df = df[df['Combustible'] == 'Gasolina 93']\n\n# Group by 'Treated' and 'date' and calculate the mean price\navg_price = filtered_df.groupby(['Treated', 'event_date'])['Precio'].mean().reset_index()\n\n# First subplot\nfor Treated, data in avg_price.groupby('Treated'):\n    axes[0,0].plot(data['event_date'], data['Precio'], label=Treated)\n\n# Add vertical lines\naxes[0,0].axvline(-0.5, linestyle='--', color='grey')\n\n# Set labels and title\naxes[0,0].set_xlabel('Fecha')\naxes[0,0].set_ylabel('Precio')\naxes[0,0].set_title('Average Price of Gasolina 93 over Time')\naxes[0,0].legend(title='Treatment', loc='best')\n\n# Second subplot\n# Pivot the dataframe to have treated and control groups as columns\npivot_avg_price = avg_price.pivot(index='event_date', columns='Treated', values='Precio')\n\n# Calculate the difference between treated and control\ndifference = pivot_avg_price['Treated'] - pivot_avg_price['Control']\n\n# Plot the difference over time\naxes[0,1].plot(difference.index, difference.values, color='blue')\n\n# Add vertical lines\naxes[0,1].axvline(-0.5, linestyle='--', color='grey')\n\n# Set labels and title\naxes[0,1].set_xlabel('Fecha')\naxes[0,1].set_ylabel('Price Difference (Treated - Control)')\naxes[0,1].set_title('Difference in Average Price of Gasolina 93 between Treated and Control')\n\n\n\n# Filter DataFrame for Combustible=='Gasolina 97'\nfiltered_df = df[df['Combustible'] == 'Gasolina 97']\n\n# Group by 'Treated' and 'date' and calculate the mean price\navg_price = filtered_df.groupby(['Treated', 'event_date'])['Precio'].mean().reset_index()\n\n# First subplot\nfor Treated, data in avg_price.groupby('Treated'):\n    axes[1,0].plot(data['event_date'], data['Precio'], label=Treated)\n\n# Add vertical lines\naxes[1,0].axvline(-0.5, linestyle='--', color='grey')\n\n# Set labels and title\naxes[1,0].set_xlabel('Fecha')\naxes[1,0].set_ylabel('Precio')\naxes[1,0].set_title('Average Price of Gasolina 97 over Time')\naxes[1,0].legend(title='Treatment', loc='best')\n\n# Second subplot\n# Pivot the dataframe to have treated and control groups as columns\npivot_avg_price = avg_price.pivot(index='event_date', columns='Treated', values='Precio')\n\n# Calculate the difference between treated and control\ndifference = pivot_avg_price['Treated'] - pivot_avg_price['Control']\n\n# Plot the difference over time\naxes[1,1].plot(difference.index, difference.values, color='blue')\n\n# Add vertical lines\naxes[1,1].axvline(-0.5, linestyle='--', color='grey')\n\n# Set labels and title\naxes[1,1].set_xlabel('Fecha')\naxes[1,1].set_ylabel('Price Difference (Treated - Control)')\naxes[1,1].set_title('Difference in Average Price of Gasolina 97 between Treated and Control')\n\n\n\n# Filter DataFrame for Combustible=='Petroleo Diesel'\nfiltered_df = df[df['Combustible'] == 'Petroleo Diesel']\n\n# Group by 'Treated' and 'date' and calculate the mean price\navg_price = filtered_df.groupby(['Treated', 'event_date'])['Precio'].mean().reset_index()\n\n# First subplot\nfor Treated, data in avg_price.groupby('Treated'):\n    axes[2,0].plot(data['event_date'], data['Precio'], label=Treated)\n\n# Add vertical lines\naxes[2,0].axvline(-0.5, linestyle='--', color='grey')\n\n# Set labels and title\naxes[2,0].set_xlabel('Fecha')\naxes[2,0].set_ylabel('Precio')\naxes[2,0].set_title('Average Price of Diesel over Time')\naxes[2,0].legend(title='Treatment', loc='best')\n\n# Second subplot\n# Pivot the dataframe to have treated and control groups as columns\npivot_avg_price = avg_price.pivot(index='event_date', columns='Treated', values='Precio')\n\n# Calculate the difference between treated and control\ndifference = pivot_avg_price['Treated'] - pivot_avg_price['Control']\n\n# Plot the difference over time\naxes[2,1].plot(difference.index, difference.values, color='blue')\n\n# Add vertical lines\naxes[2,1].axvline(-0.5, linestyle='--', color='grey')\n\n# Set labels and title\naxes[2,1].set_xlabel('Fecha')\naxes[2,1].set_ylabel('Price Difference (Treated - Control)')\naxes[2,1].set_title('Difference in Average Price of Diesel between Treated and Control')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nFigure 3: Price Evolution in Combustibles, by Treatment\n\n\n\n\n\nNotably, the market’s high price transparency is reflected in closely aligned pre-merger prices between treated and non-treated stations. However, this coherence is less pronounced in the case of Diesel, where price trends in time appear to diverge.\nNevertheless, the graphical assessment suggests that the crucial assumption of the Difference-in-Differences (DiD) methodology —the parallel trend assumption— appears to be upheld within our sample, particularly evident for Gasoline 93 and 97. Our event-study framework aims to further elucidate and quantify these patterns."
  },
  {
    "objectID": "posts/gasoline_merger_did/merger-report.html#empirical-model",
    "href": "posts/gasoline_merger_did/merger-report.html#empirical-model",
    "title": "Retail Gasoline Merger in Chile: An Ex-Post Merger Evaluation",
    "section": "4.3. Empirical Model",
    "text": "4.3. Empirical Model\nI examined the impact of the merger on retail gasoline prices within affected local markets using a difference-in-differences (DiD) framework that compares the price changes in a selection of stations that were located in overlap areas with the change in the same outcome variable in other stations from the non-overlap areas before and after the merger. My baseline specification takes the following form:\n\nP_{it} = \\beta \\times Treat_i\\times \\textbf{1}[t-t^*_0\\geq0] + \\gamma_t + \\lambda_i + \\varepsilon_{it}\n\nwhere the outcome variable, P_{it}, denotes the price of service station i in year-month t. The treatment variable, Treat_i, takes the value of 1 if station i is located within the 5 kilometers of radius distance towards the target firm. The post-period indicator variable, \\textbf{1}[t-t^*_0\\geq0], equals 1 if period t belongs to the post-merger period (i.e., after January 2018) whose starting period is $t^*_0 $ , and 0 otherwise. Year-month and unit fixed effects are captured by \\gamma_t and \\lambda_i, respectively. The coefficient of interest is \\beta, which measures the average treatment effect of the merger. It identifies the additional variation in prices experienced by the stations in overlap areas compared to the control stations after the merger took place.\nUnder certain assumptions, parameter \\beta capture the causal effect of the treatment on the outcome. In our context, these effects are primarily identified by comparing units in the treated areas and units in no-treated areas, before and after the merger phase. The crucial identifying assumption is the so-called parallel trends assumption of a DiD model: that absent the merger, the treated service stations would have experienced the same outcome trend as the control stations.\nIn addition, I also report estimates from a more flexible specification that allows the coefficient to vary by the relative periods after the merger, by estimating the following equation:\n\nP_{it} = \\sum_{h=T_1}^{T_0}(\\beta_h \\times Treat_i\\times \\textbf{1}[t-t^*_s\\geq0]) + \\gamma_t + \\lambda_i + \\varepsilon_{it}\n\nwhere T_0 and T_1 are the lowest and highest number of lags and leads, respectively, to consider surrounding the treatment period. Because I normalize the coefficient on the periods just prior to the merger announcement to zero (i.e., \\beta_{-1}=0), each coefficient of \\beta_h can be interpreted as the price change in treated stations relative to no-treated stations after h periods of the merger, with all of the \\beta_h’s being estimated relative to the omitted year (i.e., h=-1).\nUsing this flexible model has two advantages. First, I can visually test the key identifying assumption, which is the parallel-trends assumption: abstent the acquisition, the prices between overlap and non-overlap areas would have evolved in parallel. Although this assumption is fundamentally untestable, plotting the \\beta_h’s of pre-periods (i.e., \\beta_{-8} to \\beta_{-2}) can provide visual evidence. Second, this event-study allows the assessment of the time-evolving effects of the merger. This flexible specification enables me to capture such time-varying effects."
  },
  {
    "objectID": "posts/gasoline_merger_did/merger-report.html#two-way-fixed-effects-results",
    "href": "posts/gasoline_merger_did/merger-report.html#two-way-fixed-effects-results",
    "title": "Retail Gasoline Merger in Chile: An Ex-Post Merger Evaluation",
    "section": "5.1. Two-Way Fixed Effects Results",
    "text": "5.1. Two-Way Fixed Effects Results\nTable 4, Table 5 and Table 6 report the baseline results from estimating the specification described in Equation 1 on Gasoline 93, 97 and Diesel prices, respectively:\n\n\nCode\nimport pyfixest as pf\n\n# Filter by Gasolina 93\ndf93 = df[(df['Combustible'] == 'Gasolina 93')]\n\n# TWFE Estimation\nfeols1 = pf.feols(\"Precio ~ treat*Post | Id + date\", data=df93, vcov={\"CRV1\": \"Id\"})\nfeols1.tidy().reset_index()\n\n\n\n\nTable 4: TWFE Results (Gasoline 93)\n\n\n\n\n            \n            \n            \n\n\n\n\n\n\n\n\n\nCoefficient\nEstimate\nStd. Error\nt value\nPr(&gt;|t|)\n2.5%\n97.5%\n\n\n\n\n0\ntreat:Post\n5.747088\n0.595788\n9.646201\n1.998401e-15\n4.563085\n6.931091\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nimport pyfixest as pf\n\n# Filter by Gasolina 97\ndf97 = df[(df['Combustible'] == 'Gasolina 97')]\n\n# TWFE Estimation\nfeols1 = pf.feols(\"Precio ~ treat*Post | Id + date\", data=df97, vcov={\"CRV1\": \"Id\"})\nfeols1.tidy().reset_index()\n\n\n\n\nTable 5: TWFE Results (Gasoline 97)\n\n\n\n\n\n\n\n\n\n\nCoefficient\nEstimate\nStd. Error\nt value\nPr(&gt;|t|)\n2.5%\n97.5%\n\n\n\n\n0\ntreat:Post\n5.689687\n0.62263\n9.138146\n2.131628e-14\n4.45234\n6.927033\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nimport pyfixest as pf\n\n# Filter by Diesel\ndfdiesel = df[(df['Combustible'] == 'Petroleo Diesel')]\n\n# TWFE Estimation\nfeols1 = pf.feols(\"Precio ~ treat*Post | Id + date\", data=dfdiesel, vcov={\"CRV1\": \"Id\"})\nfeols1.tidy().reset_index()\n\n\n\n\nTable 6: TWFE Results (Diesel)\n\n\n\n\n\n\n\n\n\n\nCoefficient\nEstimate\nStd. Error\nt value\nPr(&gt;|t|)\n2.5%\n97.5%\n\n\n\n\n0\ntreat:Post\n2.496932\n0.971307\n2.570692\n0.01183\n0.566663\n4.427201\n\n\n\n\n\n\n\n\n\n\nThe estimated average effect on all these cases suggests that the merger caused a statistically significant average increase in price in overlap areas by around $5-6 Chilean pesos for the Gasoline 93 and 97 and around $2-3 for Diesel. These price effects are all below 1%."
  },
  {
    "objectID": "posts/gasoline_merger_did/merger-report.html#event-study-results",
    "href": "posts/gasoline_merger_did/merger-report.html#event-study-results",
    "title": "Retail Gasoline Merger in Chile: An Ex-Post Merger Evaluation",
    "section": "5.2. Event-Study Results",
    "text": "5.2. Event-Study Results\nI now estimate the event-study specification outlined in Equation 2 with the period before the merger as the reference period. Figure 4, Figure 5 and Figure 6 shows the the estimated coefficients (and the corresponding 95% confidence intervals) after controlling for observed characteristics of the TWFE model.\n\n\n\n            \n            \n            \n\n\n\n\nCode\ndf93 = df[(df['Combustible'] == 'Gasolina 93')]\n\nfeols_event_study = pf.feols(\n    \"Precio ~ i(event_date, treat, ref=-1) | Id + event_date\",\n    data=df93,\n    vcov={\"CRV1\": \"Id\"}\n)\n\n\nfeols_event_study.iplot(\n    coord_flip=False,\n    #title=\"TWFE-Estimator\",\n    figsize=[900, 400],\n    xintercept=22.5,\n    yintercept=0\n) + theme(legend_position='none') + scale_color_viridis(option=\"magma\", end=0.5)\n\n\n\n\n   \n   \n\n\nFigure 4: Event Study Results - Gasoline 93\n\n\n\n\n\n\nCode\ndf97 = df[(df['Combustible'] == 'Gasolina 97')]\n\nfeols_event_study = pf.feols(\n    \"Precio ~ i(event_date, treat, ref=-1) | Id + event_date\",\n    data=df97,\n    vcov={\"CRV1\": \"Id\"}\n)\n\nfeols_event_study.iplot(\n    coord_flip=False,\n    title=\"TWFE-Estimator\",\n    figsize=[900, 400],\n    xintercept=22.5,\n    yintercept=0,\n) + theme(legend_position='none') + scale_color_viridis(option=\"magma\")\n\n\n\n\n   \n   \n\n\nFigure 5: Event Study Results - Gasoline 97\n\n\n\n\n\n\nCode\ndfdiesel = df[(df['Combustible'] == 'Petroleo Diesel')]\n\nfeols_event_study = pf.feols(\n    \"Precio ~ i(event_date, treat, ref=-1) | Id + event_date\",\n    data=dfdiesel,\n    vcov={\"CRV1\": \"Id\"}\n)\n\nfeols_event_study.iplot(\n    coord_flip=False,\n    #title=\"TWFE-Estimator\",\n    figsize=[900, 400],\n    xintercept=22.5,\n    yintercept=0,\n) + theme(legend_position='none') + scale_color_viridis(option=\"magma\", begin=0.7)\n\n\n\n\n   \n   \n\n\nFigure 6: Event Study Results - Diesel\n\n\n\n\nThe visual evidence support that the parallel trend assumption holds, which is the main identifying assumption to interpret the effects as causal. In all models, the estimated coefficients for the pre-treatment period are close to zero and statistically insignificant. After the merger, I found that service station prices increased significantly, confirming my baseline regression results.\nIt’s worth noting that the results for Gasoline 93 are particularly clear in Figure 4, with treatment effects exhibiting persistence or stability post-merger. Conversely, in the case of Gasoline 97, the treatment effects tend to diminish after approximately 20 months. Similar downward trends are observed for diesel prices."
  },
  {
    "objectID": "posts/event_study_best_practices/event_study_best_practices.html",
    "href": "posts/event_study_best_practices/event_study_best_practices.html",
    "title": "Five Ways to Improve Your Event Study Plots",
    "section": "",
    "text": "The standard event study plot — point estimates with 95% pointwise confidence intervals — is ubiquitous in applied economics. But it leaves a surprising amount of information on the table.\nFreyaldenhoven, Hansen, Pérez & Shapiro (2021) proposed several practical enhancements that make event study plots more informative, more honest, and easier to interpret. Most of these are simple to implement yet rarely used in practice.\nIn this post, we implement five enhancements using the diff-diff Python package:\n\nSimultaneous confidence bands — correct for multiple testing across periods\nParenthetical outcome labels — show the baseline outcome level for economic interpretation\nJoint pre-trend Wald test — formal test instead of visual inspection\nLeveling-off test — are post-treatment effects constant over time?\nPutting it all together — a complete “best practices” event study plot\n\nEach enhancement builds on the previous one. By the end, we will have transformed a standard event study into a much richer visualization.\n\n1 Setup\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom scipy import stats as scipy_stats\n\nfrom diff_diff import MultiPeriodDiD, generate_did_data\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nnp.random.seed(42)\n\nc:\\Users\\danny\\anaconda3\\Lib\\site-packages\\pandas\\core\\computation\\expressions.py:22: UserWarning: Pandas requires version '2.10.2' or newer of 'numexpr' (version '2.8.7' currently installed).\n  from pandas.core.computation.check import NUMEXPR_INSTALLED\nc:\\Users\\danny\\anaconda3\\Lib\\site-packages\\pandas\\core\\arrays\\masked.py:56: UserWarning: Pandas requires version '1.4.2' or newer of 'bottleneck' (version '1.3.7' currently installed).\n  from pandas.core import (\n\n\nWe generate panel data with 200 units over 10 periods, where treatment begins at period 5. The true treatment effect is 3.0, and we include a slight time trend to make the example realistic.\n\n# Generate panel data\ndata = generate_did_data(\n    n_units=200,\n    n_periods=10,\n    treatment_effect=3.0,\n    treatment_fraction=0.4,\n    treatment_period=5,\n    noise_sd=2.0,\n    seed=42\n)\n\n# Fit event study\ndid = MultiPeriodDiD()\nresults = did.fit(\n    data,\n    outcome='outcome',\n    treatment='treated',\n    time='period',\n    post_periods=[5, 6, 7, 8, 9],\n    reference_period=4,\n    unit='unit',\n)\n\nresults.print_summary()\n\n================================================================================\n           Multi-Period Difference-in-Differences Estimation Results            \n================================================================================\n\nObservations:                   2000\nTreated observations:            800\nControl observations:           1200\nPre-treatment periods:             5\nPost-treatment periods:            5\nR-squared:                    0.4288\n\n--------------------------------------------------------------------------------\n                   Pre-Period Effects (Parallel Trends Test)                    \n--------------------------------------------------------------------------------\nPeriod              Estimate    Std. Err.     t-stat      P&gt;|t|   Sig.\n--------------------------------------------------------------------------------\n0                    -0.1379       0.5116     -0.270     0.7875       \n1                     0.0645       0.4904      0.132     0.8953       \n2                     0.4116       0.4916      0.837     0.4025       \n3                    -0.3952       0.5158     -0.766     0.4436       \n[ref: 4]               0.0000          ---        ---        ---       \n--------------------------------------------------------------------------------\n\n--------------------------------------------------------------------------------\n                         Post-Period Treatment Effects                          \n--------------------------------------------------------------------------------\nPeriod              Estimate    Std. Err.     t-stat      P&gt;|t|   Sig.\n--------------------------------------------------------------------------------\n5                     2.8728       0.4665      6.158     0.0000    ***\n6                     2.7684       0.5160      5.366     0.0000    ***\n7                     3.0072       0.5079      5.921     0.0000    ***\n8                     2.3354       0.4812      4.853     0.0000    ***\n9                     2.1831       0.5191      4.205     0.0000    ***\n--------------------------------------------------------------------------------\n\n--------------------------------------------------------------------------------\n                 Average Treatment Effect (across post-periods)                 \n--------------------------------------------------------------------------------\nParameter           Estimate    Std. Err.     t-stat      P&gt;|t|   Sig.\n--------------------------------------------------------------------------------\nAvg ATT               2.6334       0.3624      7.266     0.0000    ***\n--------------------------------------------------------------------------------\n\n95% Confidence Interval: [1.9226, 3.3441]\n\nSignif. codes: '***' 0.001, '**' 0.01, '*' 0.05, '.' 0.1\n================================================================================\n\n\nLet’s first see the standard event study plot — point estimates with pointwise 95% CIs:\n\n\nCode\n# Extract data from results\nref_period = results.reference_period\nperiods_all = sorted(results.period_effects.keys())\nestimates = [results.period_effects[p].effect for p in periods_all]\nses = [results.period_effects[p].se for p in periods_all]\n\n# Add reference period (beta = 0, se = 0)\nperiods_plot = sorted(periods_all + [ref_period])\nest_plot = []\nse_plot = []\nfor p in periods_plot:\n    if p == ref_period:\n        est_plot.append(0.0)\n        se_plot.append(0.0)\n    else:\n        est_plot.append(results.period_effects[p].effect)\n        se_plot.append(results.period_effects[p].se)\n\nest_plot = np.array(est_plot)\nse_plot = np.array(se_plot)\nperiods_plot = np.array(periods_plot)\n\nz = scipy_stats.norm.ppf(0.975)\n\nfig, ax = plt.subplots(figsize=(10, 6))\nax.errorbar(periods_plot, est_plot, yerr=z * se_plot,\n            fmt='o-', color='#2563eb', capsize=4, markersize=7,\n            linewidth=1.5, label='Pointwise 95% CI')\nax.axhline(0, color='gray', linestyle='--', linewidth=1)\nax.axvline(ref_period + 0.5, color='gray', linestyle=':', linewidth=1.5, alpha=0.7)\nax.set_xlabel('Period', fontsize=12)\nax.set_ylabel('Treatment Effect', fontsize=12)\nax.set_title('Standard Event Study', fontsize=14, fontweight='bold')\nax.legend(fontsize=10)\nax.grid(True, alpha=0.2)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nFigure 1: Standard event study plot with pointwise 95% confidence intervals. This is what most papers show — but it can be improved.\n\n\n\n\n\n\n\n2 Simultaneous Confidence Bands\nThe 95% confidence intervals in Figure 1 are pointwise: each interval has 95% coverage for its own period. But when we look at all periods simultaneously — as we always do in an event study — the joint coverage is much less than 95%.\nThis matters most for pre-trend testing. If we have 4 pre-treatment coefficients, each with a 95% CI, the probability that all four CIs contain the true value simultaneously is roughly 0.95^4 \\approx 0.81, not 0.95. We are over-rejecting the null of parallel trends.\nSup-t bands (Montiel Olea & Plagborg-Møller, 2019) solve this by finding a critical value c_\\alpha^{\\text{sup-t}} such that:\nP\\left(\\max_t \\frac{|\\hat{\\beta}_t - \\beta_t|}{\\hat{\\sigma}_t} \\leq c_\\alpha^{\\text{sup-t}}\\right) = 1 - \\alpha\nThe simultaneous band is then \\hat{\\beta}_t \\pm c_\\alpha^{\\text{sup-t}} \\cdot \\hat{\\sigma}_t. It is always wider than the pointwise CI — the price of honest joint inference.\nThe critical value is computed by simulation from the multivariate normal distribution implied by the variance-covariance matrix of the estimates.\n\ndef compute_supt_critical_value(vcov, alpha=0.05, n_sim=10000, seed=42):\n    \"\"\"\n    Compute the sup-t critical value for simultaneous confidence bands.\n    \n    Parameters\n    ----------\n    vcov : array-like\n        Variance-covariance matrix of the estimates.\n    alpha : float\n        Significance level.\n    n_sim : int\n        Number of simulation draws.\n    \n    Returns\n    -------\n    float\n        The sup-t critical value.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    k = vcov.shape[0]\n    \n    # Compute correlation matrix\n    sd = np.sqrt(np.diag(vcov))\n    corr = vcov / np.outer(sd, sd)\n    \n    # Ensure positive semi-definite\n    eigvals = np.linalg.eigvalsh(corr)\n    if np.any(eigvals &lt; -1e-10):\n        corr = corr + (-min(eigvals) + 1e-6) * np.eye(k)\n    \n    # Simulate from MVN(0, corr)\n    draws = rng.multivariate_normal(np.zeros(k), corr, size=n_sim)\n    \n    # Max absolute value across dimensions for each draw\n    max_abs = np.max(np.abs(draws), axis=1)\n    \n    # Critical value: (1-alpha) quantile\n    c_supt = np.quantile(max_abs, 1 - alpha)\n    \n    return c_supt\n\n\n# Extract VCV sub-matrix for the event study coefficients\n# results.vcov is the full regression VCV; we need the interaction terms\nidx_map = results.interaction_indices  # {period: column_index}\nidx_list = [idx_map[p] for p in periods_all]  # ordered by period\n\nvcov_full = results.vcov\nvcov_es = vcov_full[np.ix_(idx_list, idx_list)]\n\n# Compute sup-t critical value\nc_supt = compute_supt_critical_value(vcov_es, alpha=0.05, n_sim=50000)\nc_pointwise = scipy_stats.norm.ppf(0.975)\n\nprint(f'Pointwise critical value (z):  {c_pointwise:.3f}')\nprint(f'Sup-t critical value:          {c_supt:.3f}')\nprint(f'Sup-t is {c_supt / c_pointwise:.1f}x wider than pointwise')\n\nPointwise critical value (z):  1.960\nSup-t critical value:          2.708\nSup-t is 1.4x wider than pointwise\n\n\n\n\nCode\n# SE for non-reference periods (reference has se=0)\nse_nz = np.array([results.period_effects[p].se for p in periods_all])\nest_nz = np.array([results.period_effects[p].effect for p in periods_all])\nperiods_nz = np.array(periods_all)\n\nfig, ax = plt.subplots(figsize=(10, 6))\n\n# Sup-t bands (outer, lighter)\nsupt_lo = np.zeros(len(periods_plot))\nsupt_hi = np.zeros(len(periods_plot))\npw_lo = np.zeros(len(periods_plot))\npw_hi = np.zeros(len(periods_plot))\n\nj = 0\nfor i, p in enumerate(periods_plot):\n    if p == ref_period:\n        supt_lo[i] = 0.0\n        supt_hi[i] = 0.0\n        pw_lo[i] = 0.0\n        pw_hi[i] = 0.0\n    else:\n        supt_lo[i] = est_nz[j] - c_supt * se_nz[j]\n        supt_hi[i] = est_nz[j] + c_supt * se_nz[j]\n        pw_lo[i] = est_nz[j] - c_pointwise * se_nz[j]\n        pw_hi[i] = est_nz[j] + c_pointwise * se_nz[j]\n        j += 1\n\nax.fill_between(periods_plot, supt_lo, supt_hi,\n                alpha=0.15, color='#2563eb', label=f'Sup-t 95% band ($c$ = {c_supt:.2f})')\nax.fill_between(periods_plot, pw_lo, pw_hi,\n                alpha=0.25, color='#2563eb', label=f'Pointwise 95% CI ($z$ = {c_pointwise:.2f})')\nax.plot(periods_plot, est_plot, 'o-', color='#2563eb', markersize=7, linewidth=1.5, zorder=3)\n\nax.axhline(0, color='gray', linestyle='--', linewidth=1)\nax.axvline(ref_period + 0.5, color='gray', linestyle=':', linewidth=1.5, alpha=0.7)\nax.set_xlabel('Period', fontsize=12)\nax.set_ylabel('Treatment Effect', fontsize=12)\nax.set_title('Pointwise CIs vs. Simultaneous Sup-t Bands', fontsize=14, fontweight='bold')\nax.legend(fontsize=10)\nax.grid(True, alpha=0.2)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nFigure 2: Pointwise CIs (dark blue) vs. simultaneous sup-t bands (light blue). The sup-t bands maintain 95% joint coverage across all periods — the honest standard for testing parallel trends.\n\n\n\n\n\nThe sup-t bands are wider — that’s the price of joint coverage. Use them when making joint statements like:\n\n“All pre-treatment coefficients are jointly insignificant” (parallel trends)\n“The treatment effect is significant at every post-treatment period”\n\nPointwise CIs remain useful for period-specific inference (“what is the effect at t=7?”).\n\n\n3 Parenthetical Outcome Labels\nEvent study coefficients show changes relative to a reference period. But a coefficient of \\hat{\\beta}_7 = 3.2 is hard to interpret without knowing the baseline level of the outcome.\nIs 3.2 economically large or small? If the average outcome at the reference period is 40, then 3.2 / 40 \\approx 8\\% — a meaningful effect. If it’s 400, then 3.2 / 400 \\approx 0.8\\% — perhaps negligible.\nFreyaldenhoven et al. (2021) recommend adding a parenthetical label on the y-axis showing the baseline outcome level, like:\n\n0 (41.94)\n\nThis tells the reader: “the normalized zero on this axis corresponds to an outcome level of 41.94.” Readers can then immediately compute percentage effects in their heads.\n\n# Compute the baseline outcome: average of treated group at reference period\nbaseline_outcome = data[\n    (data['treated'] == 1) & (data['period'] == ref_period)\n]['outcome'].mean()\n\nprint(f'Average outcome for treated group at reference period {ref_period}: {baseline_outcome:.2f}')\n\nAverage outcome for treated group at reference period 4: 11.91\n\n\n\n\nCode\nfig, ax = plt.subplots(figsize=(10, 6))\n\nax.fill_between(periods_plot, supt_lo, supt_hi,\n                alpha=0.15, color='#2563eb', label='Sup-t 95% band')\nax.fill_between(periods_plot, pw_lo, pw_hi,\n                alpha=0.25, color='#2563eb', label='Pointwise 95% CI')\nax.plot(periods_plot, est_plot, 'o-', color='#2563eb', markersize=7, linewidth=1.5, zorder=3)\n\nax.axhline(0, color='gray', linestyle='--', linewidth=1)\nax.axvline(ref_period + 0.5, color='gray', linestyle=':', linewidth=1.5, alpha=0.7)\n\n# Parenthetical outcome label on y-axis\nax.set_ylabel(f'Treatment Effect\\n0 ({baseline_outcome:.2f})', fontsize=12)\n\nax.set_xlabel('Period', fontsize=12)\nax.set_title('Event Study with Baseline Outcome Label', fontsize=14, fontweight='bold')\nax.legend(fontsize=10)\nax.grid(True, alpha=0.2)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nFigure 3: The parenthetical label ‘0 (XX.XX)’ on the y-axis tells the reader the baseline outcome level, enabling immediate computation of percentage effects.\n\n\n\n\n\n\n\n4 Joint Pre-Trend Wald Test\nEyeballing whether pre-treatment coefficients “look close to zero” is subjective and has low statistical power. A formal joint Wald test provides a proper statistical answer.\nUnder the null hypothesis of parallel trends, all pre-treatment coefficients are jointly zero:\nH_0: \\beta_{-K} = \\beta_{-K+1} = \\cdots = \\beta_{-2} = 0\nThe Wald statistic is:\nW = \\hat{\\boldsymbol{\\beta}}_{\\text{pre}}^\\prime \\, \\hat{V}_{\\text{pre}}^{-1} \\, \\hat{\\boldsymbol{\\beta}}_{\\text{pre}} \\sim \\chi^2(K-1)\nwhere \\hat{\\boldsymbol{\\beta}}_{\\text{pre}} is the vector of pre-treatment coefficients and \\hat{V}_{\\text{pre}} is their variance-covariance sub-matrix.\n\n\n\n\n\n\nAlready in diff-diff for Imputation DiD\n\n\n\nThe ImputationDiD estimator has a built-in pretrend_test() method (Borusyak et al. 2024, Equation 9). The implementation below is a generalized version that works with any estimator that provides a VCV matrix, including MultiPeriodDiD.\n\n\n\ndef pretrend_wald_test(estimates, vcov, pre_indices):\n    \"\"\"\n    Joint Wald test for pre-treatment coefficients = 0.\n    \n    Parameters\n    ----------\n    estimates : array-like\n        All event study coefficient estimates (excluding reference).\n    vcov : array-like\n        VCV matrix for these estimates.\n    pre_indices : list of int\n        Indices into estimates/vcov for pre-treatment coefficients.\n    \n    Returns\n    -------\n    dict with 'f_stat', 'p_value', 'df'\n    \"\"\"\n    beta_pre = np.array([estimates[i] for i in pre_indices])\n    V_pre = vcov[np.ix_(pre_indices, pre_indices)]\n    \n    k = len(beta_pre)\n    W = beta_pre @ np.linalg.solve(V_pre, beta_pre)\n    p_value = 1 - scipy_stats.chi2.cdf(W, df=k)\n    \n    return {'f_stat': W, 'p_value': p_value, 'df': k}\n\n\n# Identify pre-treatment indices (all periods &lt; reference)\npre_periods = [p for p in periods_all if p &lt; ref_period]\npre_indices = [periods_all.index(p) for p in pre_periods]\n\npt_test = pretrend_wald_test(estimates, vcov_es, pre_indices)\n\nprint(f'Joint Pre-Trend Wald Test')\nprint(f'  H0: beta_pre = 0 (parallel trends)')\nprint(f'  Chi2({pt_test[\"df\"]}) = {pt_test[\"f_stat\"]:.3f}')\nprint(f'  p-value = {pt_test[\"p_value\"]:.4f}')\nif pt_test['p_value'] &gt; 0.05:\n    print(f'  -&gt; Fail to reject parallel trends at 5% level')\nelse:\n    print(f'  -&gt; Reject parallel trends at 5% level')\n\nJoint Pre-Trend Wald Test\n  H0: beta_pre = 0 (parallel trends)\n  Chi2(4) = 2.315\n  p-value = 0.6781\n  -&gt; Fail to reject parallel trends at 5% level\n\n\n\n\n5 Leveling-Off Test\nOnce we’ve established a treatment effect exists, a natural follow-up question is: are the post-treatment effects constant over time, or do they grow/decay?\nIf effects are approximately constant, we can summarize them with a single average ATT — a much simpler message. The leveling-off test formalizes this:\nH_0: \\beta_0 = \\beta_1 = \\cdots = \\beta_K\nWe test this using a contrast matrix R that takes successive differences of the post-treatment coefficients:\nR = \\begin{pmatrix} -1 & 1 & 0 & \\cdots \\\\ 0 & -1 & 1 & \\cdots \\\\ \\vdots & & \\ddots & \\end{pmatrix}\nThe Wald statistic is:\nW = (R\\hat{\\boldsymbol{\\beta}}_{\\text{post}})^\\prime (R\\hat{V}_{\\text{post}}R^\\prime)^{-1} (R\\hat{\\boldsymbol{\\beta}}_{\\text{post}}) \\sim \\chi^2(K)\nIf we fail to reject, it’s reasonable to summarize the post-treatment effects with a constant effects step function: a flat line at the average post-treatment coefficient.\n\ndef leveling_off_test(estimates, vcov, post_indices):\n    \"\"\"\n    Test H0: all post-treatment coefficients are equal.\n    \n    Parameters\n    ----------\n    estimates : array-like\n        All event study coefficient estimates (excluding reference).\n    vcov : array-like\n        VCV matrix for these estimates.\n    post_indices : list of int\n        Indices into estimates/vcov for post-treatment coefficients.\n    \n    Returns\n    -------\n    dict with 'f_stat', 'p_value', 'df', 'avg_effect'\n    \"\"\"\n    beta_post = np.array([estimates[i] for i in post_indices])\n    V_post = vcov[np.ix_(post_indices, post_indices)]\n    \n    k = len(beta_post)\n    \n    # Contrast matrix: successive differences\n    R = np.zeros((k - 1, k))\n    for i in range(k - 1):\n        R[i, i] = -1\n        R[i, i + 1] = 1\n    \n    Rb = R @ beta_post\n    RVR = R @ V_post @ R.T\n    \n    W = Rb @ np.linalg.solve(RVR, Rb)\n    p_value = 1 - scipy_stats.chi2.cdf(W, df=k - 1)\n    \n    # Average effect (for step function overlay)\n    avg_effect = np.mean(beta_post)\n    \n    return {'f_stat': W, 'p_value': p_value, 'df': k - 1, 'avg_effect': avg_effect}\n\n\n# Identify post-treatment indices\npost_periods = [p for p in periods_all if p &gt; ref_period]\npost_indices = [periods_all.index(p) for p in post_periods]\n\nlo_test = leveling_off_test(estimates, vcov_es, post_indices)\n\nprint(f'Leveling-Off Test')\nprint(f'  H0: beta_post are all equal (constant effects)')\nprint(f'  Chi2({lo_test[\"df\"]}) = {lo_test[\"f_stat\"]:.3f}')\nprint(f'  p-value = {lo_test[\"p_value\"]:.4f}')\nprint(f'  Average post-treatment effect: {lo_test[\"avg_effect\"]:.3f}')\nif lo_test['p_value'] &gt; 0.05:\n    print(f'  -&gt; Fail to reject constant effects')\n    print(f'  -&gt; Can summarize with average ATT = {lo_test[\"avg_effect\"]:.3f}')\nelse:\n    print(f'  -&gt; Reject constant effects (effects vary over time)')\n\nLeveling-Off Test\n  H0: beta_post are all equal (constant effects)\n  Chi2(4) = 3.398\n  p-value = 0.4936\n  Average post-treatment effect: 2.633\n  -&gt; Fail to reject constant effects\n  -&gt; Can summarize with average ATT = 2.633\n\n\n\n\n6 Putting It All Together\nNow let’s combine all five enhancements into a single publication-ready event study plot:\n\nSup-t bands (outer, light) alongside pointwise CIs (inner, dark)\nParenthetical outcome label on the y-axis\nPre-trend Wald test p-value shown below the plot\nLeveling-off test p-value shown below the plot\nConstant effects step function overlaid when leveling-off is not rejected\n\n\ndef plot_event_study_enhanced(\n    results,\n    data,\n    *,\n    outcome_col='outcome',\n    treatment_col='treated',\n    time_col='period',\n    alpha=0.05,\n    n_sim=50000,\n    show_supt=True,\n    show_baseline_outcome=True,\n    show_pretrend_pval=True,\n    show_leveloff_pval=True,\n    show_constant_fit=True,\n    color='#2563eb',\n    figsize=(11, 7),\n    title='Event Study',\n):\n    \"\"\"\n    Enhanced event study plot with Freyaldenhoven et al. (2021) best practices.\n    \"\"\"\n    ref_period = results.reference_period\n    all_periods = sorted(results.period_effects.keys())\n    \n    # Extract estimates and SEs\n    est = np.array([results.period_effects[p].effect for p in all_periods])\n    se = np.array([results.period_effects[p].se for p in all_periods])\n    \n    # Full period array including reference\n    p_plot = sorted(all_periods + [ref_period])\n    e_plot = np.array([0.0 if p == ref_period else results.period_effects[p].effect for p in p_plot])\n    s_plot = np.array([0.0 if p == ref_period else results.period_effects[p].se for p in p_plot])\n    p_plot = np.array(p_plot)\n    \n    # VCV sub-matrix\n    idx_map = results.interaction_indices\n    idx_list = [idx_map[p] for p in all_periods]\n    vcov = results.vcov[np.ix_(idx_list, idx_list)]\n    \n    # Critical values\n    z = scipy_stats.norm.ppf(1 - alpha / 2)\n    c_sup = compute_supt_critical_value(vcov, alpha=alpha, n_sim=n_sim) if show_supt else z\n    \n    # Pre/post indices\n    pre_idx = [all_periods.index(p) for p in all_periods if p &lt; ref_period]\n    post_idx = [all_periods.index(p) for p in all_periods if p &gt; ref_period]\n    \n    # Tests\n    pt = pretrend_wald_test(est, vcov, pre_idx) if show_pretrend_pval else None\n    lo = leveling_off_test(est, vcov, post_idx) if show_leveloff_pval else None\n    \n    # Baseline outcome\n    baseline = None\n    if show_baseline_outcome:\n        baseline = data[\n            (data[treatment_col] == 1) & (data[time_col] == ref_period)\n        ][outcome_col].mean()\n    \n    # --- Plot ---\n    fig, ax = plt.subplots(figsize=figsize)\n    \n    # Bands\n    if show_supt:\n        ax.fill_between(p_plot, e_plot - c_sup * s_plot, e_plot + c_sup * s_plot,\n                        alpha=0.12, color=color, label=f'Sup-t {int((1-alpha)*100)}% band')\n    ax.fill_between(p_plot, e_plot - z * s_plot, e_plot + z * s_plot,\n                    alpha=0.25, color=color, label=f'Pointwise {int((1-alpha)*100)}% CI')\n    ax.plot(p_plot, e_plot, 'o-', color=color, markersize=7, linewidth=1.5, zorder=3)\n    \n    # Constant effects step function\n    if show_constant_fit and lo is not None and lo['p_value'] &gt; alpha:\n        post_periods_arr = np.array([p for p in p_plot if p &gt; ref_period])\n        ax.hlines(lo['avg_effect'],\n                  xmin=post_periods_arr[0] - 0.3, xmax=post_periods_arr[-1] + 0.3,\n                  color='#dc2626', linewidth=2.5, linestyle='-', alpha=0.8,\n                  label=f'Constant fit ({lo[\"avg_effect\"]:.2f})', zorder=2)\n    \n    # Reference lines\n    ax.axhline(0, color='gray', linestyle='--', linewidth=1)\n    ax.axvline(ref_period + 0.5, color='gray', linestyle=':', linewidth=1.5, alpha=0.7)\n    \n    # Labels\n    ax.set_xlabel('Period', fontsize=12)\n    if baseline is not None:\n        ax.set_ylabel(f'Treatment Effect\\n0 ({baseline:.2f})', fontsize=12)\n    else:\n        ax.set_ylabel('Treatment Effect', fontsize=12)\n    ax.set_title(title, fontsize=14, fontweight='bold')\n    ax.legend(fontsize=9, loc='upper left')\n    ax.grid(True, alpha=0.2)\n    \n    # Test annotations below the plot\n    footnotes = []\n    if pt is not None:\n        footnotes.append(\n            f'Pre-trend Wald test: $\\\\chi^2$({pt[\"df\"]}) = {pt[\"f_stat\"]:.2f}, '\n            f'p = {pt[\"p_value\"]:.3f}'\n        )\n    if lo is not None:\n        footnotes.append(\n            f'Leveling-off test: $\\\\chi^2$({lo[\"df\"]}) = {lo[\"f_stat\"]:.2f}, '\n            f'p = {lo[\"p_value\"]:.3f}'\n        )\n    if footnotes:\n        footnote_text = '    |    '.join(footnotes)\n        ax.text(0.5, -0.12, footnote_text,\n                transform=ax.transAxes, fontsize=9.5, ha='center',\n                color='#444444', style='italic')\n    \n    plt.tight_layout()\n    plt.subplots_adjust(bottom=0.15)\n    plt.show()\n    \n    return {'pretrend_test': pt, 'leveloff_test': lo, 'baseline_outcome': baseline,\n            'c_supt': c_sup if show_supt else None}\n\n\n\nCode\ninfo = plot_event_study_enhanced(\n    results, data,\n    title='Best Practices Event Study',\n)\n\n\n\n\n\n\n\n\nFigure 4: The complete best-practices event study: simultaneous bands, baseline outcome label, formal test p-values, and constant effects fit. Compare with Figure 1.\n\n\n\n\n\nCompare Figure 4 with Figure 1. The enhanced version communicates much more:\n\nThe outer band shows that pre-treatment coefficients are jointly consistent with zero (sup-t band covers zero for all pre-periods)\nThe parenthetical label lets readers immediately gauge economic magnitude\nThe p-values provide formal statistical answers instead of visual guessing\nThe constant fit line (shown when the leveling-off test does not reject) gives a clean summary of the post-treatment effect\n\nAll of this can be produced with a single function call.\n\n\n7 Before and After\nTo drive the point home, here is a side-by-side comparison:\n\n\nCode\nfig, axes = plt.subplots(1, 2, figsize=(16, 6.5))\n\n# --- Left: Standard ---\nax = axes[0]\nax.errorbar(periods_plot, est_plot, yerr=z * se_plot,\n            fmt='o-', color='#2563eb', capsize=4, markersize=7,\n            linewidth=1.5, label='Pointwise 95% CI')\nax.axhline(0, color='gray', linestyle='--', linewidth=1)\nax.axvline(ref_period + 0.5, color='gray', linestyle=':', linewidth=1.5, alpha=0.7)\nax.set_xlabel('Period', fontsize=12)\nax.set_ylabel('Treatment Effect', fontsize=12)\nax.set_title('Standard', fontsize=14, fontweight='bold')\nax.legend(fontsize=9)\nax.grid(True, alpha=0.2)\n\n# --- Right: Enhanced ---\nax = axes[1]\n\n# Sup-t band\nax.fill_between(periods_plot, est_plot - c_supt * se_plot, est_plot + c_supt * se_plot,\n                alpha=0.12, color='#2563eb', label='Sup-t 95% band')\n# Pointwise\nax.fill_between(periods_plot, est_plot - z * se_plot, est_plot + z * se_plot,\n                alpha=0.25, color='#2563eb', label='Pointwise 95% CI')\nax.plot(periods_plot, est_plot, 'o-', color='#2563eb', markersize=7, linewidth=1.5, zorder=3)\n\n# Constant fit\nif lo_test['p_value'] &gt; 0.05:\n    post_p = periods_plot[periods_plot &gt; ref_period]\n    ax.hlines(lo_test['avg_effect'],\n              xmin=post_p[0] - 0.3, xmax=post_p[-1] + 0.3,\n              color='#dc2626', linewidth=2.5, linestyle='-', alpha=0.8,\n              label=f'Constant fit ({lo_test[\"avg_effect\"]:.2f})', zorder=2)\n\nax.axhline(0, color='gray', linestyle='--', linewidth=1)\nax.axvline(ref_period + 0.5, color='gray', linestyle=':', linewidth=1.5, alpha=0.7)\nax.set_xlabel('Period', fontsize=12)\nax.set_ylabel(f'Treatment Effect\\n0 ({baseline_outcome:.2f})', fontsize=12)\nax.set_title('Enhanced (Freyaldenhoven et al.)', fontsize=14, fontweight='bold')\nax.legend(fontsize=9, loc='upper left')\nax.grid(True, alpha=0.2)\n\n# Footnotes\nfootnote = (\n    f'Pre-trend Wald: $\\chi^2$({pt_test[\"df\"]}) = {pt_test[\"f_stat\"]:.2f}, '\n    f'p = {pt_test[\"p_value\"]:.3f}    |    '\n    f'Leveling-off: $\\chi^2$({lo_test[\"df\"]}) = {lo_test[\"f_stat\"]:.2f}, '\n    f'p = {lo_test[\"p_value\"]:.3f}'\n)\nax.text(0.5, -0.12, footnote, transform=ax.transAxes, fontsize=9, ha='center',\n        color='#444444', style='italic')\n\nplt.tight_layout()\nplt.subplots_adjust(bottom=0.15)\nplt.show()\n\n\n\n\n\n\n\n\nFigure 5: Before (standard) vs. After (enhanced). The same data, the same estimates — but the enhanced version communicates far more to the reader.\n\n\n\n\n\n\n\n8 Conclusions\nStandard event study plots are a good starting point, but they can — and should — communicate more. The five enhancements from Freyaldenhoven et al. (2021) are all straightforward to implement and add substantial value:\n\n\n\n\n\n\n\n\nEnhancement\nWhat It Adds\nWhen to Use\n\n\n\n\nSup-t bands\nJoint coverage across all periods\nAlways (especially for pre-trend assessment)\n\n\nOutcome label\nEconomic magnitude interpretation\nAlways\n\n\nPre-trend Wald test\nFormal parallel trends test\nAlways\n\n\nLeveling-off test\nConstant vs. dynamic effects\nWhen summarizing post-treatment effects\n\n\nConstant fit line\nVisual summary of average ATT\nWhen leveling-off is not rejected\n\n\n\nThe plot_event_study_enhanced() function wraps all of these into a single call. It works with any MultiPeriodDiD result that provides a variance-covariance matrix.\nThese are presentation improvements — they make existing results clearer. For robustness improvements (what to do when parallel trends might not hold), see our companion post on sensitivity analysis with HonestDiD, smoothest confounding paths, and functional SCBs.\n\n\n9 References\n\nFreyaldenhoven, S., Hansen, C., Pérez, J. P., & Shapiro, J. M. (2021). Visualization, Identification, and Estimation in the Linear Panel Event-Study Design. NBER Working Paper No. 29170.\nMontiel Olea, J. L., & Plagborg-Møller, M. (2019). Simultaneous Confidence Bands: Theory, Implementation, and an Application to SVARs. Journal of Applied Econometrics, 34(1), 1-17.\nBorusyak, K., Jaravel, X., & Spiess, J. (2024). Revisiting Event Study Designs: Robust and Efficient Estimation. Review of Economic Studies, 91(6), 3253-3285.\nRoth, J. (2022). Pretest with Caution: Event-Study Estimates after Testing for Parallel Trends. American Economic Review: Insights, 4(3), 305-322.\nGerber, I. (2025). diff-diff: A comprehensive Python package for Difference-in-Differences. GitHub.\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "posts/causal_arima/carima_data.html",
    "href": "posts/causal_arima/carima_data.html",
    "title": "Causal ARIMA Approach to Estimate Price Policy Changes on Sales",
    "section": "",
    "text": "C-ARIMA to estimate the causal effects in time series settings where no control unit is available.\nWhat is the effect of price reduction on sales? Imagine you are a popular e-commerce platform for books, such as Waterstones. As a part of your marketing strategy, you decide to reduce prices on all books. Now, you’re eager to assess the impact of this decision on the daily volume of sales. How can you measure this effect?\nObservational studies such as this pose significant challenges to identifying and estimating causal effects. Unlike A/B testing or randomized experiments, where the assignment mechanism (the process determining which units receive treatment and which receive control) is controlled and known, observational studies lack this clarity.\nPopular methods such as Difference-in-Differences (DiD) (Angrist and Pischke 2008) and Synthetic Control Methods (SCM) (Abadie, Diamond, and Hainmueller 2010) have been extensively used to evaluate the impact of interventions in the absence of experimental data across various fields, including economics and marketing. Recent advancements even combine these approaches, as seen in the Synthetic Difference-in-Differences (SDiD) (Arkhangelsky et al. 2019) estimator.\nHowever, these methods require the presence of control units that did not experience the intervention. In cases of widespread policy changes affecting all units—such as our book price reduction example—finding untreated units is often impossible.\nOn a recent article published in The Econometrics Journal (Volume 26, Issue 1), Menchetti, Cipollini, and Mealli (2022) propose a novel approach, Causal-ARIMA (C-ARIMA), to estimate the causal effect of an intervention in observational time series settings where no control unit is available.\nIn this post, we will explore how to use C-ARIMA in R estimate the impact of a treatment in settings where no controls or comparisons are available. We will demonstrate that, under certain structural assumptions, the Causal ARIMA approach can effectively recover the average treatment effect, providing insights for decision-making in scenarios where traditional methods may not suffice."
  },
  {
    "objectID": "posts/causal_arima/carima_data.html#estimation-procedure-and-inference",
    "href": "posts/causal_arima/carima_data.html#estimation-procedure-and-inference",
    "title": "Causal ARIMA Approach to Estimate Price Policy Changes on Sales",
    "section": "\n2.1 Estimation Procedure and Inference",
    "text": "2.1 Estimation Procedure and Inference\nIn order to estimate the causal effects with C-ARIMA, we need to follow a three-step process:\n\nEstimate the ARIMA model only in the pre-intervention period, so as to learn the dynamics of the dependent variable and the links with the covariates without being influenced by the treatment;\nBased on the process learned in the pre-intervention period, perform a prediction step and obtain an estimate of the counterfactual outcome during the post-intervention period in the absence of intervention;\nBy comparing the observations with the corresponding forecasts at any time point in the post-intervention period, evaluate the resulting differences, which represent the estimated point causal effects.\n\nLet W_{i,t} ∈ (0, 1) be a random variable describing the treatment assignment of unit i ∈ {1, . . . , N} at time t ∈ {1, . . . , T}, where 1 denotes that a “treatment” (or “intervention”) has taken place and 0 denotes control. Then, our estimands of interest are:\n\n\\text{Point Causal Effect: } \\tau_t(w,w') = Y_t(w) - Y_t(w') \\\\\n\\text{Cumulative Causal Effect: } \\Delta_t(w,w') = \\sum^{t}_{s+t^*+1} \\tau_t(w,w') \\\\\n\\text{Average Causal Effect: } \\bar\\tau_t(w,w') = \\frac{\\Delta_t(w,w')}{t-t^*}\n\nThen, we have two options to perform inference on the estimated effects: (i) we can rely in the Normality of the error terms, or (ii) we can resort to a Bootstrap Strategy by using resampled residuals in order to compute empirical critical values."
  },
  {
    "objectID": "posts/causal_arima/carima_data.html#c-arima-using-fable-package",
    "href": "posts/causal_arima/carima_data.html#c-arima-using-fable-package",
    "title": "Causal ARIMA Approach to Estimate Price Policy Changes on Sales",
    "section": "\n3.1 C-ARIMA using fable package",
    "text": "3.1 C-ARIMA using fable package\nLet’s start by fitting an ARIMA model on our pre-intervention period (between January and March 15):\n\nfit &lt;- df1 |&gt;\n    filter_index(\"2014-01-05\" ~ \"2014-03-15\") |&gt;\n    model(ARIMA(y ~ x1)\n)\n\nreport(fit) ## ARIMA(0,0,0)(0,0,1)[7]\n\nSeries: y \nModel: LM w/ ARIMA(0,0,0)(0,0,1)[7] errors \n\nCoefficients:\n        sma1      x1\n      0.2050  1.1993\ns.e.  0.1229  0.0019\n\nsigma^2 estimated as 1.429:  log likelihood=-110.95\nAIC=227.9   AICc=228.26   BIC=234.64\n\n\nNow we estimate the counterfactual outcome during the post-intervention period in the absence of intervention. For this, we first need to include the post-intervention evolution of the x1 regressor:\n\ndf1_x1 &lt;- df1 %&gt;% \n  select(Date, x1) %&gt;% \n  filter_index(\"2014-03-16\" ~ \"2014-04-14\")\n\nNow we generate the forecasted values:\n\n# Generate the forecasted values using the fitted model\nforecast_result &lt;- forecast(fit, new_data = df1_x1)\n\n\n\n\n\n\n\n\nFigure 2: Sales ($) ARIMA Forecast\n\n\n\n\nFigure 2 provides a graphical representation of the observed time series and the forecasted series in the absence of intervention. At the 1-month time horizon, the causal effect is significantly positive at the 5% level. Additionally, our ARIMA model is able to closely follow the series during the pre-intervention period. We can summarize this fitness level by calculating RMSE and R-Squared scores:\n\n##RMSE & R2_Score\nfore1 &lt;- fitted(fit) %&gt;% select(Date, .fitted)\n\naccuracy1 &lt;- df1 %&gt;% \n  left_join(fore1, by=\"Date\") %&gt;% \n  filter_index(\"2014-01-05\" ~ \"2014-03-15\")\n\nRMSE_CARIMA &lt;- RMSE(accuracy1$.fitted, accuracy1$y)\nR2_CARIMA &lt;- R2_Score(accuracy1$.fitted, accuracy1$y)\n\n\n\nPre-Intervention RMSE: 1.178076 \n\n\nPre-Intervention R-Squared: 0.8794006 \n\n\n\n3.1.1 Point Causal Effects\nThe point causal effect at time t can be estimated with the following code:\n\nATE_df &lt;- df1 %&gt;% \n  filter(Date &gt;= ymd(\"2014-03-16\") & Date &lt;= ymd(\"2014-04-14\"))\n\n# Point Effect\npoint_effect &lt;- ATE_df$y - forecast_result$y\n\n# Calculate the point estimate\npoint_estimate &lt;- mean(point_effect)\n\n# Calculate the lower bound of the 95% confidence interval\nlower_bound &lt;- quantile(point_effect, 0.025)\n\n# Calculate the upper bound of the 95% confidence interval\nupper_bound &lt;- quantile(point_effect, 0.975)\n\n# Create a data frame\ncausal_effect_df &lt;- data.frame(\n  Date = ATE_df$Date,\n  Point_Estimate = point_estimate,\n  Lower_Bound = lower_bound,\n  Upper_Bound = upper_bound\n)\n\n\n\n\n\n\n\n\nFigure 3: Causal Effect with 95% Confidence Intervals\n\n\n\n\nIn Figure 3 we observe that the impact of the price policy change is quite constant in time, around $10.\n\n3.1.2 Average Causal Effects\nThe temporal average effect indicates the number of additional books sold daily, on average, due to the permanent price reduction. We also want to do some inference, standard errors and confidence intervals. A confidence interval gives an interval within which we expect y_t to lie with a specified probability. For example, assuming that distribution of observations is normal, a 95% prediction interval for the h- step forecast is:\n\n\\hat{y}_{T+h|T} ± (1.96) \\hat \\sigma_h\n\nLet’s code this:\n\n# Calculate the point estimate\naverage_treatment_effect  &lt;- mean(point_estimate)\n\n# Standard error of the point estimates\nvariance_point_effect &lt;- distributional::variance(point_effect)\nstandard_deviation &lt;- sqrt(variance_point_effect)\nstandard_error &lt;- standard_deviation / sqrt(length(point_effect))\n\n# Confidence interval for the average treatment effect\nci_lower &lt;- average_treatment_effect - 1.96 * mean(standard_error)\nci_upper &lt;- average_treatment_effect + 1.96 * mean(standard_error)\n\n\n\nAverage Causal Effect: 10.39115 \n\n\nStandard Error: 0.2217059 \n\n\n95% Confidence Interval: [ 9.956609 ,  10.8257 ]\n\n\n\n3.1.3 Bootstrapped Confidence Intervals\nWhen a normal distribution for the residuals is an unreasonable assumption, one alternative is to use bootstrapping, which only assumes that the residuals are uncorrelated with constant variance. This is easily achieved by simply adding bootstrap=TRUE in the forecast() function.\n\nforecast_result &lt;- forecast(fit, new_data = df1_x1, bootstrap=TRUE)\n\n# Point Effect\npoint_effect &lt;- ATE_df$y - forecast_result$y\n\n# Calculate the point estimate\npoint_estimate &lt;- mean(point_effect)\n\n# Calculate the point estimate\naverage_treatment_effect  &lt;- mean(point_estimate)\n\n# Standard error of the point estimates\nvariance_point_effect &lt;- distributional::variance(point_effect)\nstandard_deviation &lt;- sqrt(variance_point_effect)\nstandard_error &lt;- standard_deviation / sqrt(length(point_effect))\n\n\n# Confidence interval for the average treatment effect\nci_lower &lt;- average_treatment_effect - 1.96 * mean(standard_error)\nci_upper &lt;- average_treatment_effect + 1.96 * mean(standard_error)\n\n\n\nAverage Treatment Effect: 10.38964 \n\n\nStandard Error: 0.2178212 \n\n\n95% Confidence Interval: [ 9.962708 ,  10.81657 ]\n\n\nThe results are quite similar in this case."
  },
  {
    "objectID": "posts/causal_arima/carima_data.html#the-causalarima-package",
    "href": "posts/causal_arima/carima_data.html#the-causalarima-package",
    "title": "Causal ARIMA Approach to Estimate Price Policy Changes on Sales",
    "section": "\n3.2 The CausalARIMA Package",
    "text": "3.2 The CausalARIMA Package\nThe authors of the paper are developing an R package called CausalArima for easier and faster application of the proposed method. The development version of the package can be accessed from https://github.com/FMenchetti/CausalArima.\nLet’s see how it works:\n\n# Install the missing dependencies first\n# install.packages(c(\"tidybayes\", \"forecast\", \"ggplot2\", \"gridExtra\", \"quantmod\"))\n\n# First update the locked packages (now that nothing holds them)\n# install.packages(c(\"xfun\", \"stringi\", \"jsonlite\", \"rlang\", \"cli\", \n#                    \"digest\", \"glue\", \"Rcpp\", \"vctrs\", \"stringr\", \"ggplot2\"))\n\n# Then install CausalArima\n#devtools::install_github(\"FMenchetti/CausalArima\")\n\nlibrary(CausalArima)\n\n\n# Causal effect estimation\n# fit the model - Causal effect estimation\nce &lt;- CausalArima(y = ts(y, start = start, frequency = 1), \n                  dates = dates, \n                  int.date = int.date,\n                  xreg =x1, \n                  nboot = 1000)\n\nHow to obtain the plot of the forecast:\n\n\n\n\n\n\n\nFigure 4: Sales ($) ARIMA Forecast\n\n\n\n\n\nimpact_p &lt;- plot(ce, type=\"impact\", color_line=mycolors[1], color_intervals=\"#91D1C2B2\")\ngrid.arrange(impact_p$plot, impact_p$cumulative_plot)\n\n\n\n\n\n\n\n\n3.2.1 Inference\nDoing inference with this package is straightforward, with the options of using the normality assumption or the bootstrap alternative:\n\nsummary(ce)\n\n                                      \nPoint causal effect            12.257 \nStandard error                 1.211  \nLeft-sided p-value             1      \nBidirectional p-value          0      \nRight-sided p-value            0      \n                                      \nCumulative causal effect       310.709\nStandard error                 6.634  \nLeft-sided p-value             1      \nBidirectional p-value          0      \nRight-sided p-value            0      \n                                      \nTemporal average causal effect 10.357 \nStandard error                 0.221  \nLeft-sided p-value             1      \nBidirectional p-value          0      \nRight-sided p-value            0      \n\nsummary_model &lt;- impact(ce, format=\"html\")\n\n\nsummary_model$impact_norm$average\n\n\n\nestimate\nsd\np_value_left\np_value_bidirectional\np_value_right\n\n\n10.357\n0.221\n1\n0\n0\n\n\n\n\n\nsummary_model$impact_boot$average\n\n\n\n\nestimates\ninf\nsup\nsd\n\n\n\nobserved\n117.049\nNA\nNA\nNA\n\n\nforecasted\n106.692\n106.264\n107.142\n0.222\n\n\nabsolute_effect\n10.357\n9.907\n10.784\n0.222\n\n\nrelative_effect\n0.097\n0.093\n0.101\n0.002"
  },
  {
    "objectID": "posts/ab_testing1/1_ab_testing.html",
    "href": "posts/ab_testing1/1_ab_testing.html",
    "title": "A/B Testing Analysis",
    "section": "",
    "text": "A/B testing Analysis using Python.\nAs data scientists, we are frequently confronted with questions like: “Does X affects Y?” Here, Y is the outcome that we care about, while X could be a new feature, product, policy or experience. For example, a website owner would like to ask if a new web page design leads to a higher click-through rate or sale. Similarly, Spotify may to know whether their new interface leads to more minutes of music played, while Zalando aims to assess the impact of their marketing actions on purchases.\nBut how do we go about answering such causal questions? For the Platform Economy, the solution lies in experimentation. By randomly assigning some kind of treatment (new interface design, new product, etc.) to a subset of users (Group A) and comparing their behavior or outcomes (such as revenue, visits, clicks, etc.) to those who did not receive the treatment (Group B), we can effectively isolate the causal impact of the change. This is at the core of A/B testing (also known as Randomized Control Trials), which is considered the workhorse method of experimentation for platforms like Spotify, Uber, Netflix, an more.\nIn this article, we will delve into the analysis involved in A/B testing using Python, with a specific focus on Randomization Checks and various methods or statistical tools for studying Average Treatment Effects. Let’s go!"
  },
  {
    "objectID": "posts/ab_testing1/1_ab_testing.html#violinplots",
    "href": "posts/ab_testing1/1_ab_testing.html#violinplots",
    "title": "A/B Testing Analysis",
    "section": "1.1. Violinplots",
    "text": "1.1. Violinplots\nA first visual approach is the violinplot. The violinplot plots depicts distributions of numeric data for one or more groups using density curves. These densities —usually smoothed by a kernel density estimator— are displayed along the y-axis so that we can compare them. By default, the .violintplot() function from the seaborn library also adds a miniature boxplot inside. Figure 1 shows the violinplots of our baseline variables:\n\n\nCode\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n\n# Create subplots\nfig, axs = plt.subplots(3, 3, figsize=(15, 12))\n\n# Plot histogram for 'days_since'\nfiltered_df = df[df['days_since'] &gt; 0]\nsns.violinplot(data=filtered_df, x='group', y='days_since', palette='viridis', ax=axs[0, 0])\naxs[0, 0].set_xlabel('Days Since Last Activity')\naxs[0, 0].set_ylabel('Customers')\naxs[0, 0].set_title('Violinplot of Days Since')\n\n# Plot histogram for 'visits'\nfiltered_df = df[df['visits'] &gt; 0]\nsns.violinplot(data=filtered_df, x='group', y='visits', palette='viridis', ax=axs[0, 1])\naxs[0, 1].set_xlabel('Visits')\naxs[0, 1].set_ylabel('Customers')\naxs[0, 1].set_title('Violinplot of Website Visits')\n\n# Plot histogram for 'past_purch'\nfiltered_df = df[(df['past_purch'] &gt; 0) & (df['past_purch'] &lt; 2000)]\nsns.violinplot(data=filtered_df, x='group', y='past_purch', palette='viridis', ax=axs[0, 2])\naxs[0, 2].set_xlabel('Past Purchases ($)')\naxs[0, 2].set_ylabel('Customers')\naxs[0, 2].set_title('Violinplot of Past Purchases')\n\n# Plot histogram for 'chard'\nfiltered_df = df[(df['chard'] &gt; 0) & (df['chard'] &lt; 2000)]\nsns.violinplot(data=filtered_df, x='group', y='chard', palette='viridis', ax=axs[1, 0])\naxs[1, 0].set_xlabel('Chardonnay')\naxs[1, 0].set_ylabel('Customers')\naxs[1, 0].set_title('Violinplot of Chardonnay Purchases')\n\n# Plot histogram for 'sav_blanc'\nfiltered_df = df[(df['sav_blanc'] &gt; 0) & (df['sav_blanc'] &lt; 2000)]\nsns.violinplot(data=filtered_df, x='group', y='sav_blanc', palette='viridis', ax=axs[1, 1])\naxs[1, 1].set_xlabel('Sauvignon Blanc')\naxs[1, 1].set_ylabel('Customers')\naxs[1, 1].set_title('Violinplot of Sauvignon Blanc Purchases')\n\n# Plot histogram for 'syrah'\nfiltered_df = df[(df['syrah'] &gt; 0) & (df['syrah'] &lt; 1000)]\nsns.violinplot(data=filtered_df, x='group', y='syrah', palette='viridis', ax=axs[1, 2])\naxs[1, 2].set_xlabel('Syrah')\naxs[1, 2].set_ylabel('Customers')\naxs[1, 2].set_title('Violinplot of Syrah Purchases')\n\n# Plot histogram for 'cab'\nfiltered_df = df[(df['cab'] &gt; 0) & (df['cab'] &lt; 500)]\nsns.violinplot(data=filtered_df, x='group', y='cab', palette='viridis', ax=axs[2, 0])\naxs[2, 0].set_xlabel('Cabernet')\naxs[2, 0].set_ylabel('Customers')\naxs[2, 0].set_title('Violinplot of Cabernet Purchases')\n\n# Hide the empty subplots\naxs[2, 1].axis('off')\naxs[2, 2].axis('off')\n\n# Adjust layout\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nFigure 1: Violinplot of Baseline Variables\n\n\n\n\n\nThe shape of the distributions across group is quite similar, indicating comparable distributions of baseline variables. This similarity confirms that the randomization process was successful in achieving balance between the treatment and control groups."
  },
  {
    "objectID": "posts/ab_testing1/1_ab_testing.html#histograms",
    "href": "posts/ab_testing1/1_ab_testing.html#histograms",
    "title": "A/B Testing Analysis",
    "section": "1.2. Histograms",
    "text": "1.2. Histograms\nAnother intuitive way to plot a distribution is the histogram. The histogram groups the data into equally wide bins and plots the number of observations within each bin.\nThe full distributions of baseline variables should also be the same between treatment groups in this case:\n\n\nCode\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Create subplots\nfig, axs = plt.subplots(3, 3, figsize=(18, 12))\n\n# Plot histogram for 'days_since'\nsns.histplot(data=df, x='days_since', hue='group', binwidth=25, stat='density', common_norm=False, alpha=0.8, multiple='stack', palette='viridis', ax=axs[0, 0])\naxs[0, 0].set_xlim(0, 700)\naxs[0, 0].set_xlabel('Days Since Last Activity')\naxs[0, 0].set_ylabel('Customers')\naxs[0, 0].set_title('Distribution of days_since by treatment group')\n\n# Plot histogram for 'visits'\nfiltered_df = df[df['visits'] &gt; 0]\nsns.histplot(data=filtered_df, x='visits', hue='group', binwidth=1, stat='density', common_norm=False, alpha=0.8, multiple='stack', palette='viridis', ax=axs[0, 1])\naxs[0, 1].set_xlim(0, 35)\naxs[0, 1].set_xlabel('Visits')\naxs[0, 1].set_ylabel('Customers')\naxs[0, 1].set_title('Distribution of website visits by treatment group')\n\n# Plot histogram for 'past_purch'\nfiltered_df = df[df['past_purch'] &gt; 0]\nsns.histplot(data=filtered_df, x='past_purch', hue='group', binwidth=25, stat='density', common_norm=False, alpha=0.8, multiple='stack', palette='viridis', ax=axs[0, 2])\naxs[0, 2].set_xlim(0, 2000)\naxs[0, 2].set_xlabel('Past Purchases ($)')\naxs[0, 2].set_ylabel('Customers')\naxs[0, 2].set_title('Distribution of past purchases by treatment group')\n\n# Plot histogram for 'chard'\nfiltered_df = df[df['chard'] &gt; 0]\nsns.histplot(data=filtered_df, x='chard', hue='group', binwidth=25, stat='density', common_norm=False, alpha=0.8, multiple='stack', palette='viridis', ax=axs[1, 0])\naxs[1, 0].set_xlim(0, 2000)\naxs[1, 0].set_xlabel('Chardonnay')\naxs[1, 0].set_ylabel('Customers')\naxs[1, 0].set_title('Distribution of Chardonnay by treatment group')\n\n# Plot histogram for 'sav_blanc'\nfiltered_df = df[df['sav_blanc'] &gt; 0]\nsns.histplot(data=filtered_df, x='sav_blanc', hue='group', binwidth=25, stat='density', common_norm=False, alpha=0.8, multiple='stack', palette='viridis', ax=axs[1, 1])\naxs[1, 1].set_xlim(0, 2000)\naxs[1, 1].set_xlabel('Sauvignon Blanc')\naxs[1, 1].set_ylabel('Customers')\naxs[1, 1].set_title('Distribution of Sauvignon Blanc by treatment group')\n\n# Plot histogram for 'syrah'\nfiltered_df = df[df['syrah'] &gt; 0]\nsns.histplot(data=filtered_df, x='syrah', hue='group', binwidth=25, stat='density', common_norm=False, alpha=0.8, multiple='stack', palette='viridis', ax=axs[1, 2])\naxs[1, 2].set_xlim(0, 1000)\naxs[1, 2].set_xlabel('Syrah')\naxs[1, 2].set_ylabel('Customers')\naxs[1, 2].set_title('Distribution of Syrah by treatment group')\n\n# Plot histogram for 'cab'\nfiltered_df = df[df['cab'] &gt; 0]\nsns.histplot(data=filtered_df, x='cab', hue='group', binwidth=25, stat='density', common_norm=False, alpha=0.8, multiple='stack', palette='viridis', ax=axs[2, 0])\naxs[2, 0].set_xlim(0, 500)\naxs[2, 0].set_xlabel('Cabernet')\naxs[2, 0].set_ylabel('Customers')\naxs[2, 0].set_title('Distribution of Cabernet by treatment group')\n\n# Hide empty subplots\nfor ax in axs.flat[7:]:\n    ax.axis('off')\n\n# Adjust layout\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nFigure 2: Histograms of Baseline Variables\n\n\n\n\n\nRandomization checks out again!"
  },
  {
    "objectID": "posts/ab_testing1/1_ab_testing.html#t-tests",
    "href": "posts/ab_testing1/1_ab_testing.html#t-tests",
    "title": "A/B Testing Analysis",
    "section": "1.3. T-Tests",
    "text": "1.3. T-Tests\nNow that we’ve visualized differences between distributions, let’s move on to a more rigorous approach for assessing statistical significance. Visualization provides intuition by allowing us to visually inspect differences, but we also want to be able to quantify the significance of these differences.\nThe most common test is the student t-test. T-tests are generally used to compare means. In this case, we want to test whether the means of the days_since, visits and past_purch distribution are the same across two groups: Treated (Email A + Email B) vs. Control. This test statistic is given by:\n\nstat = \\frac{|\\bar x_1 - \\bar x_2|}{\\sqrt{s^2 / n }}\n\nWhere \\bar x is the sample mean and s is the sample standard deviation. Under mild conditions, the test statistic is asymptotically distributed as a student t distribution.\nWe use the ttest_ind function from scipy to perform the t-test. The function returns both the test statistic and the implied p-value.\n\nfrom scipy.stats import ttest_ind\n\n# Treated vs Control Categorical Variable\ndf['treat'] = df['group'].map({'email_A': 'treated', 'email_B': 'treated', 'ctrl': 'control'})\n\n## Days Since\ndays_since_treated = df.loc[df.treat=='treated', 'days_since'].values\ndays_since_ctrl = df.loc[df.treat=='control', 'days_since'].values\n\nstat, p_value = ttest_ind(days_since_treated, days_since_ctrl)\nprint(f\"days_since t-test: statistic={stat:.4f}, p-value={p_value:.4f}\")\n\ndays_since t-test: statistic=0.0154, p-value=0.9877\n\n\n\n\nvisits t-test: statistic=-0.2451, p-value=0.8064\npast_purch t-test: statistic=0.4345, p-value=0.6639\n\n\nThe p-values from these tests all are above 0.1, therefore we do not reject the null hypothesis of no difference in means across treatment and control groups.\nThe thing is, our example has more than 2 groups to compare. With multiple groups, the most popular test is the F-test. The F-test compares the variance of a variable across different groups. The F-test statistic is\n\n\\text{stat} = \\frac{\\text{between-group variance}}{\\text{within-group variance}} = \\frac{\\sum_{g} \\big( \\bar x_g - \\bar x \\big) / (G-1)}{\\sum_{g} \\sum_{i \\in g} \\big( \\bar x_i - \\bar x_g \\big) / (N-G)}\n\nWhere G is the number of groups, N is the number of observations, \\bar x is the overall mean and \\bar x_g is the mean within group g. Under the null hypothesis of group independence, the f-statistic is F-distributed.\n\nfrom scipy.stats import f_oneway\n\n# Days Since\ndays_since_A = df.loc[df.group=='email_A', 'days_since'].values\ndays_since_B = df.loc[df.group=='email_B', 'days_since'].values\ndays_since_ctrl = df.loc[df.group=='ctrl', 'days_since'].values\n\nstat, p_value = f_oneway(days_since_A, days_since_B, days_since_ctrl)\nprint(f\"days_since F-statistic={stat:.4f}, p-value={p_value:.4f}\")\n\ndays_since F-statistic=0.1642, p-value=0.8485\n\n\n\n\nvisits F-statistic=0.0336, p-value=0.9670\npast_purch F-statistic=0.2618, p-value=0.7697\n\n\nHere again, the p-values are telling us that these distributions across treatment are likely similar."
  },
  {
    "objectID": "posts/ab_testing1/1_ab_testing.html#standardized-mean-difference",
    "href": "posts/ab_testing1/1_ab_testing.html#standardized-mean-difference",
    "title": "A/B Testing Analysis",
    "section": "1.4. Standardized Mean Difference",
    "text": "1.4. Standardized Mean Difference\nIn general, it is good practice to always perform a test for difference in means on all variables across the treatment and control group, when we are running A/B tests. However, since the denominator of the t-test statistic depends on the sample size, the corresponding p-values are not directly comparable across studies.\nAs alternative, we can use the standardized mean difference (SMD), which is just a standardized difference, which can be computed as:\n\nSMD = \\frac{|\\bar x_1 - \\bar x_2|}{\\sqrt{(s^2_1 + s^2_2) / 2}}\n\nUsually a value below 0.1 is considered a “small” difference.\nIt is good practice to collect average values of all variables across treatment and control group and a measure of distance between the two — either the t-test or the SMD — into a table that is called balance table.\nLet’s use the create_table_one function from the causalml library to generate it.\n\n\nCode\nimport numpy as np\nimport pandas as pd\n\ndef smd(feature, treatment):\n    \"\"\"Calculate the standard mean difference (SMD) of a feature between the\n    treatment and control groups.\n\n    Args:\n        feature (pandas.Series): a column of a feature to calculate SMD for\n        treatment (pandas.Series): a column that indicate whether a row is in\n                                   the treatment group or not\n\n    Returns:\n        (float): The SMD of the feature\n    \"\"\"\n    t = feature[treatment == 1]\n    c = feature[treatment == 0]\n    return (t.mean() - c.mean()) / np.sqrt(0.5 * (t.var() + c.var()))\n\n\n\ndef create_table_one(data, treatment_col, features):\n    \"\"\"Report balance in input features between the treatment and control groups.\n\n    Args:\n        data (pandas.DataFrame): total or matched sample data\n        treatment_col (str): the column name for the treatment\n        features (list of str): the column names of features\n\n    Returns:\n        (pandas.DataFrame): A table with the means and standard deviations in\n            the treatment and control groups, and the SMD between two groups\n            for the features.\n    \"\"\"\n    t1 = pd.pivot_table(\n        data[features + [treatment_col]],\n        columns=treatment_col,\n        aggfunc=[lambda x: \"{:.2f} ({:.2f})\".format(x.mean(), x.std())],\n    )\n    t1.columns = t1.columns.droplevel(level=0)\n    t1[\"SMD\"] = data[features].apply(lambda x: smd(x, data[treatment_col])).round(4)\n\n    n_row = pd.pivot_table(\n        data[[features[0], treatment_col]], columns=treatment_col, aggfunc=[\"count\"]\n    )\n    n_row.columns = n_row.columns.droplevel(level=0)\n    n_row[\"SMD\"] = \"\"\n    n_row.index = [\"n\"]\n\n    t1 = pd.concat([n_row, t1], axis=0)\n    t1.columns.name = \"\"\n    t1.columns = [\"Control\", \"Treatment\", \"SMD\"]\n    t1.index.name = \"Variable\"\n\n    return t1\n\n\n\ndf['treat'] = df['group'].map({'email_A': 1, 'email_B': 1, 'ctrl': 0})\n\ncreate_table_one(df, 'treat', ['days_since', 'visits', 'past_purch', 'chard', 'sav_blanc', 'syrah', 'cab'])\n\n\n\nTable 3: Balance Table\n\n\n\n\n\n\n\n\n\n\nControl\nTreatment\nSMD\n\n\nVariable\n\n\n\n\n\n\n\nn\n41330\n82658\n\n\n\ncab\n16.52 (47.27)\n16.26 (46.67)\n-0.0053\n\n\nchard\n71.67 (199.39)\n74.14 (211.84)\n0.012\n\n\ndays_since\n89.98 (89.50)\n89.99 (89.95)\n0.0001\n\n\npast_purch\n188.27 (298.18)\n189.06 (303.33)\n0.0026\n\n\nsav_blanc\n73.63 (203.37)\n71.87 (197.25)\n-0.0088\n\n\nsyrah\n26.45 (73.91)\n26.79 (75.04)\n0.0045\n\n\nvisits\n5.95 (2.85)\n5.94 (2.86)\n-0.0015\n\n\n\n\n\n\n\n\n\n\nIn the first two columns of Table 3, we observe the average of the different variables across the treatment and control groups, with standard errors provided in parentheses. However, it’s in the last column where the values of the standardized mean difference (SMD) are presented. A standardized difference below |0.1| for all variables suggests that the two groups are likely similar."
  },
  {
    "objectID": "posts/ab_testing1/1_ab_testing.html#difference-in-means",
    "href": "posts/ab_testing1/1_ab_testing.html#difference-in-means",
    "title": "A/B Testing Analysis",
    "section": "2.1. Difference in Means",
    "text": "2.1. Difference in Means\nLet’s start with the first causal question (Treated vs Control). We can just compare the average outcome post-treatment Y_1 across control and treated units and randomization guarantees that this difference is, on average, due to the treatment alone:\n\n\\widehat {ATE}^{simple} = \\bar Y_{t=1, d=1} - \\bar Y_{t=1, d=0}\n\nIn our case, we compute the average purchases (purch) after the campaign in the treatment group, minus the average revenue post ad campaign in the control group:\n\n\nCode\nate = df.loc[df.treat == 'treated', 'purch'].mean() - df.loc[df.treat == 'control', 'purch'].mean()\nn_treat = df[df.treat == 'treated'].shape[0]\n\nprint( f'Average Treatment Effects: ${ate.round(2)}' )\nprint( f'Average Increased Revenues: ${round(ate*n_treat):,.0f}' )\n\n\nAverage Treatment Effects: $13.32\nAverage Increased Revenues: $1,101,358\n\n\nThis \\text{ATE} is telling us that, on average, each recipient of the email campaign resulted in an additional revenue of $13.32 compared to those who did not receive the campaign. In other words, the email campaign had a positive impact on increasing purchases.\nThis suggests that, overall, the email campaign resulted in an increased revenues of $1,101,358."
  },
  {
    "objectID": "posts/ab_testing1/1_ab_testing.html#linear-regression",
    "href": "posts/ab_testing1/1_ab_testing.html#linear-regression",
    "title": "A/B Testing Analysis",
    "section": "2.2. Linear Regression",
    "text": "2.2. Linear Regression\nWe can obtain the same estimate by regressing the post-treatment outcome purch on the treatment indicator treat:\n\nY_{i} = \\alpha + \\beta D_i + \\varepsilon_i\n\nwhere D_i is the treatment indicator (equal to 1 for treated units and 0 for control units), \\alpha is the intercept term and \\beta is our coefficient of interest representing the Average Treatment Effect.\n\nimport statsmodels.formula.api as smf\n\nsmf.ols('purch ~ treat', data=df).fit().summary().tables[1]\n\n\n\nTable 6: A/B Regression Results I\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n12.4203\n0.268\n46.360\n0.000\n11.895\n12.945\n\n\ntreat[T.treated]\n13.3243\n0.328\n40.608\n0.000\n12.681\n13.967\n\n\n\n\n\n\n\n\nThe advantage of running a linear regression is that we can directly test whether the difference in outcomes between the treatment and control groups is statistically significant, providing us with more robust information regarding the effectiveness of the campaign.\nTable 6 is not only confirming an \\text{ATE} of $13.32, but also telling us that this difference is statistically different from zero. This provides stronger evidence regarding the effectiveness of the email campaign in increasing purchases compared to the control group.\nWe can also compare both types of campaigns against our control group:\n\nsmf.ols('purch ~ group', data=df).fit().summary().tables[1]\n\n\n\nTable 7: A/B Regression Results II\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n12.4203\n0.268\n46.360\n0.000\n11.895\n12.945\n\n\ngroup[T.email_A]\n13.2026\n0.379\n34.846\n0.000\n12.460\n13.945\n\n\ngroup[T.email_B]\n13.4460\n0.379\n35.488\n0.000\n12.703\n14.189\n\n\n\n\n\n\n\n\nIt’s important to note that this regression results in Table 7 are not directly addressing our second causal question (Email A vs. Email B). Instead, the coefficients obtained from the regression are comparing each email campaign against the control group. In any case, the table suggests that, on average, Campaign B generates more purchases. It remains to be determined whether this difference is statistically significant."
  },
  {
    "objectID": "posts/ab_testing1/1_ab_testing.html#distribution-comparison",
    "href": "posts/ab_testing1/1_ab_testing.html#distribution-comparison",
    "title": "A/B Testing Analysis",
    "section": "2.3. Distribution Comparison",
    "text": "2.3. Distribution Comparison\nFigure 3 illustrates the purchase distribution by treatment, providing a comprehensive view of the data. However, it’s important to note that while we have established a statistically significant difference, our estimation of this distribution assumes a Normal Distribution, which may not always hold true. In future articles, we will delve deeper into alternative methods that do not rely on such assumptions, ensuring a more robust analysis.\n\n\nCode\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Control vs Treated\ndf_control = df[df['treat'] == 'control']\ndf_treated = df[df['treat'] == 'treated']\n\n# Calculate mean and standard error for email_A and email_B\nmean_open_A = df_control['purch'].mean()\nstd_error_A = df_control['purch'].std() / np.sqrt(len(df_control))\n\nmean_open_B = df_treated['purch'].mean()\nstd_error_B = df_treated['purch'].std() / np.sqrt(len(df_treated))\n\nnormal_data_A = np.random.normal(mean_open_A, std_error_A, 10000)\nnormal_data_B = np.random.normal(mean_open_B, std_error_B, 10000)\n\nfig, axs = plt.subplots(1, 1)\n\nsns.histplot(normal_data_A, kde=True, stat='density', label='Control', bins=20, palette='viridis')\nsns.histplot(normal_data_B, kde=True, stat='density', label='Treated', bins=20, palette='viridis')\n\nplt.xlabel('Purchases')\nplt.ylabel('Density')\nplt.title('Distribution of Purchases, by Treatment')\nplt.axvline(mean_open_A, color=sns.color_palette()[0], linestyle='--', label='Mean Control')\nplt.axvline(mean_open_B, color=sns.color_palette()[1], linestyle='--', label='Mean Treated')\nplt.errorbar(mean_open_A, 0.02, xerr=std_error_A, fmt='o', color=sns.color_palette()[0], label='95% CI Control')\nplt.errorbar(mean_open_B, 0.02, xerr=std_error_B, fmt='o', color=sns.color_palette()[1], label='95% CI Treated')\nplt.legend(ncol=3, bbox_to_anchor=(0.5, -0.15), loc='upper center') \n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nFigure 3: Purchase Distribution, by Treatment\n\n\n\n\n\nAgain, assuming Normality, we can also estimate confidence intervals:\n\n\nCode\nimport scipy.stats as stats\n\ndf_control = df[df['treat'] == 'control']\ndf_treated = df[df['treat'] == 'treated']\n        \n# Calculate mean and standard error for email_A and email_B\nmean_open_A = df_control['purch'].mean()\nstd_error_A = df_control['purch'].std() / np.sqrt(len(df_control))\n\nmean_open_B = df_treated['purch'].mean()\nstd_error_B = df_treated['purch'].std() / np.sqrt(len(df_treated))\n    \n# Calculate the confidence interval using the t-distribution\nconfidence_interval_A = stats.norm.interval(0.95, loc=mean_open_A, scale=std_error_A)\nconfidence_interval_B = stats.norm.interval(0.95, loc=mean_open_B, scale=std_error_B)\n\n\n\n\nMean Response Control: 12.4\nMean Response Treated: 25.7\n95% Confidence Interval for Control: [12.00, 12.84]\n95% Confidence Interval for Treated: [25.34, 26.15]"
  },
  {
    "objectID": "posts/ab_testing1/1_ab_testing.html#email-a-vs-email-b",
    "href": "posts/ab_testing1/1_ab_testing.html#email-a-vs-email-b",
    "title": "A/B Testing Analysis",
    "section": "2.4. Email A vs Email B",
    "text": "2.4. Email A vs Email B\nNow, let’s turn our attention to the second causal question: Which email design, Email A or Email B, is more effective in driving engagement and sales?\n\n\nCode\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef plot_variable_comparison(df):\n    variables = ['open', 'click', 'purch']\n\n    fig, axs = plt.subplots(1, 3, figsize=(18, 6))\n\n    for i, variable in enumerate(variables):\n        # Control vs Treated\n        df_control = df[df['group'] == 'email_A']\n        df_treated = df[df['group'] == 'email_B']\n\n        # Calculate mean and standard error for control and treated groups\n        mean_control = df_control[variable].mean()\n        std_error_control = df_control[variable].std() / np.sqrt(len(df_control))\n        mean_treated = df_treated[variable].mean()\n        std_error_treated = df_treated[variable].std() / np.sqrt(len(df_treated))\n\n        # Generate normal distribution data for control and treated groups\n        normal_data_control = np.random.normal(mean_control, std_error_control, 10000)\n        normal_data_treated = np.random.normal(mean_treated, std_error_treated, 10000)\n\n        # Plot histograms for control and treated groups\n        sns.histplot(normal_data_control, kde=True, stat='density', label='Control', bins=20, palette='viridis', ax=axs[i])\n        sns.histplot(normal_data_treated, kde=True, stat='density', label='Treated', bins=20, palette='viridis', ax=axs[i])\n\n        # Set labels and title\n        axs[i].set_xlabel(f'{variable.capitalize()}')\n        axs[i].set_ylabel('Density')\n        axs[i].set_title(f'Distribution of {variable.capitalize()}, by Treatment')\n\n        # Plot mean and error bars\n        axs[i].axvline(mean_control, color=sns.color_palette()[0], linestyle='--', label='Mean Control')\n        axs[i].axvline(mean_treated, color=sns.color_palette()[1], linestyle='--', label='Mean Treated')\n        axs[i].errorbar(mean_control, 0.02, xerr=std_error_control, fmt='o', color=sns.color_palette()[0], label='95% CI Control')\n        axs[i].errorbar(mean_treated, 0.02, xerr=std_error_treated, fmt='o', color=sns.color_palette()[1], label='95% CI Treated')\n\n        # Add legend\n        #axs[i].legend(ncol=3, bbox_to_anchor=(0.5, -0.15), loc='upper center') \n\n    #\n    handles, labels = axs[i].get_legend_handles_labels()\n    fig.legend(handles, labels, loc='upper center', ncol=3, bbox_to_anchor=(0.5, 0.0))\n    \n    # Adjust layout\n    plt.tight_layout()\n    plt.show()\n\n\n# Example usage:\nplot_variable_comparison(df)\n\n\n\n\n\n\n\n\nFigure 4: Outcome Distribution, by AB Treatment\n\n\n\n\n\n\n\nCode\nimport scipy.stats as stats\n\ndf_email_A = df[df['group'] == 'email_A']\ndf_email_B = df[df['group'] == 'email_B']\n        \n# Calculate mean and standard error for email_A and email_B\nmean_open_A = df_email_A['purch'].mean()\nstd_error_A = df_email_A['purch'].std() / np.sqrt(len(df_email_A))\n\nmean_open_B = df_email_B['purch'].mean()\nstd_error_B = df_email_B['purch'].std() / np.sqrt(len(df_email_B))\n    \n# Calculate the confidence interval using the t-distribution\nconfidence_interval_A = stats.norm.interval(0.95, loc=mean_open_A, scale=std_error_A)\nconfidence_interval_B = stats.norm.interval(0.95, loc=mean_open_B, scale=std_error_B)\n\n\n\n\nMean Response email_A: 25.6\nMean Response email_B: 25.9\n95% Confidence Interval for email_A: [25.06, 26.19]\n95% Confidence Interval for email_B: [25.29, 26.44]\n\n\nInterestingly enough, Email A appears to outperform Email B in terms of opens and clicks (Figure 4). However, this level of engagement does not necessarily translate to higher purchase rates. Purchases from Email B are higher on average, but the large overlap in their distribution is telling us that this difference might not be significant. While purchases from Email B are higher on average, the large overlap in their distribution suggests that this difference might not be significant.\nTo confirm this conclusion, we can conduct a linear regression comparing Email A and Email B purchases:\n\n\nCode\ndf_ab = df[df['group'] != 'ctrl']\nsmf.ols('purch ~ group', data=df_ab).fit().summary().tables[1]\n\n\n\n\nTable 8: A/B Regression Results III\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n25.6228\n0.291\n88.070\n0.000\n25.053\n26.193\n\n\ngroup[T.email_B]\n0.2435\n0.411\n0.592\n0.554\n-0.563\n1.050\n\n\n\n\n\n\n\n\nIndeed, despite observing higher purchases associated with Email B, the obtained p-value of 0.554 indicates that this difference is not statistically significant.\nThese findings show how important it is to look at different metrics and really dig into the data to figure out what’s working best for engaging customers and boosting sales."
  },
  {
    "objectID": "notebooks/diff_diff_plotting_exploration.html",
    "href": "notebooks/diff_diff_plotting_exploration.html",
    "title": "diff-diff Plotting Functions: Current State & Improvement Roadmap",
    "section": "",
    "text": "Goal: Explore the existing plotting capabilities of diff-diff and identify gaps relative to R’s ggfixest. The end goal is to contribute Python plotting functions that are as polished and flexible as ggfixest."
  },
  {
    "objectID": "notebooks/diff_diff_plotting_exploration.html#table-of-contents",
    "href": "notebooks/diff_diff_plotting_exploration.html#table-of-contents",
    "title": "diff-diff Plotting Functions: Current State & Improvement Roadmap",
    "section": "Table of Contents",
    "text": "Table of Contents\n\nSetup & Data\nExisting diff-diff Plot Functions\nWhat ggfixest Does Better\nGap Analysis\nPrototyping Improvements"
  },
  {
    "objectID": "notebooks/diff_diff_plotting_exploration.html#setup-data",
    "href": "notebooks/diff_diff_plotting_exploration.html#setup-data",
    "title": "diff-diff Plotting Functions: Current State & Improvement Roadmap",
    "section": "1. Setup & Data",
    "text": "1. Setup & Data\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.ticker as mticker\n\nfrom diff_diff import (\n    DifferenceInDifferences,\n    TwoWayFixedEffects,\n    MultiPeriodDiD,\n    CallawaySantAnna,\n    SyntheticDiD,\n    ImputationDiD,\n    SunAbraham,\n    generate_did_data,\n    load_mpdta,\n    plot_event_study,\n    plot_group_effects,\n)\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nprint('diff-diff imported successfully')\n\nc:\\Users\\danny\\anaconda3\\Lib\\site-packages\\pandas\\core\\computation\\expressions.py:22: UserWarning: Pandas requires version '2.10.2' or newer of 'numexpr' (version '2.8.7' currently installed).\n  from pandas.core.computation.check import NUMEXPR_INSTALLED\nc:\\Users\\danny\\anaconda3\\Lib\\site-packages\\pandas\\core\\arrays\\masked.py:56: UserWarning: Pandas requires version '1.4.2' or newer of 'bottleneck' (version '1.3.7' currently installed).\n  from pandas.core import (\n\n\ndiff-diff imported successfully\n\n\n\n1.1. Generate Panel Data\nWe use diff-diff’s built-in data generators plus the mpdta staggered adoption dataset.\n\n# Simple pre/post DiD data\ndata_simple = generate_did_data(\n    n_units=200, n_periods=10, treatment_effect=5.0,\n    treatment_fraction=0.5, treatment_period=5, seed=42\n)\nprint(f\"Simple DiD data: {data_simple.shape}\")\nprint(data_simple.head())\n\nSimple DiD data: (2000, 6)\n   unit  period  treated  post    outcome  true_effect\n0     0       0        1     0  10.947009          0.0\n1     0       1        1     0  12.516916          0.0\n2     0       2        1     0  11.700019          0.0\n3     0       3        1     0  12.753373          0.0\n4     0       4        1     0  10.559262          0.0\n\n\n\n# Staggered adoption data (mpdta - for Callaway-Sant'Anna demos)\ntry:\n    mpdta = load_mpdta()\n    print(f\"MPDTA data: {mpdta.shape}\")\n    print(mpdta.head())\n    print(f\"\\nTreatment cohorts: {sorted(mpdta['first_treat'].unique())}\")\nexcept Exception as e:\n    print(f\"Could not load mpdta: {e}\")\n    # Fallback: generate staggered data manually\n    rng = np.random.default_rng(42)\n    units = []\n    for i in range(50):\n        cohort = rng.choice([0, 5, 7, 9])  # 0 = never treated\n        for t in range(1, 11):\n            treated = 1 if cohort &gt; 0 and t &gt;= cohort else 0\n            y = 2 + 0.3 * t + rng.uniform(0, 3) + treated * 3.0 + rng.normal(0, 1)\n            units.append({'unit': i, 'period': t, 'y': y, 'first_treat': cohort if cohort &gt; 0 else np.inf})\n    mpdta = pd.DataFrame(units)\n    print(f\"Generated staggered data: {mpdta.shape}\")\n\nMPDTA data: (2500, 7)\n   countyreal  year     lpop    lemp  first_treat  treat  cohort\n0           1  2003  11.3683  9.5457         2006      1    2006\n1           1  2004  11.3119  9.5772         2006      1    2006\n2           1  2005  11.3795  9.5767         2006      1    2006\n3           1  2006  11.3683  9.5670         2006      1    2006\n4           1  2007  11.3053  9.6027         2006      1    2006\n\nTreatment cohorts: [0, 2004, 2006, 2007]"
  },
  {
    "objectID": "notebooks/diff_diff_plotting_exploration.html#existing-diff-diff-plot-functions",
    "href": "notebooks/diff_diff_plotting_exploration.html#existing-diff-diff-plot-functions",
    "title": "diff-diff Plotting Functions: Current State & Improvement Roadmap",
    "section": "2. Existing diff-diff Plot Functions",
    "text": "2. Existing diff-diff Plot Functions\nLet’s test each plotting function that diff-diff provides.\n\n2.1. plot_event_study()\nThe main event study visualization. Works with MultiPeriodDiD, CallawaySantAnna, SunAbraham, ImputationDiD results, or raw DataFrames.\n\n# Multi-period DiD event study\nmp_did = MultiPeriodDiD(alpha=0.05)\nmp_results = mp_did.fit(\n    data_simple,\n    outcome='outcome',\n    treatment='treated',\n    time='period',\n    post_periods=list(range(5, 10)),\n)\nprint(\"MultiPeriodDiD results:\")\nprint(f\"  avg ATT: {mp_results.avg_att:.3f}\")\nprint(f\"  avg SE:  {mp_results.avg_se:.3f}\")\nprint(f\"  Period effects: {mp_results.period_effects}\")\n\nMultiPeriodDiD results:\n  avg ATT: 4.977\n  avg SE:  0.292\n  Period effects: {0: PeriodEffect(period=0, effect=0.0532, SE=0.3949, p=0.8929), 1: PeriodEffect(period=1, effect=0.1473, SE=0.3855, p=0.7025), 2: PeriodEffect(period=2, effect=0.2097, SE=0.3903, p=0.5911), 3: PeriodEffect(period=3, effect=-0.0177, SE=0.3999, p=0.9647), 5: PeriodEffect(period=5, effect=5.0691***, SE=0.3787, p=0.0000), 6: PeriodEffect(period=6, effect=5.0104***, SE=0.3990, p=0.0000), 7: PeriodEffect(period=7, effect=5.1575***, SE=0.3952, p=0.0000), 8: PeriodEffect(period=8, effect=4.8875***, SE=0.3774, p=0.0000), 9: PeriodEffect(period=9, effect=4.7609***, SE=0.3976, p=0.0000)}\n\n\n\n# Default plot_event_study\nplot_event_study(mp_results, title='plot_event_study() — Default')\n\n\n\n\n\n\n\n\n\n# Customized plot_event_study\nplot_event_study(\n    mp_results,\n    title='plot_event_study() — Customized',\n    color='#440154',\n    marker='s',\n    markersize=10,\n    linewidth=2,\n    figsize=(12, 6),\n    shade_color='#e8e0f0',\n)\n\n\n\n\n\n\n\n\n\n\n2.2. plot_event_study() with Callaway-Sant’Anna\nThe same function also works with staggered DiD estimators.\n\n# Callaway-Sant'Anna\ntry:\n    cs = CallawaySantAnna(control_group='never_treated', alpha=0.05)\n    cs_results = cs.fit(\n        mpdta,\n        outcome='lemp' if 'lemp' in mpdta.columns else 'y',\n        unit='countyreal' if 'countyreal' in mpdta.columns else 'unit',\n        time='year' if 'year' in mpdta.columns else 'period',\n        first_treat='first_treat',\n        aggregate='event_study',\n    )\n    print(f\"CS Overall ATT: {cs_results.overall_att:.4f}\")\n    print(f\"Event study effects: {cs_results.event_study_effects[:3]}...\")\n    plot_event_study(cs_results, title='Callaway-Sant\\'Anna Event Study')\nexcept Exception as e:\n    print(f\"CS estimation failed: {e}\")\n\nCS Overall ATT: -0.0214\nCS estimation failed: unhashable type: 'slice'\n\n\n\n\n2.3. plot_group_effects()\nShows treatment effects separately by cohort (group). Only works with CallawaySantAnnaResults.\n\ntry:\n    plot_group_effects(cs_results, title='Treatment Effects by Cohort')\nexcept Exception as e:\n    print(f\"plot_group_effects failed: {e}\")\n\n\n\n\n\n\n\n\n\n\n2.4. plot_event_study() with Manual Data\nCan also pass a plain DataFrame or dictionaries directly.\n\n# Manual event study data\nes_df = pd.DataFrame({\n    'period': [-4, -3, -2, -1, 0, 1, 2, 3, 4],\n    'effect': [0.2, -0.1, 0.05, 0.0, 0.0, 2.5, 3.1, 3.8, 4.2],\n    'se':     [0.3,  0.25, 0.2, 0.15, 0.0, 0.4, 0.45, 0.5, 0.55],\n})\n\nplot_event_study(\n    es_df,\n    reference_period=0,\n    title='Manual Event Study Data',\n)"
  },
  {
    "objectID": "notebooks/diff_diff_plotting_exploration.html#what-ggfixest-does-better",
    "href": "notebooks/diff_diff_plotting_exploration.html#what-ggfixest-does-better",
    "title": "diff-diff Plotting Functions: Current State & Improvement Roadmap",
    "section": "3. What ggfixest Does Better",
    "text": "3. What ggfixest Does Better\nR’s ggfixest by Grant McDermott provides several features that diff-diff currently lacks. Here’s a comparison:\n\n3.1. ggfixest Key Functions\n\n\n\n\n\n\n\n\nggfixest\ndiff-diff Equivalent\nStatus\n\n\n\n\nggiplot()\nplot_event_study()\nPartial — missing ribbon, multi-CI, aggregate effects\n\n\nggcoefplot()\nNone\nMissing — no general coefficient plot\n\n\naggr_es()\nNone\nMissing — no aggregation + overlay utility\n\n\niplot_data()\nNone\nMissing — no tidy data extraction layer\n\n\ncoefplot_data()\nNone\nMissing — no tidy data extraction layer\n\n\n\n\n\n3.2. Feature Gap Analysis\n\n\n\n\n\n\n\n\n\nFeature\nggfixest\ndiff-diff\nPriority\n\n\n\n\nRibbon/shaded CIs\ngeom_style='ribbon'\nNot available\nHIGH\n\n\nMultiple CI levels\nci_level=c(.8, .95)\nNot available\nHIGH\n\n\nMulti-model comparison\ndodge or facet\nNot available\nHIGH\n\n\nAggregate effects overlay\naggr_eff='post'\nNot available\nMEDIUM\n\n\nTidy data extraction\niplot_data()\nNot available\nHIGH\n\n\nggplot2-like composability\nFull + chaining\nRaw matplotlib\nMEDIUM\n\n\nDictionary relabeling\ndict parameter\nNot available\nLOW\n\n\nReference period styling\nHollow marker + line\nHollow marker only\nLOW\n\n\nCoefficient grouping\ngroup parameter\nNot available\nLOW"
  },
  {
    "objectID": "notebooks/diff_diff_plotting_exploration.html#gap-analysis-side-by-side-examples",
    "href": "notebooks/diff_diff_plotting_exploration.html#gap-analysis-side-by-side-examples",
    "title": "diff-diff Plotting Functions: Current State & Improvement Roadmap",
    "section": "4. Gap Analysis: Side-by-Side Examples",
    "text": "4. Gap Analysis: Side-by-Side Examples\nLet’s visualize what’s currently possible vs. what we want to achieve.\n\n4.1. Gap: Ribbon CIs (Shaded Confidence Bands)\nggfixest offers geom_style = 'ribbon' for smooth shaded confidence bands. diff-diff only supports point + errorbar.\n\n# What diff-diff currently produces (errorbar only)\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Current: errorbar style\nperiods = es_df['period'].values\neffects = es_df['effect'].values\nse = es_df['se'].values\nci_low = effects - 1.96 * se\nci_high = effects + 1.96 * se\n\nax = axes[0]\nax.errorbar(periods, effects, yerr=1.96*se, fmt='o-', color='#2563eb',\n            capsize=4, markersize=8, linewidth=1.5)\nax.axhline(0, color='gray', linestyle='--', linewidth=1)\nax.axvline(-0.5, color='gray', linestyle=':', linewidth=1, alpha=0.5)\nax.set_title('Current: Errorbar Style', fontsize=13)\nax.set_xlabel('Period Relative to Treatment')\nax.set_ylabel('Treatment Effect')\nax.grid(True, alpha=0.2)\n\n# Target: ribbon style (like ggfixest)\nax = axes[1]\nax.plot(periods, effects, 'o-', color='#2563eb', markersize=8, linewidth=2, zorder=3)\nax.fill_between(periods, ci_low, ci_high, alpha=0.2, color='#2563eb', label='95% CI')\nax.axhline(0, color='gray', linestyle='--', linewidth=1)\nax.axvline(-0.5, color='gray', linestyle=':', linewidth=1, alpha=0.5)\nax.set_title('Target: Ribbon Style (like ggfixest)', fontsize=13)\nax.set_xlabel('Period Relative to Treatment')\nax.set_ylabel('Treatment Effect')\nax.legend(fontsize=10)\nax.grid(True, alpha=0.2)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n4.2. Gap: Multiple CI Levels\nggfixest can show nested CIs (e.g., 80% inner + 95% outer). diff-diff only supports a single level.\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Current: single CI level\nax = axes[0]\nax.errorbar(periods, effects, yerr=1.96*se, fmt='o-', color='#2563eb',\n            capsize=4, markersize=8, linewidth=1.5)\nax.axhline(0, color='gray', linestyle='--', linewidth=1)\nax.set_title('Current: Single CI (95%)', fontsize=13)\nax.set_xlabel('Period Relative to Treatment')\nax.set_ylabel('Treatment Effect')\nax.grid(True, alpha=0.2)\n\n# Target: nested CIs (80% + 95%)\nax = axes[1]\nci80_low = effects - 1.28 * se\nci80_high = effects + 1.28 * se\nci95_low = effects - 1.96 * se\nci95_high = effects + 1.96 * se\n\nax.fill_between(periods, ci95_low, ci95_high, alpha=0.15, color='#2563eb', label='95% CI')\nax.fill_between(periods, ci80_low, ci80_high, alpha=0.3, color='#2563eb', label='80% CI')\nax.plot(periods, effects, 'o-', color='#2563eb', markersize=8, linewidth=2, zorder=3)\nax.axhline(0, color='gray', linestyle='--', linewidth=1)\nax.set_title('Target: Nested CIs (80% + 95%)', fontsize=13)\nax.set_xlabel('Period Relative to Treatment')\nax.set_ylabel('Treatment Effect')\nax.legend(fontsize=10)\nax.grid(True, alpha=0.2)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n4.3. Gap: Multi-Model Comparison (Dodge & Facet)\nggfixest can plot multiple estimators side-by-side (multi_style='dodge') or in facets. This is critical for comparing TWFE vs. robust estimators.\n\n# Simulate results from 3 estimators\nnp.random.seed(42)\nperiods_rel = np.arange(-4, 5)\ntrue_effect = np.where(periods_rel &gt;= 0, 3.0 + 0.5 * periods_rel, 0)\n\nmodels = {\n    'TWFE':              true_effect + np.random.normal(0, 0.3, len(periods_rel)),\n    'Callaway-Sant\\'Anna': true_effect + np.random.normal(0, 0.25, len(periods_rel)),\n    'Sun-Abraham':       true_effect + np.random.normal(0, 0.35, len(periods_rel)),\n}\nmodel_se = {k: np.abs(np.random.normal(0.4, 0.1, len(periods_rel))) for k in models}\n\n# --- Dodge style ---\nfig, axes = plt.subplots(1, 2, figsize=(16, 5.5))\n\nax = axes[0]\ncolors = ['#440154', '#35b779', '#fde725']\nn_models = len(models)\nwidth = 0.25\n\nfor j, (name, eff) in enumerate(models.items()):\n    offset = (j - (n_models - 1) / 2) * width\n    se_vals = model_se[name]\n    ax.errorbar(periods_rel + offset, eff, yerr=1.96*se_vals,\n                fmt='o', color=colors[j], capsize=3, markersize=6,\n                linewidth=1.2, label=name)\n\nax.axhline(0, color='gray', linestyle='--', linewidth=1)\nax.axvline(-0.5, color='gray', linestyle=':', linewidth=1, alpha=0.5)\nax.set_title('Target: Multi-Model Dodge', fontsize=13)\nax.set_xlabel('Period Relative to Treatment')\nax.set_ylabel('Treatment Effect')\nax.legend(fontsize=9)\nax.grid(True, alpha=0.2)\n\n# --- Facet style ---\nax = axes[1]\n# Create a mini 1x3 facet within the right panel\naxes[1].remove()\ngs = fig.add_gridspec(1, 3, left=0.55, right=0.98, wspace=0.3, bottom=0.12, top=0.88)\n\nfor j, (name, eff) in enumerate(models.items()):\n    ax_f = fig.add_subplot(gs[0, j])\n    se_vals = model_se[name]\n    ax_f.fill_between(periods_rel, eff - 1.96*se_vals, eff + 1.96*se_vals,\n                      alpha=0.2, color=colors[j])\n    ax_f.plot(periods_rel, eff, 'o-', color=colors[j], markersize=5, linewidth=1.5)\n    ax_f.axhline(0, color='gray', linestyle='--', linewidth=0.8)\n    ax_f.axvline(-0.5, color='gray', linestyle=':', linewidth=0.8, alpha=0.5)\n    ax_f.set_title(name, fontsize=10)\n    if j == 0:\n        ax_f.set_ylabel('Effect', fontsize=9)\n    ax_f.set_xlabel('Period', fontsize=9)\n    ax_f.tick_params(labelsize=8)\n    ax_f.grid(True, alpha=0.2)\n\nfig.suptitle('Multi-Model Comparison: Dodge vs Facet', fontsize=14, y=0.98)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n4.4. Gap: Aggregate Effects Overlay\nggfixest’s aggr_eff = 'post' adds a shaded rectangle showing the mean post-treatment effect. Very useful for summarizing the overall ATT alongside the event study.\n\nfig, ax = plt.subplots(figsize=(10, 5.5))\n\neffects_sim = np.array([0.2, -0.1, 0.05, 0.0, 0.0, 2.5, 3.1, 3.8, 4.2])\nse_sim = np.array([0.3, 0.25, 0.2, 0.15, 0.0, 0.4, 0.45, 0.5, 0.55])\n\n# Event study\nax.fill_between(periods, effects_sim - 1.96*se_sim, effects_sim + 1.96*se_sim,\n                alpha=0.2, color='#2563eb')\nax.plot(periods, effects_sim, 'o-', color='#2563eb', markersize=8, linewidth=2, zorder=3)\n\n# Aggregate post-treatment effect (mean of post periods)\npost_mask = periods &gt;= 0\npost_mean = effects_sim[post_mask].mean()\npost_se = np.sqrt(np.mean(se_sim[post_mask]**2))  # simplified\n\nax.axhspan(post_mean - 1.96*post_se, post_mean + 1.96*post_se,\n           xmin=0.5, xmax=1.0, alpha=0.1, color='#dc2626')\nax.axhline(post_mean, xmin=0.5, color='#dc2626', linestyle='-', linewidth=2,\n           alpha=0.7, label=f'Mean Post ATT = {post_mean:.2f}')\n\nax.axhline(0, color='gray', linestyle='--', linewidth=1)\nax.axvline(-0.5, color='gray', linestyle=':', linewidth=1, alpha=0.5)\nax.set_title('Target: Event Study + Aggregate Effect Overlay', fontsize=13)\nax.set_xlabel('Period Relative to Treatment')\nax.set_ylabel('Treatment Effect')\nax.legend(fontsize=10)\nax.grid(True, alpha=0.2)\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "notebooks/diff_diff_plotting_exploration.html#prototyping-improvements",
    "href": "notebooks/diff_diff_plotting_exploration.html#prototyping-improvements",
    "title": "diff-diff Plotting Functions: Current State & Improvement Roadmap",
    "section": "5. Prototyping Improvements",
    "text": "5. Prototyping Improvements\nBelow we prototype the key missing features as standalone functions. These could be contributed to diff-diff.\n\n5.1. iplot_data() — Tidy Data Extraction\nThe foundation layer. Extracts a standardized DataFrame from any diff-diff result object, ready for custom plotting. This mirrors ggfixest’s iplot_data().\n\nfrom scipy import stats as scipy_stats\n\ndef iplot_data(results, ci_level=0.95, reference_period=None):\n    \"\"\"\n    Extract tidy event study data from diff-diff results.\n    \n    Returns a DataFrame with columns:\n      period, estimate, se, ci_low, ci_high, ci_level, is_ref\n    \"\"\"\n    z = scipy_stats.norm.ppf(1 - (1 - ci_level) / 2)\n    \n    # Handle DataFrame input\n    if isinstance(results, pd.DataFrame):\n        df = results.copy()\n        if 'period' not in df.columns:\n            raise ValueError(\"DataFrame must have 'period' column\")\n        if 'effect' in df.columns:\n            df = df.rename(columns={'effect': 'estimate'})\n        df['ci_low'] = df['estimate'] - z * df['se']\n        df['ci_high'] = df['estimate'] + z * df['se']\n        df['ci_level'] = ci_level\n        df['is_ref'] = df['period'] == reference_period if reference_period is not None else False\n        return df[['period', 'estimate', 'se', 'ci_low', 'ci_high', 'ci_level', 'is_ref']]\n    \n    # Handle dict input\n    if isinstance(results, dict) and 'effects' in results:\n        periods = sorted(results['effects'].keys())\n        rows = []\n        for p in periods:\n            est = results['effects'][p]\n            se_val = results.get('se', {}).get(p, 0)\n            rows.append({\n                'period': p, 'estimate': est, 'se': se_val,\n                'ci_low': est - z * se_val, 'ci_high': est + z * se_val,\n                'ci_level': ci_level,\n                'is_ref': p == reference_period if reference_period is not None else False,\n            })\n        return pd.DataFrame(rows)\n    \n    # Handle diff-diff result objects\n    # Try extracting event_study_effects (CallawaySantAnna, etc.)\n    if hasattr(results, 'event_study_effects'):\n        es = results.event_study_effects\n        rows = []\n        for entry in es:\n            p = entry.get('event_time', entry.get('period', 0))\n            est = entry.get('att', entry.get('estimate', 0))\n            se_val = entry.get('se', 0)\n            rows.append({\n                'period': p, 'estimate': est, 'se': se_val,\n                'ci_low': est - z * se_val, 'ci_high': est + z * se_val,\n                'ci_level': ci_level,\n                'is_ref': p == reference_period if reference_period is not None else False,\n            })\n        return pd.DataFrame(rows).sort_values('period').reset_index(drop=True)\n    \n    raise TypeError(f\"Unsupported result type: {type(results)}\")\n\n\n# Demo\ntidy = iplot_data(es_df, ci_level=0.95, reference_period=0)\nprint(\"Tidy event study data:\")\ntidy\n\nTidy event study data:\n\n\n\n\n\n\n\n\n\nperiod\nestimate\nse\nci_low\nci_high\nci_level\nis_ref\n\n\n\n\n0\n-4\n0.20\n0.30\n-0.387989\n0.787989\n0.95\nFalse\n\n\n1\n-3\n-0.10\n0.25\n-0.589991\n0.389991\n0.95\nFalse\n\n\n2\n-2\n0.05\n0.20\n-0.341993\n0.441993\n0.95\nFalse\n\n\n3\n-1\n0.00\n0.15\n-0.293995\n0.293995\n0.95\nFalse\n\n\n4\n0\n0.00\n0.00\n0.000000\n0.000000\n0.95\nTrue\n\n\n5\n1\n2.50\n0.40\n1.716014\n3.283986\n0.95\nFalse\n\n\n6\n2\n3.10\n0.45\n2.218016\n3.981984\n0.95\nFalse\n\n\n7\n3\n3.80\n0.50\n2.820018\n4.779982\n0.95\nFalse\n\n\n8\n4\n4.20\n0.55\n3.122020\n5.277980\n0.95\nFalse\n\n\n\n\n\n\n\n\n\n5.2. ggiplot() — Enhanced Event Study Plot\nThe main function, inspired by ggfixest::ggiplot(). Supports ribbon CIs, multiple CI levels, multi-model comparison, and aggregate effects overlay.\n\ndef ggiplot(\n    results,\n    *,\n    geom_style='pointrange',     # 'pointrange', 'errorbar', 'ribbon'\n    ci_level=0.95,               # float or list of floats for nested CIs\n    reference_period=None,\n    aggr_eff=None,               # 'post', 'pre', 'both'\n    # Multi-model: pass dict {name: results}\n    multi_style='dodge',         # 'dodge' or 'facet'\n    # Aesthetics\n    colors=None,\n    figsize=(10, 6),\n    title='Event Study',\n    xlabel='Period Relative to Treatment',\n    ylabel='Treatment Effect',\n    show_zero=True,\n    show_ref_line=True,\n    shade_pre=False,\n    ax=None,\n    show=True,\n):\n    \"\"\"\n    Enhanced event study plot inspired by ggfixest::ggiplot().\n    \n    Parameters\n    ----------\n    results : DataFrame, dict of DataFrames, or diff-diff result object\n        If dict, keys are model names and values are result objects (multi-model).\n    geom_style : str\n        'pointrange' (default), 'errorbar', or 'ribbon'\n    ci_level : float or list of float\n        Confidence level(s). Pass [0.80, 0.95] for nested CIs.\n    aggr_eff : str or None\n        'post' to overlay mean post-treatment effect, 'pre' for pre, 'both' for both.\n    multi_style : str\n        'dodge' (side-by-side) or 'facet' (separate panels).\n    \"\"\"\n    default_colors = ['#2563eb', '#dc2626', '#16a34a', '#ea580c', '#8b5cf6']\n    if colors is None:\n        colors = default_colors\n    \n    # Normalize ci_level to list\n    if isinstance(ci_level, (int, float)):\n        ci_levels = [ci_level]\n    else:\n        ci_levels = sorted(ci_level, reverse=True)  # widest first\n    \n    # Handle multi-model dict input\n    is_multi = isinstance(results, dict) and not ('effects' in results)\n    \n    if is_multi and multi_style == 'facet':\n        n_models = len(results)\n        fig, axes_arr = plt.subplots(1, n_models, figsize=(figsize[0], figsize[1]), sharey=True)\n        if n_models == 1:\n            axes_arr = [axes_arr]\n        \n        for idx, (name, res) in enumerate(results.items()):\n            ggiplot(\n                res, geom_style=geom_style, ci_level=ci_level,\n                reference_period=reference_period, aggr_eff=aggr_eff,\n                colors=[colors[idx % len(colors)]],\n                title=name, xlabel=xlabel,\n                ylabel=ylabel if idx == 0 else '',\n                show_zero=show_zero, show_ref_line=show_ref_line,\n                ax=axes_arr[idx], show=False,\n            )\n        \n        fig.suptitle(title, fontsize=14, y=1.02)\n        plt.tight_layout()\n        if show:\n            plt.show()\n        return axes_arr\n    \n    # Single model or dodge\n    if ax is None:\n        fig, ax = plt.subplots(figsize=figsize)\n    \n    if is_multi:\n        # Dodge: plot all models on same axis with offset\n        model_names = list(results.keys())\n        n_models = len(model_names)\n        dodge_width = 0.2\n        \n        for idx, (name, res) in enumerate(results.items()):\n            tidy = iplot_data(res, ci_level=ci_levels[-1], reference_period=reference_period)\n            offset = (idx - (n_models - 1) / 2) * dodge_width\n            color = colors[idx % len(colors)]\n            x = tidy['period'].values + offset\n            \n            if geom_style == 'ribbon':\n                for cl in ci_levels:\n                    td = iplot_data(res, ci_level=cl, reference_period=reference_period)\n                    alpha_fill = 0.15 + 0.15 * (1 - ci_levels.index(cl) / max(1, len(ci_levels) - 1))\n                    ax.fill_between(td['period'].values + offset,\n                                    td['ci_low'], td['ci_high'],\n                                    alpha=alpha_fill, color=color)\n                ax.plot(x, tidy['estimate'], 'o-', color=color, markersize=6,\n                        linewidth=1.5, label=name, zorder=3)\n            elif geom_style == 'errorbar':\n                ax.errorbar(x, tidy['estimate'],\n                            yerr=[tidy['estimate'] - tidy['ci_low'],\n                                  tidy['ci_high'] - tidy['estimate']],\n                            fmt='o', color=color, capsize=3, markersize=6,\n                            linewidth=1.2, label=name)\n            else:  # pointrange\n                ax.errorbar(x, tidy['estimate'],\n                            yerr=[tidy['estimate'] - tidy['ci_low'],\n                                  tidy['ci_high'] - tidy['estimate']],\n                            fmt='o', color=color, capsize=0, markersize=6,\n                            linewidth=2, label=name)\n        \n        ax.legend(fontsize=10)\n    \n    else:\n        # Single model\n        color = colors[0]\n        \n        # Get tidy data for the widest CI level\n        tidy = iplot_data(results, ci_level=ci_levels[-1], reference_period=reference_period)\n        x = tidy['period'].values\n        \n        if geom_style == 'ribbon':\n            for cl in ci_levels:\n                td = iplot_data(results, ci_level=cl, reference_period=reference_period)\n                alpha_fill = 0.15 + 0.15 * (1 - ci_levels.index(cl) / max(1, len(ci_levels) - 1))\n                label = f'{cl:.0%} CI' if len(ci_levels) &gt; 1 else f'{cl:.0%} CI'\n                ax.fill_between(x, td['ci_low'], td['ci_high'],\n                                alpha=alpha_fill, color=color, label=label)\n            ax.plot(x, tidy['estimate'], 'o-', color=color, markersize=8,\n                    linewidth=2, zorder=3)\n        \n        elif geom_style == 'errorbar':\n            for cl in ci_levels:\n                td = iplot_data(results, ci_level=cl, reference_period=reference_period)\n                lw = 1.5 if cl == ci_levels[-1] else 2.5\n                cs = 4 if cl == ci_levels[-1] else 0\n                label = f'{cl:.0%} CI'\n                ax.errorbar(x, td['estimate'],\n                            yerr=[td['estimate'] - td['ci_low'],\n                                  td['ci_high'] - td['estimate']],\n                            fmt='o' if cl == ci_levels[-1] else 'none',\n                            color=color, capsize=cs, markersize=8,\n                            linewidth=lw, label=label, zorder=2 + ci_levels.index(cl))\n        \n        else:  # pointrange\n            for cl in ci_levels:\n                td = iplot_data(results, ci_level=cl, reference_period=reference_period)\n                lw = 1.5 if cl == ci_levels[-1] else 3\n                ax.errorbar(x, td['estimate'],\n                            yerr=[td['estimate'] - td['ci_low'],\n                                  td['ci_high'] - td['estimate']],\n                            fmt='o' if cl == ci_levels[-1] else 'none',\n                            color=color, capsize=0, markersize=8,\n                            linewidth=lw, zorder=2 + ci_levels.index(cl))\n        \n        # Reference period marker\n        if reference_period is not None:\n            ref_row = tidy[tidy['period'] == reference_period]\n            if len(ref_row) &gt; 0:\n                ax.plot(reference_period, ref_row['estimate'].values[0],\n                        'o', color='white', markersize=10, markeredgecolor=color,\n                        markeredgewidth=2, zorder=5)\n    \n    # Aggregate effect overlay\n    if aggr_eff is not None:\n        tidy_agg = iplot_data(results, ci_level=ci_levels[-1], reference_period=reference_period)\n        \n        if aggr_eff in ('post', 'both'):\n            post = tidy_agg[tidy_agg['period'] &gt; (reference_period or -0.5)]\n            if len(post) &gt; 0:\n                mean_eff = post['estimate'].mean()\n                mean_se = np.sqrt(np.mean(post['se']**2))\n                z_val = scipy_stats.norm.ppf(1 - (1 - ci_levels[-1]) / 2)\n                ax.axhspan(mean_eff - z_val*mean_se, mean_eff + z_val*mean_se,\n                           alpha=0.08, color='#dc2626')\n                ax.axhline(mean_eff, color='#dc2626', linewidth=2, alpha=0.6,\n                           label=f'Mean Post = {mean_eff:.2f}')\n        \n        if aggr_eff in ('pre', 'both'):\n            pre = tidy_agg[tidy_agg['period'] &lt; (reference_period or 0)]\n            if len(pre) &gt; 0:\n                mean_eff = pre['estimate'].mean()\n                ax.axhline(mean_eff, color='#6b7280', linewidth=1.5, linestyle=':',\n                           alpha=0.6, label=f'Mean Pre = {mean_eff:.2f}')\n        \n        ax.legend(fontsize=10)\n    \n    # Reference lines\n    if show_zero:\n        ax.axhline(0, color='gray', linestyle='--', linewidth=1, alpha=0.5)\n    if show_ref_line and reference_period is not None:\n        ax.axvline(reference_period + 0.5, color='gray', linestyle=':', linewidth=1, alpha=0.5)\n    \n    ax.set_title(title, fontsize=13)\n    ax.set_xlabel(xlabel, fontsize=11)\n    ax.set_ylabel(ylabel, fontsize=11)\n    ax.grid(True, alpha=0.2)\n    \n    if show:\n        plt.tight_layout()\n        plt.show()\n    \n    return ax\n\nprint('ggiplot() defined')\n\nggiplot() defined\n\n\n\n\n5.3. Demo: All Geom Styles\n\nfig, axes = plt.subplots(1, 3, figsize=(16, 5), sharey=True)\n\nfor ax, style in zip(axes, ['pointrange', 'errorbar', 'ribbon']):\n    ggiplot(\n        es_df, geom_style=style, ci_level=0.95,\n        reference_period=0, title=f'geom_style = \\'{style}\\'',\n        ax=ax, show=False,\n    )\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n5.4. Demo: Nested CIs (80% + 95%)\n\nfig, axes = plt.subplots(1, 3, figsize=(16, 5), sharey=True)\n\nfor ax, style in zip(axes, ['pointrange', 'errorbar', 'ribbon']):\n    ggiplot(\n        es_df, geom_style=style, ci_level=[0.80, 0.95],\n        reference_period=0, title=f'{style} + nested CIs',\n        ax=ax, show=False,\n    )\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n5.5. Demo: Multi-Model Comparison\n\n# Create fake multi-model results as DataFrames\nnp.random.seed(42)\nbase = np.array([0.2, -0.1, 0.05, 0.0, 0.0, 2.5, 3.1, 3.8, 4.2])\n\nmodel_results = {\n    'TWFE': pd.DataFrame({\n        'period': np.arange(-4, 5),\n        'effect': base + np.random.normal(0, 0.2, 9),\n        'se': np.abs(np.random.normal(0.35, 0.05, 9)),\n    }),\n    'Sun-Abraham': pd.DataFrame({\n        'period': np.arange(-4, 5),\n        'effect': base + np.random.normal(0, 0.15, 9),\n        'se': np.abs(np.random.normal(0.3, 0.05, 9)),\n    }),\n    'Imputation': pd.DataFrame({\n        'period': np.arange(-4, 5),\n        'effect': base + np.random.normal(0, 0.1, 9),\n        'se': np.abs(np.random.normal(0.25, 0.05, 9)),\n    }),\n}\n\n# Dodge\nggiplot(\n    model_results, geom_style='errorbar', ci_level=0.95,\n    reference_period=0, multi_style='dodge',\n    title='Multi-Model: Dodge Style',\n)\n\n# Facet\nggiplot(\n    model_results, geom_style='ribbon', ci_level=0.95,\n    reference_period=0, multi_style='facet',\n    title='Multi-Model: Facet Style',\n    colors=['#2563eb', '#dc2626', '#16a34a'],\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\narray([&lt;Axes: title={'center': 'TWFE'}, xlabel='Period Relative to Treatment', ylabel='Treatment Effect'&gt;,\n       &lt;Axes: title={'center': 'Sun-Abraham'}, xlabel='Period Relative to Treatment'&gt;,\n       &lt;Axes: title={'center': 'Imputation'}, xlabel='Period Relative to Treatment'&gt;],\n      dtype=object)\n\n\n\n\n5.6. Demo: Aggregate Effect Overlay\n\nggiplot(\n    es_df, geom_style='ribbon', ci_level=[0.80, 0.95],\n    reference_period=0, aggr_eff='both',\n    title='Ribbon + Nested CIs + Aggregate Effects',\n    figsize=(11, 6),\n)"
  },
  {
    "objectID": "notebooks/diff_diff_plotting_exploration.html#contribution-roadmap",
    "href": "notebooks/diff_diff_plotting_exploration.html#contribution-roadmap",
    "title": "diff-diff Plotting Functions: Current State & Improvement Roadmap",
    "section": "6. Contribution Roadmap",
    "text": "6. Contribution Roadmap\n\nPhase 1: Core (High Priority)\n\niplot_data() — Tidy data extraction from all result types\nggiplot() — Enhanced event study plot with:\n\ngeom_style: pointrange / errorbar / ribbon\nci_level: single or nested CIs\nMulti-model comparison (dodge + facet)\nAggregate effects overlay (aggr_eff)\nReference period styling\n\n\n\n\nPhase 2: Extensions (Medium Priority)\n\nggcoefplot() — General coefficient plot (not just event study)\naggr_es() — Standalone aggregation utility (pre/post/both/diff)\nplotnine backend — Optional ggplot2-like composability via plotnine\n\n\n\nPhase 3: Polish (Lower Priority)\n\nDictionary relabeling support\nCoefficient grouping\nTheme presets (minimal, classic, publication)\nExport to SVG/PDF helpers\n\n\n\nDesign Decisions\n\nMatplotlib first: Keep matplotlib as the primary backend (already used in diff-diff). Optional plotnine layer on top.\nComposable: All functions return ax so users can chain modifications.\nSensible defaults: One-line call should produce a publication-quality plot.\nBackward compatible: Existing plot_event_study() continues to work."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Daniel Redel",
    "section": "",
    "text": "I am an Economics Consultant at Compass Lexecon, where I conduct econometric analyses in competition economics and regulatory matters. My work sits at the intersection of causal inference, machine learning, and applied economics. I hold an MSc in Econometrics from Tilburg University and an MSc in Applied Economics from PUC Chile.\nLearn more about me →\n \n  \n   \n  \n    \n     Twitter\n  \n  \n    \n     LinkedIn\n  \n  \n    \n     Github"
  },
  {
    "objectID": "index.html#latest-posts",
    "href": "index.html#latest-posts",
    "title": "Daniel Redel",
    "section": "Latest Posts",
    "text": "Latest Posts\nClick here to check out more posts.\n\n\n\n\n\n\n\n\n\n\nFive Ways to Improve Your Event Study Plots\n\n\n\n\n\n\nDaniel Redel\n\n\nFeb 18, 2026\n\n\n\n\n\n\n\n\n\n\n\n\n\nInterpreting Event Studies Across Modern DiD Methods\n\n\n\n\n\n\nDaniel Redel\n\n\nFeb 18, 2026\n\n\n\n\n\n\n\n\n\n\n\n\n\nSensitivity Analysis for Parallel Trends\n\n\n\n\n\n\nDaniel Redel\n\n\nFeb 18, 2026\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "I am an Economics Consultant at Compass Lexecon, specializing in econometric analyses for competition economics, mergers, and regulatory disputes. My expertise spans causal inference, machine learning, and applied economics, and I’m increasingly focused on applying data science methods to economic problems. I hold an MSc in Econometrics & Mathematical Economics from Tilburg University and an MSc in Applied Economics from PUC Chile.\n \n  \n   \n  \n    \n     Twitter\n  \n  \n    \n     LinkedIn\n  \n  \n    \n     Github"
  },
  {
    "objectID": "about.html#skills",
    "href": "about.html#skills",
    "title": "About Me",
    "section": "Skills",
    "text": "Skills"
  },
  {
    "objectID": "about.html#experience",
    "href": "about.html#experience",
    "title": "About Me",
    "section": "Experience",
    "text": "Experience\n\n\nEconomics Consultant\nCompass Lexecon, Berlin\n\n\nSep 2025 – present\n\n\n\n\nJunior Data Consultant\nFactX, Utrecht\n\n\nFeb 2024 – Aug 2025\n\n\n\n\nThesis Internship\nCPB Netherlands Bureau for Economic Policy Analysis, The Hague\n\n\nMay 2023 – Oct 2023\n\n\n\n\nResearch Analyst\nCentroCompetencia, Santiago\n\n\nJul 2021 – Apr 2023"
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "About Me",
    "section": "Education",
    "text": "Education\n\n\nMSc. in Econometrics & Mathematical Economics\nTilburg University\n\n\nJan 2024\n\n\n\n\nMSc. in Applied Economics\nPUC Chile\n\n\n2021\n\n\n\n\nBSc. in Economics & Business Administration\nPUC Chile\n\n\n2020"
  },
  {
    "objectID": "BLOG_PLAN.html",
    "href": "BLOG_PLAN.html",
    "title": "Blog Post Plan — DiD Series",
    "section": "",
    "text": "#\nPost\nFile\nStatus\n\n\n\n\n1\nInterpreting Event Studies Across Modern DiD Methods\nposts/event_study_comparison/event_study_comparison.ipynb\nDone\n\n\n2\nFive Ways to Improve Your Event Study Plots\nposts/event_study_best_practices/event_study_best_practices.ipynb\nDone\n\n\n3\nSensitivity Analysis for Parallel Trends\nposts/parallel_trends_sensitivity/parallel_trends_sensitivity.ipynb\nDone\n\n\n—\nSynthDiD post updated with TROP (Athey et al. 2025)\nposts/synthetic_did/synthetic_did.ipynb\nDone\n\n\n—\nProject entry for diff-diff contribution\nprojects.qmd\nDone\n\n\n\n\n\n\n\n\n\nCluster-robust, heteroskedasticity-robust, bootstrap, Wild Cluster Bootstrap\nHansen (2025) — “Standard Errors for Difference-in-Difference Regression”\nReference PDF in notebooks/J of Applied Econometrics - 2025 - Hansen...pdf\n\n\n\n\n\nCover images needed for all new posts:\n\nposts/event_study_comparison/ (referenced as event_study_cover.png)\nposts/event_study_best_practices/ (referenced as best_practices_cover.png)\nposts/parallel_trends_sensitivity/ (referenced as sensitivity_cover.png)\nassets/cover/diff-diff.png (project card)\n\n\n\n\n\n\n\nnotebooks/diff_diff_plotting_exploration.ipynb — ggfixest gap analysis, iplot_data(), ggiplot() prototypes\nnotebooks/honest_inference_exploration.ipynb — sup-t bands, smoothest path, functional SCBs, multi-model fairness\n\n\n\n\n\nBuilt at C:\\Users\\danny\\OneDrive\\Profesional\\diff-diff-contrib\nVisualization & honest inference extensions for the diff-diff package"
  },
  {
    "objectID": "BLOG_PLAN.html#completed",
    "href": "BLOG_PLAN.html#completed",
    "title": "Blog Post Plan — DiD Series",
    "section": "",
    "text": "#\nPost\nFile\nStatus\n\n\n\n\n1\nInterpreting Event Studies Across Modern DiD Methods\nposts/event_study_comparison/event_study_comparison.ipynb\nDone\n\n\n2\nFive Ways to Improve Your Event Study Plots\nposts/event_study_best_practices/event_study_best_practices.ipynb\nDone\n\n\n3\nSensitivity Analysis for Parallel Trends\nposts/parallel_trends_sensitivity/parallel_trends_sensitivity.ipynb\nDone\n\n\n—\nSynthDiD post updated with TROP (Athey et al. 2025)\nposts/synthetic_did/synthetic_did.ipynb\nDone\n\n\n—\nProject entry for diff-diff contribution\nprojects.qmd\nDone"
  },
  {
    "objectID": "BLOG_PLAN.html#remaining",
    "href": "BLOG_PLAN.html#remaining",
    "title": "Blog Post Plan — DiD Series",
    "section": "",
    "text": "Cluster-robust, heteroskedasticity-robust, bootstrap, Wild Cluster Bootstrap\nHansen (2025) — “Standard Errors for Difference-in-Difference Regression”\nReference PDF in notebooks/J of Applied Econometrics - 2025 - Hansen...pdf\n\n\n\n\n\nCover images needed for all new posts:\n\nposts/event_study_comparison/ (referenced as event_study_cover.png)\nposts/event_study_best_practices/ (referenced as best_practices_cover.png)\nposts/parallel_trends_sensitivity/ (referenced as sensitivity_cover.png)\nassets/cover/diff-diff.png (project card)"
  },
  {
    "objectID": "BLOG_PLAN.html#exploration-notebooks-source-material",
    "href": "BLOG_PLAN.html#exploration-notebooks-source-material",
    "title": "Blog Post Plan — DiD Series",
    "section": "",
    "text": "notebooks/diff_diff_plotting_exploration.ipynb — ggfixest gap analysis, iplot_data(), ggiplot() prototypes\nnotebooks/honest_inference_exploration.ipynb — sup-t bands, smoothest path, functional SCBs, multi-model fairness"
  },
  {
    "objectID": "BLOG_PLAN.html#python-package-separate-project",
    "href": "BLOG_PLAN.html#python-package-separate-project",
    "title": "Blog Post Plan — DiD Series",
    "section": "",
    "text": "Built at C:\\Users\\danny\\OneDrive\\Profesional\\diff-diff-contrib\nVisualization & honest inference extensions for the diff-diff package"
  },
  {
    "objectID": "myposts.html",
    "href": "myposts.html",
    "title": "Blog",
    "section": "",
    "text": "Order By\n      Default\n      \n        Date - Oldest\n      \n      \n        Date - Newest\n      \n      \n        Title\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nFive Ways to Improve Your Event Study Plots\n\n\n\nCausal Inference\n\nDifference-in-Differences\n\nEvent Studies\n\nPython\n\n\n\n\n\n\n\nDaniel Redel\n\n\nFeb 18, 2026\n\n\n\n\n\n\n\n\n\n\n\n\n\nInterpreting Event Studies Across Modern DiD Methods\n\n\n\nCausal Inference\n\nDifference-in-Differences\n\nEvent Studies\n\nPython\n\n\n\n\n\n\n\nDaniel Redel\n\n\nFeb 18, 2026\n\n\n\n\n\n\n\n\n\n\n\n\n\nSensitivity Analysis for Parallel Trends\n\n\n\nCausal Inference\n\nDifference-in-Differences\n\nEvent Studies\n\nPython\n\n\n\n\n\n\n\nDaniel Redel\n\n\nFeb 18, 2026\n\n\n\n\n\n\n\n\n\n\n\n\n\nDiD vs Synthetic Control vs Synthetic DiD\n\n\n\nCausal Inference\n\nDifference-in-Differences\n\nSynthetic Control\n\nPython\n\n\n\n\n\n\n\nDaniel Redel\n\n\nFeb 18, 2026\n\n\n\n\n\n\n\n\n\n\n\n\n\nCausal Forests: Heterogeneous Treatment Effects\n\n\n\nCausal Inference\n\nMachine Learning\n\nHeterogeneous Treatment Effects\n\nPython\n\n\n\n\n\n\n\nDaniel Redel\n\n\nFeb 17, 2026\n\n\n\n\n\n\n\n\n\n\n\n\n\nCausal ARIMA Approach to Estimate Price Policy Changes on Sales\n\n\n\nCausal Inference\n\nR\n\nTime Series\n\nARIMA\n\n\n\n\n\n\n\nDaniel Redel\n\n\nMay 22, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\nBayesian Approach to A/B Testing\n\n\n\nBayesian Statistics\n\nCausal Inference\n\nA/B Testing\n\nPython\n\n\n\n\n\n\n\nDaniel Redel\n\n\nMay 10, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\nMerger Simulation of Broadband Internet Services\n\n\n\nCompetition & Antitrust\n\nR\n\nBinary Choice Models\n\nSimulation\n\n\n\n\n\n\n\nDaniel Redel\n\n\nMay 9, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\nRetail Gasoline Merger in Chile: An Ex-Post Merger Evaluation\n\n\n\nCausal Inference\n\nCompetition & Antitrust\n\nDiff-in-Diff\n\nEconometrics\n\nSpatial Analysis\n\nGasoline Industry\n\n\n\n\n\n\n\nDaniel Redel\n\n\nApr 22, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\nA/B Testing Analysis\n\n\n\nCausal Inference\n\nA/B Testing\n\nPython\n\n\n\n\n\n\n\nDaniel Redel\n\n\nApr 5, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\nNonparametric & Semiparametric Binary-Choice Regressions\n\n\n\nNonparametric Regression\n\nSemiparametric Regression\n\nBinary Choice Models\n\nKernel Density Estimation\n\n\n\n\n\n\n\nDaniel Redel\n\n\nDec 11, 2022\n\n\n\n\n\n\nNo matching items\n\n  \n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "notebooks/honest_inference_exploration.html",
    "href": "notebooks/honest_inference_exploration.html",
    "title": "Honest Inference for Event Study Plots",
    "section": "",
    "text": "Goal: Assess and prototype honest inference features for diff-diff plotting, inspired by:\nKey papers: - Rambachan & Roth (2023) — “A More Credible Approach to Parallel Trends” - Freyaldenhoven, Hansen, Perez Perez, Shapiro (2021) — “Visualization, Identification, and Estimation in the Linear Panel Event-Study Design” (NBER w29170) - Roth (2026) — “Interpreting Event-Studies from Recent Difference-in-Differences Methods” - Fang & Liebl (2026) — “Functional Difference-in-Differences” (fdid)"
  },
  {
    "objectID": "notebooks/honest_inference_exploration.html#table-of-contents1.-setup-data2.-what-diff-diff-already-has-honestdid3.-gap-1-simultaneous-confidence-bands-eventstudyr4.-gap-2-smoothest-confounding-path-eventstudyr5.-gap-3-functional-scbs-fdid6.-insights-from-the-papers7.-compatibility-assessment8.-prototypes9.-multi-model-comparison-fairness10.-freyaldenhoven-et-al.-plot-features11.-comprehensive-roadmap",
    "href": "notebooks/honest_inference_exploration.html#table-of-contents1.-setup-data2.-what-diff-diff-already-has-honestdid3.-gap-1-simultaneous-confidence-bands-eventstudyr4.-gap-2-smoothest-confounding-path-eventstudyr5.-gap-3-functional-scbs-fdid6.-insights-from-the-papers7.-compatibility-assessment8.-prototypes9.-multi-model-comparison-fairness10.-freyaldenhoven-et-al.-plot-features11.-comprehensive-roadmap",
    "title": "Honest Inference for Event Study Plots",
    "section": "Table of Contents1. Setup & Data2. What diff-diff Already Has: HonestDiD3. Gap 1: Simultaneous Confidence Bands (eventstudyr)4. Gap 2: Smoothest Confounding Path (eventstudyr)5. Gap 3: Functional SCBs (fdid)6. Insights from the Papers7. Compatibility Assessment8. Prototypes9. Multi-Model Comparison Fairness10. Freyaldenhoven et al. Plot Features11. Comprehensive Roadmap",
    "text": "Table of Contents1. Setup & Data2. What diff-diff Already Has: HonestDiD3. Gap 1: Simultaneous Confidence Bands (eventstudyr)4. Gap 2: Smoothest Confounding Path (eventstudyr)5. Gap 3: Functional SCBs (fdid)6. Insights from the Papers7. Compatibility Assessment8. Prototypes9. Multi-Model Comparison Fairness10. Freyaldenhoven et al. Plot Features11. Comprehensive Roadmap"
  },
  {
    "objectID": "notebooks/honest_inference_exploration.html#setup-data",
    "href": "notebooks/honest_inference_exploration.html#setup-data",
    "title": "Honest Inference for Event Study Plots",
    "section": "1. Setup & Data",
    "text": "1. Setup & Data\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.ticker as mticker\nfrom scipy import stats as scipy_stats\nfrom scipy.optimize import minimize, linprog\n\nfrom diff_diff import (\n    MultiPeriodDiD,\n    CallawaySantAnna,\n    generate_did_data,\n    load_mpdta,\n    plot_event_study,\n)\n\n# HonestDiD from diff-diff\nfrom diff_diff import HonestDiD\nfrom diff_diff.honest_did import SensitivityResults, HonestDiDResults\nfrom diff_diff.visualization import plot_sensitivity, plot_honest_event_study\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nprint('All imports successful')\n\nc:\\Users\\danny\\anaconda3\\Lib\\site-packages\\pandas\\core\\computation\\expressions.py:22: UserWarning: Pandas requires version '2.10.2' or newer of 'numexpr' (version '2.8.7' currently installed).\n  from pandas.core.computation.check import NUMEXPR_INSTALLED\nc:\\Users\\danny\\anaconda3\\Lib\\site-packages\\pandas\\core\\arrays\\masked.py:56: UserWarning: Pandas requires version '1.4.2' or newer of 'bottleneck' (version '1.3.7' currently installed).\n  from pandas.core import (\n\n\nAll imports successful\n\n\n\n# Generate panel data\ndata = generate_did_data(\n    n_units=200, n_periods=10, treatment_effect=5.0,\n    treatment_fraction=0.5, treatment_period=5, seed=42\n)\n\n# Multi-period DiD\nmp_did = MultiPeriodDiD(alpha=0.05)\nmp_results = mp_did.fit(\n    data, outcome='outcome', treatment='treated',\n    time='period', post_periods=list(range(5, 10)),\n)\nprint(f'ATT = {mp_results.avg_att:.3f} (SE = {mp_results.avg_se:.3f})')\n\n# Also prepare staggered data\ntry:\n    mpdta = load_mpdta()\n    cs = CallawaySantAnna(control_group='never_treated', alpha=0.05)\n    cs_results = cs.fit(\n        mpdta, outcome='lemp', unit='countyreal',\n        time='year', first_treat='first_treat', aggregate='event_study',\n    )\n    print(f'CS Overall ATT = {cs_results.overall_att:.4f}')\nexcept Exception as e:\n    print(f'CS failed: {e}')\n    cs_results = None\n\n# Synthetic event study data for plotting demos\nnp.random.seed(42)\nperiods = np.arange(-5, 6)\ntrue_effect = np.where(periods &gt;= 0, 3.0 + 0.3 * periods, 0)\nnoise = np.random.normal(0, 0.3, len(periods))\nestimates = true_effect + noise\nse_vals = np.abs(np.random.normal(0.4, 0.08, len(periods)))\n# Set reference period to 0\nestimates[periods == -1] = 0.0\nse_vals[periods == -1] = 0.0\n\nes_df = pd.DataFrame({\n    'period': periods,\n    'estimate': estimates,\n    'se': se_vals,\n})\n\nprint(f'Synthetic ES data: {len(es_df)} periods')\nes_df\n\nATT = 4.977 (SE = 0.292)\nCS Overall ATT = -0.0214\nSynthetic ES data: 11 periods\n\n\n\n\n\n\n\n\n\nperiod\nestimate\nse\n\n\n\n\n0\n-5\n0.149014\n0.362742\n\n\n1\n-4\n-0.041479\n0.419357\n\n\n2\n-3\n0.194307\n0.246938\n\n\n3\n-2\n0.456909\n0.262007\n\n\n4\n-1\n0.000000\n0.000000\n\n\n5\n0\n2.929759\n0.318974\n\n\n6\n1\n3.773764\n0.425140\n\n\n7\n2\n3.830230\n0.327358\n\n\n8\n3\n3.759158\n0.287016\n\n\n9\n4\n4.362768\n0.517252\n\n\n10\n5\n4.360975\n0.381938"
  },
  {
    "objectID": "notebooks/honest_inference_exploration.html#what-diff-diff-already-has-honestdid",
    "href": "notebooks/honest_inference_exploration.html#what-diff-diff-already-has-honestdid",
    "title": "Honest Inference for Event Study Plots",
    "section": "2. What diff-diff Already Has: HonestDiD",
    "text": "2. What diff-diff Already Has: HonestDiD\ndiff-diff already implements the core of Rambachan & Roth (2023):\n\n\n\n\n\n\n\n\nFeature\nStatus\nFunction\n\n\n\n\nSmoothness restriction (\\(\\Delta^{SD}\\))\nAvailable\nHonestDiD(method='smoothness')\n\n\nRelative magnitude (\\(\\Delta^{RM}\\))\nAvailable\nHonestDiD(method='relative_magnitude')\n\n\nCombined (\\(\\Delta^{SDRM}\\))\nAvailable\nHonestDiD(method='combined')\n\n\nSensitivity analysis\nAvailable\nHonestDiD.sensitivity_analysis()\n\n\nBreakdown value\nAvailable\nHonestDiD.breakdown_value()\n\n\nSensitivity plot\nAvailable\nplot_sensitivity()\n\n\nHonest event study plot\nAvailable\nplot_honest_event_study()\n\n\n\nLet’s demo these existing functions.\n\n# HonestDiD with relative magnitude restriction\nhonest = HonestDiD(method='relative_magnitude', M=1.0, alpha=0.05)\nhonest_results = honest.fit(mp_results)\n\nprint('=== HonestDiD Results (Relative Magnitude, M=1.0) ===')\nhonest_results.print_summary()\n\n=== HonestDiD Results (Relative Magnitude, M=1.0) ===\n======================================================================\n               Honest DiD Sensitivity Analysis Results                \n                       (Rambachan & Roth 2023)                        \n======================================================================\n\nMethod:                        Relative Magnitudes (Delta^RM)\nRestriction parameter (M):     1.0000\nCI method:                     C-LF\n\n----------------------------------------------------------------------\n              Original Estimate (under parallel trends)               \n----------------------------------------------------------------------\nPoint estimate:                4.9771\nStandard error:                0.2918\n\n----------------------------------------------------------------------\n               Robust Results (allowing for violations)               \n----------------------------------------------------------------------\nIdentified set:                [4.7673, 5.1868]\n95% Robust CI:                 [4.1955, 5.7587]\n\nEffect robust to violations:   Yes\n\n----------------------------------------------------------------------\n                            Interpretation                            \n----------------------------------------------------------------------\nPost-treatment violations bounded at 1.0x max pre-period violation.\nEffect remains POSITIVE even with violations up to M=1.0.\n\n======================================================================\n\n\n\n# Sensitivity analysis\nsens_results = honest.sensitivity_analysis(mp_results)\nsens_results.print_summary()\n\n# Breakdown value\nbd = honest.breakdown_value(mp_results)\nprint(f'\\nBreakdown value: {bd}')\n\n======================================================================\n                   Honest DiD Sensitivity Analysis                    \n======================================================================\n\nMethod:                        relative_magnitude\nOriginal estimate:             4.9771\nOriginal SE:                   0.2918\nM values tested:               9\n\nBreakdown value:               None (always significant)\n\n----------------------------------------------------------------------\nM           Lower Bound  Upper Bound     CI Lower     CI Upper\n----------------------------------------------------------------------\n0.0000           4.9771       4.9771       4.4052       5.5489\n0.2500           4.9246       5.0295       4.3528       5.6014\n0.5000           4.8722       5.0820       4.3004       5.6538\n0.7500           4.8198       5.1344       4.2479       5.7062\n1.0000           4.7673       5.1868       4.1955       5.7587\n1.2500           4.7149       5.2393       4.1431       5.8111\n1.5000           4.6625       5.2917       4.0906       5.8635\n1.7500           4.6100       5.3441       4.0382       5.9160\n2.0000           4.5576       5.3966       3.9858       5.9684\n\n======================================================================\n\nBreakdown value: None\n\n\n\nfig, axes = plt.subplots(1, 2, figsize=(16, 5.5))\n\n# Left: Sensitivity plot\nplot_sensitivity(sens_results, ax=axes[0], show=False,\n                 title='Existing: plot_sensitivity()')\n\n# Right: Honest event study\nplot_honest_event_study(honest_results, ax=axes[1], show=False,\n                        title='Existing: plot_honest_event_study()')\n\nplt.tight_layout()\nplt.show()\n\n\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\nCell In[5], line 8\n      4 plot_sensitivity(sens_results, ax=axes[0], show=False,\n      5                  title='Existing: plot_sensitivity()')\n      7 # Right: Honest event study\n----&gt; 8 plot_honest_event_study(honest_results, ax=axes[1], show=False,\n      9                         title='Existing: plot_honest_event_study()')\n     11 plt.tight_layout()\n     12 plt.show()\n\nFile c:\\Users\\danny\\anaconda3\\Lib\\site-packages\\diff_diff\\visualization.py:858, in plot_honest_event_study(honest_results, periods, reference_period, figsize, title, xlabel, ylabel, original_color, honest_color, marker, markersize, capsize, ax, show)\n    853 # Plot honest CIs (thicker, foreground)\n    854 yerr_honest = [\n    855     [e - lower for e, lower in zip(effects, honest_ci_lower)],\n    856     [u - e for e, u in zip(effects, honest_ci_upper)],\n    857 ]\n--&gt; 858 ax.errorbar(\n    859     x_vals,\n    860     effects,\n    861     yerr=yerr_honest,\n    862     fmt=\"none\",\n    863     color=honest_color,\n    864     capsize=capsize,\n    865     linewidth=2,\n    866     label=f\"Honest CI (M={honest_results.M:.2f})\",\n    867 )\n    869 # Plot point estimates\n    870 for i, (x, effect, period) in enumerate(zip(x_vals, effects, periods)):\n\nFile c:\\Users\\danny\\anaconda3\\Lib\\site-packages\\matplotlib\\__init__.py:1465, in _preprocess_data.&lt;locals&gt;.inner(ax, data, *args, **kwargs)\n   1462 @functools.wraps(func)\n   1463 def inner(ax, *args, data=None, **kwargs):\n   1464     if data is None:\n-&gt; 1465         return func(ax, *map(sanitize_sequence, args), **kwargs)\n   1467     bound = new_sig.bind(ax, *args, **kwargs)\n   1468     auto_label = (bound.arguments.get(label_namer)\n   1469                   or bound.kwargs.get(label_namer))\n\nFile c:\\Users\\danny\\anaconda3\\Lib\\site-packages\\matplotlib\\axes\\_axes.py:3674, in Axes.errorbar(self, x, y, yerr, xerr, fmt, ecolor, elinewidth, capsize, barsabove, lolims, uplims, xlolims, xuplims, errorevery, capthick, **kwargs)\n   3671 res = np.zeros(err.shape, dtype=bool)  # Default in case of nan\n   3672 if np.any(np.less(err, -err, out=res, where=(err == err))):\n   3673     # like err&lt;0, but also works for timedelta and nan.\n-&gt; 3674     raise ValueError(\n   3675         f\"'{dep_axis}err' must not contain negative values\")\n   3676 # This is like\n   3677 #     elow, ehigh = np.broadcast_to(...)\n   3678 #     return dep - elow * ~lolims, dep + ehigh * ~uplims\n   3679 # except that broadcast_to would strip units.\n   3680 low, high = dep + np.vstack([-(1 - lolims), 1 - uplims]) * err\n\nValueError: 'yerr' must not contain negative values\n\n\n\n\n\n\n\n\n\n\nAssessment of existing HonestDiD support:\n\nThe core estimation is solid (smoothness, relative magnitude, combined restrictions)\nplot_sensitivity() shows how CIs widen as \\(\\bar{M}\\) increases — a separate diagnostic plot\nplot_honest_event_study() shows robust vs. standard CIs side-by-side on the event study\n\nWhat’s missing: 1. No simultaneous confidence bands (sup-t) — current CIs are pointwise 2. No smoothest confounding path overlay 3. No functional SCBs (equivalence/relevance testing) 4. No integration with the ggiplot() prototype from the plotting notebook 5. No way to overlay honest inference inside the main event study plot in a unified way"
  },
  {
    "objectID": "notebooks/honest_inference_exploration.html#gap-1-simultaneous-confidence-bands-eventstudyr",
    "href": "notebooks/honest_inference_exploration.html#gap-1-simultaneous-confidence-bands-eventstudyr",
    "title": "Honest Inference for Event Study Plots",
    "section": "3. Gap 1: Simultaneous Confidence Bands (eventstudyr)",
    "text": "3. Gap 1: Simultaneous Confidence Bands (eventstudyr)\n\nThe Problem with Pointwise CIs\nStandard event study plots show pointwise 95% CIs: each period’s CI has 95% coverage individually. But when viewing all periods together, the probability that at least one CI fails to cover the truth is much higher than 5%.\neventstudyr solves this with sup-t simultaneous bands (Montiel Olea & Plagborg-Moller, 2019):\n\\[\\text{Simultaneous band} = \\hat{\\beta}_t \\pm c_{\\alpha}^{\\text{sup-t}} \\cdot \\hat{\\sigma}_t\\]\nwhere \\(c_{\\alpha}^{\\text{sup-t}}\\) is the critical value from:\n\\[P\\left(\\max_t \\left|\\frac{\\hat{\\beta}_t - \\beta_t}{\\hat{\\sigma}_t}\\right| \\leq c_{\\alpha}^{\\text{sup-t}}\\right) = 1 - \\alpha\\]\nThis is computed via simulation from the multivariate normal distribution of the coefficient estimates using their variance-covariance matrix \\(\\hat{\\Sigma}\\).\nKey insight: sup-t bands are always wider than pointwise CIs. If the zero line is inside the sup-t band for all pre-treatment periods simultaneously, it provides much stronger evidence for parallel trends.\n\ndef compute_supt_critical_value(vcov, alpha=0.05, n_sim=10000, seed=42):\n    \"\"\"\n    Compute the sup-t critical value for simultaneous confidence bands.\n    \n    Following Montiel Olea & Plagborg-Moller (2019).\n    \n    Parameters\n    ----------\n    vcov : array-like (K x K)\n        Variance-covariance matrix of the K coefficients.\n    alpha : float\n        Significance level (default 0.05 for 95% bands).\n    n_sim : int\n        Number of simulations for computing the critical value.\n    \n    Returns\n    -------\n    c_supt : float\n        Critical value such that P(max|Z_k/sigma_k| &lt;= c_supt) = 1 - alpha.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    K = vcov.shape[0]\n    sigma = np.sqrt(np.diag(vcov))\n    \n    # Draw from MVN(0, vcov)\n    draws = rng.multivariate_normal(np.zeros(K), vcov, size=n_sim)\n    \n    # Standardize by sigma\n    t_stats = np.abs(draws / sigma[np.newaxis, :])\n    \n    # Take the max over coefficients for each draw\n    max_t = np.max(t_stats, axis=1)\n    \n    # Critical value is the (1-alpha) quantile\n    c_supt = np.quantile(max_t, 1 - alpha)\n    \n    return c_supt\n\n\n# Demo: compare pointwise vs sup-t critical values\nK = 8  # 8 pre/post periods\nrng = np.random.default_rng(42)\n\n# Generate a realistic vcov (correlated coefficients)\nA = rng.normal(0, 0.3, (K, K))\nvcov = A @ A.T + 0.1 * np.eye(K)  # positive definite\nsigma = np.sqrt(np.diag(vcov))\n\nc_pointwise = scipy_stats.norm.ppf(0.975)  # 1.96\nc_supt = compute_supt_critical_value(vcov, alpha=0.05)\n\nprint(f'Pointwise critical value (z_0.025): {c_pointwise:.3f}')\nprint(f'Sup-t critical value (K={K}):       {c_supt:.3f}')\nprint(f'Sup-t / Pointwise ratio:            {c_supt / c_pointwise:.2f}x wider')\n\nPointwise critical value (z_0.025): 1.960\nSup-t critical value (K=8):       2.701\nSup-t / Pointwise ratio:            1.38x wider\n\n\n\n# Visual comparison: pointwise vs sup-t bands\nfig, axes = plt.subplots(1, 2, figsize=(15, 5.5), sharey=True)\n\nperiods_demo = es_df['period'].values\nest_demo = es_df['estimate'].values\nse_demo = es_df['se'].values\n\n# Build a realistic vcov for our synthetic data\nK_demo = len(periods_demo)\n# Use a simple AR(1) correlation structure\nrho = 0.5\ncorr = np.array([[rho**abs(i-j) for j in range(K_demo)] for i in range(K_demo)])\nvcov_demo = np.outer(se_demo, se_demo) * corr\n\nc_supt_demo = compute_supt_critical_value(vcov_demo, alpha=0.05)\n\n# Left: pointwise CIs\nax = axes[0]\nci_low_pw = est_demo - 1.96 * se_demo\nci_high_pw = est_demo + 1.96 * se_demo\nax.fill_between(periods_demo, ci_low_pw, ci_high_pw, alpha=0.2, color='#2563eb', label='95% Pointwise CI')\nax.plot(periods_demo, est_demo, 'o-', color='#2563eb', markersize=7, linewidth=2, zorder=3)\nax.axhline(0, color='gray', linestyle='--', linewidth=1)\nax.axvline(-0.5, color='gray', linestyle=':', linewidth=1, alpha=0.5)\nax.set_title('Standard: Pointwise CIs', fontsize=13)\nax.set_xlabel('Period Relative to Treatment')\nax.set_ylabel('Treatment Effect')\nax.legend(fontsize=10)\nax.grid(True, alpha=0.2)\n\n# Right: sup-t bands\nax = axes[1]\nci_low_supt = est_demo - c_supt_demo * se_demo\nci_high_supt = est_demo + c_supt_demo * se_demo\n# Show pointwise as inner band\nax.fill_between(periods_demo, ci_low_supt, ci_high_supt, alpha=0.12, color='#dc2626', label='95% Simultaneous (sup-t)')\nax.fill_between(periods_demo, ci_low_pw, ci_high_pw, alpha=0.2, color='#2563eb', label='95% Pointwise')\nax.plot(periods_demo, est_demo, 'o-', color='#2563eb', markersize=7, linewidth=2, zorder=3)\nax.axhline(0, color='gray', linestyle='--', linewidth=1)\nax.axvline(-0.5, color='gray', linestyle=':', linewidth=1, alpha=0.5)\nax.set_title('Honest: Pointwise + Sup-t Bands', fontsize=13)\nax.set_xlabel('Period Relative to Treatment')\nax.legend(fontsize=10)\nax.grid(True, alpha=0.2)\n\nplt.tight_layout()\nplt.show()\n\nprint(f'\\nSup-t bands are {c_supt_demo/1.96:.1f}x wider than pointwise CIs')\nprint('This accounts for multiple testing across all periods simultaneously')\n\n\n\n\n\n\n\n\n\nSup-t bands are nanx wider than pointwise CIs\nThis accounts for multiple testing across all periods simultaneously"
  },
  {
    "objectID": "notebooks/honest_inference_exploration.html#gap-2-smoothest-confounding-path-eventstudyr",
    "href": "notebooks/honest_inference_exploration.html#gap-2-smoothest-confounding-path-eventstudyr",
    "title": "Honest Inference for Event Study Plots",
    "section": "4. Gap 2: Smoothest Confounding Path (eventstudyr)",
    "text": "4. Gap 2: Smoothest Confounding Path (eventstudyr)\n\nThe Idea\nFrom Freyaldenhoven et al. (2021), the smoothest path of confounding trend that is consistent with the estimated pre-treatment coefficients provides a visual benchmark. If the pre-treatment event study coefficients are indistinguishable from zero, this path shows what the “most conservative” violation of parallel trends could look like.\nFormally, we solve:\n\\[\\min_{\\delta} \\sum_{t} (\\delta_{t+1} - 2\\delta_t + \\delta_{t-1})^2 \\quad \\text{s.t.} \\quad \\delta_{\\text{pre}} \\text{ consistent with data}\\]\nThis gives the smoothest confound that could explain the pre-treatment pattern, which is then extrapolated post-treatment to show how much bias we might expect.\neventstudyr overlays this as a dashed line on the event study plot.\n\ndef compute_smoothest_path(pre_estimates, pre_periods, post_periods, ref_period=-1):\n    \"\"\"\n    Compute the smoothest confounding path consistent with pre-treatment estimates.\n    \n    Minimizes the sum of squared second differences (curvature) of the path,\n    subject to the path matching the pre-treatment estimates.\n    \n    Parameters\n    ----------\n    pre_estimates : array-like\n        Estimated coefficients for pre-treatment periods (excluding reference).\n    pre_periods : array-like\n        Period indices for pre-treatment.\n    post_periods : array-like\n        Period indices for post-treatment (for extrapolation).\n    ref_period : int\n        The reference (normalized) period.\n    \n    Returns\n    -------\n    all_periods : array\n        Combined pre + ref + post periods.\n    smoothest : array\n        The smoothest confounding path for all periods.\n    \"\"\"\n    # All periods including reference\n    all_periods = np.sort(np.concatenate([pre_periods, [ref_period], post_periods]))\n    T = len(all_periods)\n    \n    # Reference period index\n    ref_idx = np.where(all_periods == ref_period)[0][0]\n    \n    # Pre-period indices (excluding reference)\n    pre_idx = [np.where(all_periods == p)[0][0] for p in pre_periods]\n    \n    # Objective: minimize sum of squared second differences\n    # delta_path = [delta_0, delta_1, ..., delta_{T-1}]\n    # Second diff: delta_{t+1} - 2*delta_t + delta_{t-1}\n    \n    def objective(delta):\n        second_diffs = delta[2:] - 2*delta[1:-1] + delta[:-2]\n        return np.sum(second_diffs**2)\n    \n    # Constraints: delta at reference = 0, delta at pre periods = estimates\n    constraints = []\n    \n    # Reference period = 0\n    constraints.append({'type': 'eq', 'fun': lambda d, idx=ref_idx: d[idx]})\n    \n    # Pre-treatment periods match estimates\n    for i, (pidx, est) in enumerate(zip(pre_idx, pre_estimates)):\n        constraints.append({'type': 'eq', 'fun': lambda d, idx=pidx, e=est: d[idx] - e})\n    \n    # Initial guess: linear interpolation\n    x0 = np.zeros(T)\n    \n    result = minimize(objective, x0, constraints=constraints, method='SLSQP')\n    \n    return all_periods, result.x\n\n\n# Demo with synthetic data\npre_mask = (es_df['period'] &lt; -1)  # exclude reference period -1\npre_est = es_df.loc[pre_mask, 'estimate'].values\npre_per = es_df.loc[pre_mask, 'period'].values\npost_per = es_df.loc[es_df['period'] &gt;= 0, 'period'].values\n\nsmooth_periods, smooth_path = compute_smoothest_path(\n    pre_est, pre_per, post_per, ref_period=-1\n)\n\nprint('Smoothest confounding path:')\nfor p, v in zip(smooth_periods, smooth_path):\n    marker = ' &lt;-- ref' if p == -1 else (' &lt;-- post' if p &gt;= 0 else '')\n    print(f'  t={p:+d}: {v:+.4f}{marker}')\n\nSmoothest confounding path:\n  t=-5: +0.1490\n  t=-4: -0.0415\n  t=-3: +0.1943\n  t=-2: +0.4569\n  t=-1: +0.0000 &lt;-- ref\n  t=+0: -0.4573 &lt;-- post\n  t=+1: -0.9148 &lt;-- post\n  t=+2: -1.3726 &lt;-- post\n  t=+3: -1.8306 &lt;-- post\n  t=+4: -2.2889 &lt;-- post\n  t=+5: -2.7473 &lt;-- post\n\n\n\nfig, axes = plt.subplots(1, 2, figsize=(15, 5.5))\n\n# Left: standard event study\nax = axes[0]\nax.fill_between(periods_demo, est_demo - 1.96*se_demo, est_demo + 1.96*se_demo,\n                alpha=0.2, color='#2563eb')\nax.plot(periods_demo, est_demo, 'o-', color='#2563eb', markersize=7, linewidth=2, zorder=3)\nax.axhline(0, color='gray', linestyle='--', linewidth=1)\nax.axvline(-0.5, color='gray', linestyle=':', linewidth=1, alpha=0.5)\nax.set_title('Standard Event Study', fontsize=13)\nax.set_xlabel('Period Relative to Treatment')\nax.set_ylabel('Treatment Effect')\nax.grid(True, alpha=0.2)\n\n# Right: with smoothest confounding path\nax = axes[1]\nax.fill_between(periods_demo, est_demo - 1.96*se_demo, est_demo + 1.96*se_demo,\n                alpha=0.2, color='#2563eb')\nax.plot(periods_demo, est_demo, 'o-', color='#2563eb', markersize=7, linewidth=2,\n        zorder=3, label='Estimated')\nax.plot(smooth_periods, smooth_path, 's--', color='#dc2626', markersize=5,\n        linewidth=2, alpha=0.8, label='Smoothest confound', zorder=4)\nax.axhline(0, color='gray', linestyle='--', linewidth=1)\nax.axvline(-0.5, color='gray', linestyle=':', linewidth=1, alpha=0.5)\nax.set_title('+ Smoothest Confounding Path', fontsize=13)\nax.set_xlabel('Period Relative to Treatment')\nax.legend(fontsize=10)\nax.grid(True, alpha=0.2)\n\nplt.tight_layout()\nplt.show()\n\nprint('\\nInterpretation: The red dashed line shows the smoothest trend violation')\nprint('consistent with pre-treatment data, extrapolated post-treatment.')\nprint('This represents the potential bias under the least curvature assumption.')\n\n\n\n\n\n\n\n\n\nInterpretation: The red dashed line shows the smoothest trend violation\nconsistent with pre-treatment data, extrapolated post-treatment.\nThis represents the potential bias under the least curvature assumption."
  },
  {
    "objectID": "notebooks/honest_inference_exploration.html#gap-3-functional-scbs-fdid",
    "href": "notebooks/honest_inference_exploration.html#gap-3-functional-scbs-fdid",
    "title": "Honest Inference for Event Study Plots",
    "section": "5. Gap 3: Functional SCBs (fdid)",
    "text": "5. Gap 3: Functional SCBs (fdid)\n\nThe Idea\nFang & Liebl (2026) treat the event study coefficients as a continuous function (Gaussian process), not discrete points. This enables:\n\nInfimum-based SCBs (pre-treatment): For equivalence testing — if the entire band lies within \\([-\\epsilon, +\\epsilon]\\), we have evidence for parallel trends (not just failure to reject)\nSupremum-based SCBs (post-treatment): For relevance testing — if the band excludes zero everywhere, there’s a significant effect at all post-treatment periods\nHonest reference bands: Shows what the bands would look like under specified assumption violations\n\n\n\nKey Innovation: Two Types of SCBs\n\n\n\n\n\n\n\n\n\nType\nRegion\nTest\nInterpretation\n\n\n\n\nInfimum (tighter)\nPre-treatment\nEquivalence\nBand inside \\([-\\epsilon, \\epsilon]\\) \\(\\Rightarrow\\) parallel trends hold\n\n\nSupremum (wider)\nPost-treatment\nRelevance\nBand outside zero \\(\\Rightarrow\\) significant effect\n\n\n\nThe infimum SCB is: \\[\\hat{\\beta}(t) \\pm \\hat{c}_{\\inf,\\alpha} \\cdot \\hat{\\sigma}(t)\\] where \\(\\hat{c}_{\\inf,\\alpha}\\) satisfies \\(P\\left(\\inf_t \\left|\\frac{\\hat{\\beta}(t) - \\beta(t)}{\\hat{\\sigma}(t)}\\right| \\leq \\hat{c}_{\\inf,\\alpha}\\right) = 1 - \\alpha\\)\nThe supremum SCB is: \\[\\hat{\\beta}(t) \\pm \\hat{c}_{\\sup,\\alpha} \\cdot \\hat{\\sigma}(t)\\] where \\(\\hat{c}_{\\sup,\\alpha}\\) satisfies \\(P\\left(\\sup_t \\left|\\frac{\\hat{\\beta}(t) - \\beta(t)}{\\hat{\\sigma}(t)}\\right| \\leq \\hat{c}_{\\sup,\\alpha}\\right) = 1 - \\alpha\\)\n\ndef compute_scb_critical_values(vcov, alpha=0.05, n_sim=10000, seed=42):\n    \"\"\"\n    Compute infimum and supremum critical values for SCBs.\n    \n    Following Fang & Liebl (2026).\n    \n    Parameters\n    ----------\n    vcov : array-like (K x K)\n        Variance-covariance matrix.\n    alpha : float\n        Significance level.\n    n_sim : int\n        Number of simulations.\n    \n    Returns\n    -------\n    c_inf : float\n        Infimum critical value (for equivalence testing, tighter).\n    c_sup : float\n        Supremum critical value (for relevance testing, same as sup-t).\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    K = vcov.shape[0]\n    sigma = np.sqrt(np.diag(vcov))\n    \n    # Draw from MVN(0, vcov)\n    draws = rng.multivariate_normal(np.zeros(K), vcov, size=n_sim)\n    \n    # Standardize\n    t_stats = np.abs(draws / sigma[np.newaxis, :])\n    \n    # Supremum: max across coefficients (same as sup-t)\n    max_t = np.max(t_stats, axis=1)\n    c_sup = np.quantile(max_t, 1 - alpha)\n    \n    # Infimum: min across coefficients (tighter bands)\n    min_t = np.min(t_stats, axis=1)\n    c_inf = np.quantile(min_t, 1 - alpha)\n    \n    return c_inf, c_sup\n\n\n# Demo: compare all three critical values\nK_demo = len(periods_demo)\nrho = 0.5\ncorr_demo = np.array([[rho**abs(i-j) for j in range(K_demo)] for i in range(K_demo)])\nvcov_full = np.outer(se_demo, se_demo) * corr_demo\n\nc_inf, c_sup = compute_scb_critical_values(vcov_full)\n\nprint(f'Pointwise critical value: {1.96:.3f}')\nprint(f'Infimum critical value:   {c_inf:.3f}  (tighter - for equivalence testing)')\nprint(f'Supremum critical value:  {c_sup:.3f}  (wider  - for relevance testing)')\nprint(f'\\nInfimum &lt; Pointwise &lt; Supremum')\n\nPointwise critical value: 1.960\nInfimum critical value:   0.339  (tighter - for equivalence testing)\nSupremum critical value:  nan  (wider  - for relevance testing)\n\nInfimum &lt; Pointwise &lt; Supremum\n\n\n\n# Visual: fdid-style event study with different SCBs for pre vs post\nfig, ax = plt.subplots(figsize=(12, 6))\n\npre_mask = periods_demo &lt; 0\npost_mask = periods_demo &gt;= 0\n\n# Pre-treatment: infimum SCBs (equivalence testing)\nci_inf_low = est_demo - c_inf * se_demo\nci_inf_high = est_demo + c_inf * se_demo\n\n# Post-treatment: supremum SCBs (relevance testing)\nci_sup_low = est_demo - c_sup * se_demo\nci_sup_high = est_demo + c_sup * se_demo\n\n# Also show pointwise for comparison\nci_pw_low = est_demo - 1.96 * se_demo\nci_pw_high = est_demo + 1.96 * se_demo\n\n# Pre-treatment region\nax.fill_between(periods_demo[pre_mask], ci_inf_low[pre_mask], ci_inf_high[pre_mask],\n                alpha=0.2, color='#2563eb', label=f'Pre: Infimum SCB (c={c_inf:.2f})')\n\n# Post-treatment region\nax.fill_between(periods_demo[post_mask], ci_sup_low[post_mask], ci_sup_high[post_mask],\n                alpha=0.2, color='#dc2626', label=f'Post: Supremum SCB (c={c_sup:.2f})')\n\n# Pointwise CIs (dashed outline)\nax.plot(periods_demo, ci_pw_low, ':', color='gray', linewidth=1, alpha=0.5)\nax.plot(periods_demo, ci_pw_high, ':', color='gray', linewidth=1, alpha=0.5,\n        label='Pointwise CI (c=1.96)')\n\n# Point estimates\nax.plot(periods_demo[pre_mask], est_demo[pre_mask], 'o-', color='#2563eb',\n        markersize=7, linewidth=2, zorder=3)\nax.plot(periods_demo[post_mask], est_demo[post_mask], 'o-', color='#dc2626',\n        markersize=7, linewidth=2, zorder=3)\n\n# Equivalence region (epsilon band around zero)\nepsilon = 0.5\nax.axhspan(-epsilon, epsilon, alpha=0.06, color='#16a34a', label=f'Equivalence region (epsilon={epsilon})')\n\nax.axhline(0, color='gray', linestyle='--', linewidth=1)\nax.axvline(-0.5, color='gray', linestyle=':', linewidth=1.5, alpha=0.7)\n\nax.set_title('fdid-Style: Infimum SCB (pre) + Supremum SCB (post)', fontsize=13)\nax.set_xlabel('Period Relative to Treatment', fontsize=11)\nax.set_ylabel('Treatment Effect', fontsize=11)\nax.legend(fontsize=9, loc='upper left')\nax.grid(True, alpha=0.2)\n\nplt.tight_layout()\nplt.show()\n\nprint('\\nInterpretation:')\nprint('- Pre-treatment: Infimum SCBs are TIGHTER than pointwise CIs.')\nprint('  If infimum band is inside the green equivalence region, we have')\nprint('  evidence FOR parallel trends (not just failure to reject).')\nprint('- Post-treatment: Supremum SCBs are WIDER than pointwise CIs.')\nprint('  If supremum band excludes zero, the treatment effect is significant')\nprint('  at ALL post-treatment periods simultaneously.')\n\n\n\n\n\n\n\n\n\nInterpretation:\n- Pre-treatment: Infimum SCBs are TIGHTER than pointwise CIs.\n  If infimum band is inside the green equivalence region, we have\n  evidence FOR parallel trends (not just failure to reject).\n- Post-treatment: Supremum SCBs are WIDER than pointwise CIs.\n  If supremum band excludes zero, the treatment effect is significant\n  at ALL post-treatment periods simultaneously."
  },
  {
    "objectID": "notebooks/honest_inference_exploration.html#insights-from-the-papers",
    "href": "notebooks/honest_inference_exploration.html#insights-from-the-papers",
    "title": "Honest Inference for Event Study Plots",
    "section": "6. Insights from the Papers",
    "text": "6. Insights from the Papers\n\n6.1. Freyaldenhoven, Hansen, Perez Perez, Shapiro (2021)\nKey recommendations for event study visualization:\n\nUse cumulative parameterization: Instead of showing \\(\\beta_t\\) (level effects), show \\(\\sum_{s \\leq t} \\beta_s\\) (cumulative effects). This is more interpretable and comparable across studies.\nReport simultaneous CIs: Pointwise CIs are misleading for joint hypothesis testing. Sup-t bands provide correct simultaneous coverage.\nOverlay the smoothest confounding path: Shows the most conservative violation of parallel trends consistent with the data.\nPre-trend tests: F-test for joint significance of pre-treatment coefficients is more powerful than visual inspection.\nCareful with the reference period: The choice of normalization period affects interpretation.\n\n\n\n6.2. Roth (2026) — “Interpreting Event-Studies from Recent DiD Methods”\nCritical warning for modern DiD practitioners:\n\nModern DiD methods (Callaway-Sant’Anna, Borusyak-Jaravel-Spiess) construct pre- and post-treatment coefficients asymmetrically. Pre-treatment coefficients are 2x2 DiD comparisons, while post-treatment coefficients are the actual treatment effects of interest.\n\nImplications: 1. Visual pre-trend tests are misleading: Small pre-treatment coefficients do not necessarily mean parallel trends hold 2. Pre-testing can cause distortions: Conditioning on passing a visual pre-trend test biases the post-treatment estimates 3. Standard event study heuristics break down: The “parallel pre-trends” visual check that works for TWFE does NOT work for modern estimators\nRecommendation: Use formal sensitivity analysis (HonestDiD) rather than visual pre-trend inspection, especially with modern DiD methods.\n\n\n6.3. Implications for Our Contribution\nThese papers reinforce each other: - Roth (2026) says: don’t rely on visual pre-trend tests alone - Freyaldenhoven et al. (2021) says: if you do visualize, use simultaneous bands + smoothest paths - Rambachan & Roth (2023) says: supplement with formal sensitivity analysis - Fang & Liebl (2026) says: unify everything with functional SCBs + equivalence testing\nOur contribution should provide all of these as composable layers on top of the standard event study plot."
  },
  {
    "objectID": "notebooks/honest_inference_exploration.html#compatibility-assessment",
    "href": "notebooks/honest_inference_exploration.html#compatibility-assessment",
    "title": "Honest Inference for Event Study Plots",
    "section": "7. Compatibility Assessment",
    "text": "7. Compatibility Assessment\n\nAre these approaches compatible or mutually exclusive?\nThey are strongly complementary. They address different aspects of the same problem:\nEvent Study Plot\n|\n+-- Layer 0: Point estimates + pointwise CIs (current diff-diff)\n|\n+-- Layer 1: Visualization improvements (ggfixest-style)\n|   +-- Ribbon CIs, nested CIs, multi-model, aggregate effects\n|\n+-- Layer 2: Simultaneous inference (eventstudyr)\n|   +-- Sup-t bands (wider CIs with correct joint coverage)\n|   +-- Smoothest confounding path overlay\n|\n+-- Layer 3: Sensitivity analysis (HonestDiD)\n|   +-- Separate sensitivity plot (CI vs. M)\n|   +-- Honest CIs overlaid on event study\n|   +-- Breakdown values\n|\n+-- Layer 4: Functional inference (fdid)\n    +-- Infimum SCBs for pre-treatment (equivalence testing)\n    +-- Supremum SCBs for post-treatment (relevance testing)\n    +-- Honest reference bands\n\n\nPotential Tensions\n\n\n\n\n\n\n\nIssue\nResolution\n\n\n\n\nHonestDiD uses separate sensitivity plots vs. fdid embeds in event study\nBoth are useful — keep sensitivity plot and add overlaid honest bands\n\n\nSup-t bands (eventstudyr) vs. supremum SCBs (fdid)\nThey’re the same for post-treatment. Sup-t = supremum SCB. Unify.\n\n\nInfimum SCBs (fdid) are novel\nNo conflict — this is a genuinely new feature. Add as option.\n\n\nRoth (2026) warns about visual pre-trends\nCompatible — formal tests supplement (not replace) plots. Add warnings.\n\n\n\n\n\nThe Unified Vision\nOne ggiplot() function that can compose all layers:\nggiplot(\n    results,\n    geom_style='ribbon',\n    ci_level=[0.80, 0.95],\n    # Honest inference options:\n    ci_type='pointwise',       # 'pointwise', 'sup-t', 'fdid'\n    show_smoothest_path=True,  # eventstudyr-style overlay\n    honest_M=1.0,             # HonestDiD-style robust CIs\n    equivalence_eps=0.5,       # fdid-style equivalence region\n    aggr_eff='post',          # aggregate effects overlay\n)"
  },
  {
    "objectID": "notebooks/honest_inference_exploration.html#prototypes",
    "href": "notebooks/honest_inference_exploration.html#prototypes",
    "title": "Honest Inference for Event Study Plots",
    "section": "8. Prototypes",
    "text": "8. Prototypes\nNow let’s prototype the key missing features that combine insights from all packages.\n\n8.1. Enhanced iplot_data() with Honest Inference\n\ndef iplot_data(\n    results,\n    ci_level=0.95,\n    ci_type='pointwise',\n    vcov=None,\n    reference_period=None,\n    n_sim=10000,\n    seed=42,\n):\n    \"\"\"\n    Extract tidy event study data with optional honest inference bands.\n    \n    Parameters\n    ----------\n    results : DataFrame or diff-diff result object\n        Event study results.\n    ci_level : float\n        Confidence level.\n    ci_type : str\n        'pointwise' (standard), 'sup-t' (simultaneous), or 'fdid' (functional).\n    vcov : array-like, optional\n        Variance-covariance matrix. Required for 'sup-t' and 'fdid'.\n        If None and ci_type != 'pointwise', falls back to diagonal (pointwise).\n    reference_period : int, optional\n        The reference period (normalized to 0).\n    \n    Returns\n    -------\n    DataFrame with columns: period, estimate, se, ci_low, ci_high, ci_level, ci_type, is_ref\n    \"\"\"\n    # Extract base data\n    if isinstance(results, pd.DataFrame):\n        df = results.copy()\n        if 'effect' in df.columns:\n            df = df.rename(columns={'effect': 'estimate'})\n    elif hasattr(results, 'event_study_effects'):\n        es = results.event_study_effects\n        rows = []\n        for entry in es:\n            p = entry.get('event_time', entry.get('period', 0))\n            est = entry.get('att', entry.get('estimate', 0))\n            se_val = entry.get('se', 0)\n            rows.append({'period': p, 'estimate': est, 'se': se_val})\n        df = pd.DataFrame(rows).sort_values('period').reset_index(drop=True)\n    elif hasattr(results, 'period_effects'):\n        # MultiPeriodDiDResults\n        pe = results.period_effects\n        rows = []\n        for p, info in pe.items():\n            if isinstance(info, dict):\n                rows.append({'period': p, 'estimate': info.get('att', 0), 'se': info.get('se', 0)})\n            else:\n                rows.append({'period': p, 'estimate': info, 'se': 0})\n        df = pd.DataFrame(rows).sort_values('period').reset_index(drop=True)\n    else:\n        raise TypeError(f'Unsupported result type: {type(results)}')\n    \n    periods = df['period'].values\n    estimates = df['estimate'].values\n    se_vals = df['se'].values\n    K = len(periods)\n    \n    # Compute critical values based on ci_type\n    if ci_type == 'pointwise':\n        z = scipy_stats.norm.ppf(1 - (1 - ci_level) / 2)\n        df['ci_low'] = estimates - z * se_vals\n        df['ci_high'] = estimates + z * se_vals\n    \n    elif ci_type == 'sup-t':\n        if vcov is None:\n            # Fall back to diagonal (equivalent to Bonferroni-like correction)\n            vcov = np.diag(se_vals**2)\n        c_sup = compute_supt_critical_value(vcov, alpha=1-ci_level, n_sim=n_sim, seed=seed)\n        df['ci_low'] = estimates - c_sup * se_vals\n        df['ci_high'] = estimates + c_sup * se_vals\n    \n    elif ci_type == 'fdid':\n        if vcov is None:\n            vcov = np.diag(se_vals**2)\n        \n        ref = reference_period if reference_period is not None else -1\n        pre_idx = np.where(periods &lt; ref)[0]\n        post_idx = np.where(periods &gt;= 0)[0]\n        \n        # Compute separate critical values for pre and post\n        if len(pre_idx) &gt; 1:\n            vcov_pre = vcov[np.ix_(pre_idx, pre_idx)]\n            c_inf_pre, _ = compute_scb_critical_values(\n                vcov_pre, alpha=1-ci_level, n_sim=n_sim, seed=seed)\n        else:\n            c_inf_pre = scipy_stats.norm.ppf(1 - (1 - ci_level) / 2)\n        \n        if len(post_idx) &gt; 1:\n            vcov_post = vcov[np.ix_(post_idx, post_idx)]\n            _, c_sup_post = compute_scb_critical_values(\n                vcov_post, alpha=1-ci_level, n_sim=n_sim, seed=seed)\n        else:\n            c_sup_post = scipy_stats.norm.ppf(1 - (1 - ci_level) / 2)\n        \n        # Apply different critical values to pre vs post\n        cv = np.full(K, scipy_stats.norm.ppf(1 - (1 - ci_level) / 2))  # default pointwise\n        cv[pre_idx] = c_inf_pre\n        cv[post_idx] = c_sup_post\n        \n        df['ci_low'] = estimates - cv * se_vals\n        df['ci_high'] = estimates + cv * se_vals\n    \n    else:\n        raise ValueError(f'Unknown ci_type: {ci_type}')\n    \n    df['ci_level'] = ci_level\n    df['ci_type'] = ci_type\n    df['is_ref'] = df['period'] == reference_period if reference_period is not None else False\n    \n    return df[['period', 'estimate', 'se', 'ci_low', 'ci_high', 'ci_level', 'ci_type', 'is_ref']]\n\n\n# Demo: compare all three CI types\nfor ct in ['pointwise', 'sup-t', 'fdid']:\n    td = iplot_data(es_df, ci_level=0.95, ci_type=ct, vcov=vcov_full, reference_period=-1)\n    mean_width = (td['ci_high'] - td['ci_low']).mean()\n    print(f'{ct:12s}: mean CI width = {mean_width:.3f}')\n\n\n\n8.2. Enhanced ggiplot() with Honest Inference Layers\n\ndef ggiplot(\n    results,\n    *,\n    # Core options\n    geom_style='ribbon',\n    ci_level=0.95,\n    reference_period=None,\n    # Honest inference options\n    ci_type='pointwise',\n    vcov=None,\n    show_pointwise=False,\n    show_smoothest_path=False,\n    equivalence_eps=None,\n    # Aggregate effects\n    aggr_eff=None,\n    # Aesthetics\n    color='#2563eb',\n    post_color=None,\n    figsize=(11, 6),\n    title='Event Study',\n    xlabel='Period Relative to Treatment',\n    ylabel='Treatment Effect',\n    ax=None,\n    show=True,\n    n_sim=10000,\n    seed=42,\n):\n    \"\"\"\n    Enhanced event study plot with honest inference layers.\n    \n    Combines insights from ggfixest, HonestDiD, eventstudyr, and fdid.\n    \n    Parameters\n    ----------\n    ci_type : str\n        'pointwise' (standard), 'sup-t' (simultaneous bands from eventstudyr),\n        or 'fdid' (functional SCBs with infimum pre / supremum post).\n    show_pointwise : bool\n        If ci_type != 'pointwise', also show pointwise CIs as dashed outline.\n    show_smoothest_path : bool\n        Overlay the smoothest confounding path (Freyaldenhoven et al.).\n    equivalence_eps : float or None\n        If set, show equivalence region [-eps, +eps] for pre-treatment (fdid).\n    \"\"\"\n    if ax is None:\n        fig, ax = plt.subplots(figsize=figsize)\n    \n    if post_color is None:\n        post_color = '#dc2626' if ci_type == 'fdid' else color\n    \n    ref = reference_period if reference_period is not None else -1\n    \n    # Get tidy data\n    td = iplot_data(results, ci_level=ci_level, ci_type=ci_type,\n                    vcov=vcov, reference_period=reference_period,\n                    n_sim=n_sim, seed=seed)\n    \n    periods = td['period'].values\n    estimates = td['estimate'].values\n    ci_low = td['ci_low'].values\n    ci_high = td['ci_high'].values\n    \n    pre_mask = periods &lt; ref\n    post_mask = periods &gt;= 0\n    ref_mask = periods == ref\n    \n    # Label for CI type\n    ci_labels = {\n        'pointwise': 'Pointwise CI',\n        'sup-t': 'Simultaneous (sup-t) band',\n        'fdid': 'Functional SCB',\n    }\n    \n    if geom_style == 'ribbon':\n        if ci_type == 'fdid':\n            # Different colors for pre vs post\n            # Pre: infimum (tighter)\n            if pre_mask.any():\n                p_pre = periods[pre_mask | ref_mask]\n                ci_l_pre = np.concatenate([ci_low[pre_mask], ci_low[ref_mask]])\n                ci_h_pre = np.concatenate([ci_high[pre_mask], ci_high[ref_mask]])\n                e_pre = np.concatenate([estimates[pre_mask], estimates[ref_mask]])\n                sort_idx = np.argsort(p_pre)\n                ax.fill_between(p_pre[sort_idx], ci_l_pre[sort_idx], ci_h_pre[sort_idx],\n                                alpha=0.2, color=color, label='Pre: Infimum SCB')\n                ax.plot(p_pre[sort_idx], e_pre[sort_idx], 'o-', color=color,\n                        markersize=7, linewidth=2, zorder=3)\n            \n            # Post: supremum (wider)\n            if post_mask.any():\n                ax.fill_between(periods[post_mask], ci_low[post_mask], ci_high[post_mask],\n                                alpha=0.2, color=post_color, label='Post: Supremum SCB')\n                ax.plot(periods[post_mask], estimates[post_mask], 'o-', color=post_color,\n                        markersize=7, linewidth=2, zorder=3)\n        else:\n            ax.fill_between(periods, ci_low, ci_high, alpha=0.2, color=color,\n                            label=f'{ci_level:.0%} {ci_labels[ci_type]}')\n            ax.plot(periods, estimates, 'o-', color=color, markersize=7,\n                    linewidth=2, zorder=3)\n    \n    elif geom_style == 'errorbar':\n        if ci_type == 'fdid':\n            if pre_mask.any():\n                p_pre = periods[pre_mask | ref_mask]\n                e_pre = estimates[pre_mask | ref_mask]\n                cl_pre = ci_low[pre_mask | ref_mask]\n                ch_pre = ci_high[pre_mask | ref_mask]\n                ax.errorbar(p_pre, e_pre,\n                            yerr=[e_pre - cl_pre, ch_pre - e_pre],\n                            fmt='o-', color=color, capsize=4, markersize=7,\n                            linewidth=1.5, label='Pre: Infimum SCB')\n            if post_mask.any():\n                ax.errorbar(periods[post_mask], estimates[post_mask],\n                            yerr=[estimates[post_mask] - ci_low[post_mask],\n                                  ci_high[post_mask] - estimates[post_mask]],\n                            fmt='o-', color=post_color, capsize=4, markersize=7,\n                            linewidth=1.5, label='Post: Supremum SCB')\n        else:\n            ax.errorbar(periods, estimates,\n                        yerr=[estimates - ci_low, ci_high - estimates],\n                        fmt='o-', color=color, capsize=4, markersize=7,\n                        linewidth=1.5, label=f'{ci_level:.0%} {ci_labels[ci_type]}')\n    \n    # Show pointwise CIs as dashed outline when using honest bands\n    if show_pointwise and ci_type != 'pointwise':\n        td_pw = iplot_data(results, ci_level=ci_level, ci_type='pointwise',\n                           reference_period=reference_period)\n        ax.plot(td_pw['period'], td_pw['ci_low'], ':', color='gray', linewidth=1, alpha=0.5)\n        ax.plot(td_pw['period'], td_pw['ci_high'], ':', color='gray', linewidth=1, alpha=0.5,\n                label='Pointwise CI')\n    \n    # Smoothest confounding path overlay\n    if show_smoothest_path:\n        pre_est_mask = (periods &lt; ref) & (periods != ref)\n        if pre_est_mask.sum() &gt;= 2:\n            sp_periods, sp_path = compute_smoothest_path(\n                estimates[pre_est_mask], periods[pre_est_mask],\n                periods[post_mask], ref_period=ref\n            )\n            ax.plot(sp_periods, sp_path, 's--', color='#f59e0b', markersize=4,\n                    linewidth=2, alpha=0.8, label='Smoothest confound', zorder=4)\n    \n    # Equivalence region\n    if equivalence_eps is not None:\n        ax.axhspan(-equivalence_eps, equivalence_eps, alpha=0.06, color='#16a34a',\n                   label=f'Equivalence region (eps={equivalence_eps})')\n    \n    # Aggregate effect overlay\n    if aggr_eff is not None:\n        if aggr_eff in ('post', 'both'):\n            post_est = estimates[post_mask]\n            post_se = td['se'].values[post_mask]\n            if len(post_est) &gt; 0:\n                mean_eff = post_est.mean()\n                mean_se = np.sqrt(np.mean(post_se**2))\n                z = scipy_stats.norm.ppf(1 - (1 - ci_level) / 2)\n                ax.axhspan(mean_eff - z*mean_se, mean_eff + z*mean_se,\n                           alpha=0.08, color='#dc2626')\n                ax.axhline(mean_eff, color='#dc2626', linewidth=2, alpha=0.6,\n                           label=f'Mean Post ATT = {mean_eff:.2f}')\n    \n    # Reference lines\n    ax.axhline(0, color='gray', linestyle='--', linewidth=1)\n    ax.axvline(ref + 0.5, color='gray', linestyle=':', linewidth=1.5, alpha=0.7)\n    \n    # Reference period marker\n    if reference_period is not None:\n        ref_row = td[td['is_ref']]\n        if len(ref_row) &gt; 0:\n            ax.plot(reference_period, ref_row['estimate'].values[0], 'o',\n                    color='white', markersize=10, markeredgecolor=color,\n                    markeredgewidth=2, zorder=5)\n    \n    ax.set_title(title, fontsize=13)\n    ax.set_xlabel(xlabel, fontsize=11)\n    ax.set_ylabel(ylabel, fontsize=11)\n    ax.legend(fontsize=9, loc='upper left')\n    ax.grid(True, alpha=0.2)\n    \n    if show:\n        plt.tight_layout()\n        plt.show()\n    \n    return ax\n\nprint('ggiplot() with honest inference defined')\n\n\n\n8.3. Demo: All CI Types Side by Side\n\nfig, axes = plt.subplots(1, 3, figsize=(18, 5.5), sharey=True)\n\nfor ax, ct in zip(axes, ['pointwise', 'sup-t', 'fdid']):\n    ggiplot(\n        es_df, geom_style='ribbon', ci_level=0.95,\n        ci_type=ct, vcov=vcov_full,\n        reference_period=-1,\n        show_pointwise=(ct != 'pointwise'),\n        title=f'ci_type = \"{ct}\"',\n        ax=ax, show=False,\n    )\n\nplt.tight_layout()\nplt.show()\n\n\n\n8.4. Demo: Full Feature Combination\n\n# The \"everything\" plot: fdid SCBs + smoothest path + equivalence region + aggregate effect\nggiplot(\n    es_df,\n    geom_style='ribbon',\n    ci_level=0.95,\n    ci_type='fdid',\n    vcov=vcov_full,\n    reference_period=-1,\n    show_pointwise=True,\n    show_smoothest_path=True,\n    equivalence_eps=0.5,\n    aggr_eff='post',\n    title='Full Feature: fdid SCBs + Smoothest Path + Equivalence + Aggregate',\n    figsize=(13, 7),\n)\n\n\n# Sup-t bands + smoothest path (eventstudyr-style)\nggiplot(\n    es_df,\n    geom_style='ribbon',\n    ci_level=0.95,\n    ci_type='sup-t',\n    vcov=vcov_full,\n    reference_period=-1,\n    show_pointwise=True,\n    show_smoothest_path=True,\n    title='eventstudyr-style: Sup-t Bands + Smoothest Confounding Path',\n    figsize=(12, 6),\n)\n\n\n# Same features but with errorbar style\nfig, axes = plt.subplots(1, 2, figsize=(16, 5.5))\n\nggiplot(\n    es_df, geom_style='errorbar', ci_type='sup-t',\n    vcov=vcov_full, reference_period=-1,\n    show_smoothest_path=True, show_pointwise=True,\n    title='Errorbar + Sup-t', ax=axes[0], show=False,\n)\n\nggiplot(\n    es_df, geom_style='errorbar', ci_type='fdid',\n    vcov=vcov_full, reference_period=-1,\n    equivalence_eps=0.5,\n    title='Errorbar + fdid SCBs', ax=axes[1], show=False,\n)\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "notebooks/honest_inference_exploration.html#multi-model-comparison-fairness",
    "href": "notebooks/honest_inference_exploration.html#multi-model-comparison-fairness",
    "title": "Honest Inference for Event Study Plots",
    "section": "9. Multi-Model Comparison Fairness",
    "text": "9. Multi-Model Comparison Fairness\n\nThe Core Issue\nWhen we overlay event studies from different estimators on the same plot, the coefficients may not mean the same thing — even if they look the same visually.\nThe problem boils down to one question: how is each pre-treatment coefficient \\(\\hat{\\beta}_t\\) constructed?\n\n\n\n9.0. Definitions: Long vs. Short Differences\nConsider a simple 2-group DiD setup with treatment starting at \\(t=0\\) and reference period \\(t=-1\\). Define:\n\\[\\bar{Y}_t^{D} = \\bar{Y}_t^{\\text{treat}} - \\bar{Y}_t^{\\text{control}} \\quad \\text{(group difference at time } t \\text{)}\\]\nThen:\nLong difference (relative to a fixed reference period): \\[\\hat{\\beta}_t^{\\text{long}} = \\bar{Y}_t^D - \\bar{Y}_{-1}^D\\]\nEvery period is compared to the same baseline (\\(t=-1\\)). So \\(\\hat{\\beta}_{-5}\\) tells you: “how different was the treated-control gap at \\(t=-5\\) compared to \\(t=-1\\)?”\nShort difference (relative to the preceding period): \\[\\hat{\\beta}_t^{\\text{short}} = \\bar{Y}_t^D - \\bar{Y}_{t-1}^D\\]\nEach period is compared to its immediate neighbor. So \\(\\hat{\\beta}_{-5}\\) tells you: “how much did the treated-control gap change between \\(t=-6\\) and \\(t=-5\\)?”\nThese are fundamentally different objects. Long differences cumulate, short differences are incremental.\n\n# Concrete numerical example: Long vs Short differences\nimport pandas as pd\nimport numpy as np\n\n# Suppose Y_treat - Y_control (group gap) evolves as:\nperiods = np.array([-5, -4, -3, -2, -1, 0, 1, 2, 3])\nY_gap   = np.array([1.0, 1.5, 2.0, 2.5, 3.0, 6.0, 7.0, 8.0, 9.0])\n#                    ^--- linear pre-trend ---^  ^--- treatment kicks in\n\nref = -1  # reference period\nref_idx = np.where(periods == ref)[0][0]\n\n# Long differences: each period vs fixed reference (t=-1)\nbeta_long = Y_gap - Y_gap[ref_idx]\n\n# Short differences: each period vs its immediate predecessor\nbeta_short = np.zeros(len(periods))\nbeta_short[0] = np.nan  # no predecessor for first period\nfor i in range(1, len(periods)):\n    beta_short[i] = Y_gap[i] - Y_gap[i-1]\n\nexample = pd.DataFrame({\n    'period': periods,\n    'Y_gap': Y_gap,\n    'beta_long (vs t=-1)': beta_long,\n    'beta_short (vs t-1)': beta_short,\n})\nprint('=== Long vs Short Differences ===')\nprint()\nprint(example.to_string(index=False, float_format='%.1f'))\nprint()\nprint('Notice:')\nprint('  - Long diffs: pre-treatment shows a trend (-2.0, -1.5, -1.0, -0.5, 0)')\nprint('  - Short diffs: pre-treatment is constant (0.5, 0.5, 0.5, 0.5)')\nprint('  - Post-treatment: long diffs = 3.0, 4.0, 5.0, 6.0 (cumulative)')\nprint('  - Post-treatment: short diffs = 3.0, 1.0, 1.0, 1.0 (incremental)')\nprint()\nprint('KEY: If pre-trends are LINEAR, short diffs hide the trend (looks flat)!')\n\n\n\n9.1. How Each Estimator Constructs Event Study Coefficients\nNow let’s go estimator by estimator. All of them produce event study coefficients \\(\\hat{\\beta}_e\\) indexed by relative time \\(e\\) (periods relative to treatment). All normalize \\(\\hat{\\beta}_{-1} = 0\\). But how they build the other coefficients differs.\n\n\nDynamic TWFE / MultiPeriodDiD\nReference: Last pre-period (\\(e=-1\\)), dropped from the regression.\nMethod: OLS regression with period dummies: \\[Y_{it} = \\alpha_i + \\gamma_t + \\sum_{e \\neq -1} \\beta_e \\cdot \\mathbb{1}[t - g_i = e] + \\varepsilon_{it}\\]\nAll \\(\\beta_e\\) coefficients (both pre and post) are long differences relative to \\(e=-1\\).\n\\(\\Rightarrow\\) Pre-treatment \\(\\hat{\\beta}_{-3}\\) answers: “how different was the gap at \\(e=-3\\) vs. \\(e=-1\\)?”\n\n\n\nSun-Abraham\nReference: Also \\(e=-1\\), also dropped from the regression.\nMethod: Same regression structure as Dynamic TWFE, but with cohort-specific interactions to handle heterogeneity: \\[Y_{it} = \\alpha_i + \\gamma_t + \\sum_{g} \\sum_{e \\neq -1} \\delta_{g,e} \\cdot \\mathbb{1}[G_i = g] \\cdot \\mathbb{1}[t - g = e] + \\varepsilon_{it}\\]\nThen aggregates: \\(\\hat{\\beta}_e = \\sum_g w_{g,e} \\cdot \\hat{\\delta}_{g,e}\\) (interaction-weighted average).\nAll \\(\\beta_e\\) are still long differences relative to \\(e=-1\\), just with proper heterogeneity handling.\nBottom line: Sun-Abraham is essentially TWFE done right. Same reference, same type of coefficient, same interpretation. “Dropping from regression” = normalizing to zero. It’s the same thing.\n\\(\\Rightarrow\\) Comparable to Dynamic TWFE: YES\n\n\n\nCallaway-Sant’Anna\nReference: \\(e=-1\\) (the period before treatment for each cohort \\(g\\)).\nMethod: Builds group-time ATTs \\(\\widehat{ATT}(g,t)\\) via 2x2 DiD comparisons, then aggregates to event study.\nHere’s where it gets tricky: CS has two modes for the base period:\nbase_period='universal' (recommended for comparison): - ALL coefficients (pre and post) use \\(t = g-1\\) as the comparison period - \\(\\hat{\\beta}_e = \\widehat{ATT}(g, g+e)\\) where each ATT compares to \\(t=g-1\\) - This is a long difference — same as TWFE/Sun-Abraham - \\(\\Rightarrow\\) Comparable: YES\nbase_period='varying' (the DEFAULT): - Post-treatment: Uses \\(t=g-1\\) as comparison → long difference ✓ - Pre-treatment: Uses \\(t-1\\) as comparison for each \\(t\\) → short difference ✗ - This means pre and post coefficients are constructed with different formulas - \\(\\Rightarrow\\) Comparable: ONLY post-treatment. Pre-treatment coefficients mean something different!\n\n\n\nImputation DiD (Borusyak-Jaravel-Spiess)\nThis one works completely differently from the others. No regression with period dummies at all.\nMethod (two steps):\n\nEstimate the counterfactual: Using only untreated observations (units before they get treated + never-treated), fit unit and time fixed effects: \\[\\hat{Y}_{it}(0) = \\hat{\\alpha}_i + \\hat{\\gamma}_t\\] This gives you a prediction of what \\(Y\\) would have been without treatment for every unit-time.\nCompute individual treatment effects: For each treated observation: \\[\\hat{\\tau}_{it} = Y_{it} - \\hat{Y}_{it}(0) = Y_{it} - \\hat{\\alpha}_i - \\hat{\\gamma}_t\\]\nAggregate by horizon: Group these \\(\\hat{\\tau}_{it}\\) by relative time \\(h = t - g_i\\) and average: \\[\\hat{\\beta}_h = \\frac{1}{N_h} \\sum_{i,t: t-g_i = h} \\hat{\\tau}_{it}\\]\n\nReference period: BJS doesn’t naturally produce a coefficient at \\(h=-1\\). Since at \\(h=-1\\) no unit is yet treated, there are no \\(\\hat{\\tau}_{it}\\) to aggregate. The code manually adds \\(\\hat{\\beta}_{-1} = 0\\) for plotting. That’s what “constructed” means.\nPre-treatment coefficients (\\(h &lt; -1\\)): These are placebo tests. They show \\(Y - \\hat{Y}(0)\\) for periods before treatment. If the model is right, these should be \\(\\approx 0\\). They’re not “differences relative to a reference” — they’re direct residuals from the counterfactual model.\nAre they comparable? The post-treatment coefficients estimate the same thing (ATT at each relative time) and should be numerically close to the other estimators. The pre-treatment coefficients are on a similar scale but have a different construction — they test model specification, not parallel trends relative to a reference.\n\\(\\Rightarrow\\) Comparable: Post-treatment YES (same estimand). Pre-treatment: similar scale but different interpretation.\n\n\n\n\nSummary Table\n\n\n\n\n\n\n\n\n\n\nEstimator\nReference\nPre-treatment coefficients\nPost-treatment\nComparable to TWFE?\n\n\n\n\nDynamic TWFE\n\\(e=-1\\) (dropped)\nLong diff vs. \\(e=-1\\)\nLong diff vs. \\(e=-1\\)\n– (baseline)\n\n\nSun-Abraham\n\\(e=-1\\) (dropped)\nLong diff vs. \\(e=-1\\)\nLong diff vs. \\(e=-1\\)\nYes (same approach, better heterogeneity)\n\n\nCS (universal)\n\\(e=-1\\) (fixed)\nLong diff vs. \\(e=-1\\)\nLong diff vs. \\(e=-1\\)\nYes\n\n\nCS (varying)\n\\(e=-1\\) (fixed)\nShort diff (\\(e\\) vs \\(e-1\\))\nLong diff vs. \\(e=-1\\)\nPost only (pre is different!)\n\n\nImputation (BJS)\n\\(e=-1\\) (constructed)\nPlacebo residuals \\(Y - \\hat{Y}(0)\\)\nImputed \\(\\hat{\\tau}\\)\nPost yes, pre similar but different meaning\n\n\n\nThe safe multi-model comparisons are: - TWFE + Sun-Abraham + CS (universal) → fully comparable - Adding Imputation → post-treatment comparable, pre-treatment interpret with caution - CS (varying) → only post-treatment is comparable; pre-treatment is a different quantity\n\n\n9.2. Visual Demo: The “Kink” Artifact (Roth, 2026)\nNow that we understand the difference between long and short differences, let’s see why it matters visually.\nSetup: There is a linear pre-trend violation (the treated-control gap grows by 0.3 each period) but NO treatment effect at all. What does each estimator’s event study look like?\n\n# Simulate: linear pre-trend violation, NO treatment effect\n# Following Roth (2026) Figure 1\nnp.random.seed(42)\nperiods_sim = np.arange(-5, 6)\nref = -1\n\n# True confounding: gap grows linearly at 0.3 per period\n# Y_gap(t) = baseline + 0.3 * t\n# There is NO treatment effect -- just a trend violation\ndelta_true = 0.3 * periods_sim\n\nse_sim = np.full(len(periods_sim), 0.25)\nnoise = lambda: np.random.normal(0, 0.12, len(periods_sim))\n\n# 1. TWFE / MultiPeriod: ALL coefficients are long diffs vs e=-1\n# beta_t = (Y_gap_t - Y_gap_{-1}) = 0.3*t - 0.3*(-1) = 0.3*(t+1)\ntwfe_coefs = delta_true - delta_true[periods_sim == ref][0] + noise()\ntwfe_coefs[periods_sim == ref] = 0.0\n\n# 2. CS (varying): pre = SHORT diffs (t vs t-1), post = LONG diffs vs ref\ncs_coefs = np.zeros(len(periods_sim))\nfor i, p in enumerate(periods_sim):\n    if p &lt; ref:\n        # Short diff: delta_t - delta_{t-1} = 0.3 (constant!)\n        cs_coefs[i] = 0.3 + np.random.normal(0, 0.12)\n    elif p == ref:\n        cs_coefs[i] = 0.0\n    else:\n        # Long diff: delta_t - delta_{ref} = 0.3*(t - ref)\n        cs_coefs[i] = 0.3 * (p - ref) + np.random.normal(0, 0.12)\n\n# 3. Imputation (BJS): residuals from counterfactual model\n# Pre: Y - Y_hat(0), where Y_hat(0) is fitted on untreated obs\n# If model is well-specified, pre should be ~0\n# But with a trend violation, pre shows the trend vs the model's \"average\"\navg_delta_pre = np.mean(delta_true[periods_sim &lt; 0])\nbjs_coefs = delta_true - avg_delta_pre + noise()\nbjs_coefs[periods_sim == ref] = 0.0\n\nfig, axes = plt.subplots(1, 3, figsize=(18, 5.5), sharey=True)\ncolors = ['#2563eb', '#dc2626', '#16a34a']\ntitles = [\n    'TWFE\\n(long diffs: pre + post)',\n    \"CS (varying base)\\n(short diffs pre, long diffs post)\",\n    'Imputation (BJS)\\n(residuals from counterfactual)',\n]\ncoefs_list = [twfe_coefs, cs_coefs, bjs_coefs]\n\nfor ax, coefs, title, color in zip(axes, coefs_list, titles, colors):\n    ax.fill_between(periods_sim, coefs - 1.96*se_sim, coefs + 1.96*se_sim,\n                    alpha=0.2, color=color)\n    ax.plot(periods_sim, coefs, 'o-', color=color, markersize=7, linewidth=2, zorder=3)\n    ax.axhline(0, color='gray', linestyle='--', linewidth=1)\n    ax.axvline(ref + 0.5, color='gray', linestyle=':', linewidth=1.5, alpha=0.7)\n    ax.set_title(title, fontsize=11, fontweight='bold')\n    ax.set_xlabel('Period Relative to Treatment')\n    if ax == axes[0]:\n        ax.set_ylabel('Estimated Coefficient')\n    ax.grid(True, alpha=0.2)\n\nfig.suptitle('TRUE EFFECT = 0 everywhere. Only a linear pre-trend violation exists.',\n             fontsize=13, y=1.03, style='italic')\nplt.tight_layout()\nplt.show()\n\nprint('What you see:')\nprint('  TWFE:        smooth upward line through zero, no break at treatment')\nprint('               --&gt; correctly suggests \"just a trend, no treatment effect\"')\nprint()\nprint('  CS (varying): flat pre-trends (~0.3 each) then sudden jump post-treatment')\nprint('               --&gt; LOOKS like a treatment effect, but it is an artifact!')\nprint('               --&gt; short diffs flatten the linear trend into a constant')\nprint('               --&gt; then post-treatment switches to long diffs = kink')\nprint()\nprint('  BJS:         upward pre-trend, then continues post-treatment')\nprint('               --&gt; shows the trend but normalized differently')\n\n\n\n9.3. Prototype: Auto-Annotation for Multi-Model Plots\nGiven these comparability issues, our ggiplot() should automatically: 1. Detect which estimators are being compared 2. Show a small text box on the plot with each model’s reference period 3. Print warnings when issues are detected (asymmetric construction, different references)\n\ndef format_model_annotation(model_dict):\n    \"\"\"\n    Generate a compact annotation string for multi-model plots.\n    Shows the reference period and construction type for each model.\n    \"\"\"\n    lines = []\n    for name, results in model_dict.items():\n        meta = get_estimator_metadata(results)\n        ref = meta['reference_period']\n        sym = 'symmetric' if meta['symmetric'] else 'ASYMMETRIC'\n        if meta['symmetric'] is None:\n            sym = 'N/A'\n        lines.append(f'{name}: ref={ref}, {sym}')\n    return '\\n'.join(lines)\n\n\ndef ggiplot_multi_annotated(\n    model_dict,\n    *,\n    geom_style='ribbon',\n    ci_level=0.95,\n    reference_period=None,\n    annotate='auto',  # 'auto', 'plot', 'print', 'both', None\n    figsize=(13, 6),\n    title='Multi-Model Event Study',\n    ax=None,\n    show=True,\n):\n    \"\"\"\n    Multi-model event study plot with automatic comparability annotations.\n    \n    Parameters\n    ----------\n    model_dict : dict\n        {name: (results_or_df, se_or_None)} or {name: DataFrame}\n    annotate : str\n        'auto' - print warnings if issues detected, annotate plot with ref periods\n        'plot' - only annotate on the plot\n        'print' - only print warnings\n        'both' - both\n        None - no annotations\n    \"\"\"\n    if ax is None:\n        fig, ax = plt.subplots(figsize=figsize)\n    \n    colors = ['#2563eb', '#dc2626', '#16a34a', '#f59e0b', '#8b5cf6']\n    z = scipy_stats.norm.ppf(1 - (1 - ci_level) / 2)\n    \n    # Run comparability check\n    report = check_multi_model_comparability(model_dict)\n    has_issues = len(report['warnings']) &gt; 0\n    \n    # Determine annotation mode\n    if annotate == 'auto':\n        annotate = 'both' if has_issues else 'plot'\n    \n    # Plot each model\n    n_models = len(model_dict)\n    dodge_width = 0.15\n    annotation_lines = []\n    \n    for idx, (name, results) in enumerate(model_dict.items()):\n        color = colors[idx % len(colors)]\n        meta = get_estimator_metadata(results)\n        \n        # Get data (handle both DataFrames and result objects)\n        if isinstance(results, pd.DataFrame):\n            df = results.copy()\n            if 'effect' in df.columns:\n                df = df.rename(columns={'effect': 'estimate'})\n            periods_m = df['period'].values\n            estimates_m = df['estimate'].values\n            se_m = df['se'].values\n        else:\n            # Try to extract from result object\n            td = iplot_data(results, ci_level=ci_level, reference_period=reference_period)\n            periods_m = td['period'].values\n            estimates_m = td['estimate'].values\n            se_m = td['se'].values\n        \n        offset = (idx - (n_models - 1) / 2) * dodge_width\n        x = periods_m + offset\n        \n        if geom_style == 'ribbon':\n            ax.fill_between(x, estimates_m - z*se_m, estimates_m + z*se_m,\n                            alpha=0.15, color=color)\n            ax.plot(x, estimates_m, 'o-', color=color, markersize=6,\n                    linewidth=1.5, label=name, zorder=3)\n        else:\n            ax.errorbar(x, estimates_m, yerr=z*se_m,\n                        fmt='o', color=color, capsize=3, markersize=6,\n                        linewidth=1.2, label=name)\n        \n        # Build annotation\n        ref_str = meta['reference_period'] or '?'\n        sym_str = ''\n        if meta['symmetric'] is False:\n            sym_str = ' [ASYMMETRIC]'\n        annotation_lines.append(f'{name}: ref={ref_str}{sym_str}')\n    \n    ax.axhline(0, color='gray', linestyle='--', linewidth=1)\n    if reference_period is not None:\n        ax.axvline(reference_period + 0.5, color='gray', linestyle=':', linewidth=1.5, alpha=0.7)\n    \n    ax.set_title(title, fontsize=13)\n    ax.set_xlabel('Period Relative to Treatment', fontsize=11)\n    ax.set_ylabel('Treatment Effect', fontsize=11)\n    ax.legend(fontsize=9)\n    ax.grid(True, alpha=0.2)\n    \n    # Annotation on the plot\n    if annotate in ('plot', 'both'):\n        note_text = 'Base reference:\\n' + '\\n'.join(annotation_lines)\n        ax.text(0.02, 0.02, note_text,\n                transform=ax.transAxes, fontsize=7.5,\n                verticalalignment='bottom',\n                bbox=dict(boxstyle='round,pad=0.4', facecolor='lightyellow',\n                          edgecolor='orange', alpha=0.85),\n                family='monospace')\n    \n    # Print warnings\n    if annotate in ('print', 'both') and has_issues:\n        print('=' * 60)\n        print('MULTI-MODEL COMPARISON WARNINGS')\n        print('=' * 60)\n        for w in report['warnings']:\n            print(f'  ! {w}')\n        if report['notes']:\n            print()\n            for n in report['notes']:\n                print(f'  * {n}')\n        print('=' * 60)\n    \n    if show:\n        plt.tight_layout()\n        plt.show()\n    \n    return ax\n\n\n# Demo: multi-model plot with annotations\n# Using synthetic DataFrames to simulate different estimators\n\nnp.random.seed(42)\nbase = np.array([0.2, -0.1, 0.05, 0.0, 0.0, 2.5, 3.1, 3.8, 4.2])\nperiods_m = np.arange(-4, 5)\n\n# Attach fake result types to DataFrames for metadata extraction\nclass CSVaryingDF(pd.DataFrame):\n    base_period = 'varying'\nCSVaryingDF.__name__ = 'CallawaySantAnnaResults'\n\nclass SADF(pd.DataFrame):\n    pass\nSADF.__name__ = 'SunAbrahamResults'\n\nclass ImpDF(pd.DataFrame):\n    pass\nImpDF.__name__ = 'ImputationDiDResults'\n\nmodel_results_annotated = {\n    \"CS (varying)\": CSVaryingDF({\n        'period': periods_m,\n        'estimate': base + np.random.normal(0, 0.2, 9),\n        'se': np.abs(np.random.normal(0.35, 0.05, 9)),\n    }),\n    'Sun-Abraham': SADF({\n        'period': periods_m,\n        'estimate': base + np.random.normal(0, 0.15, 9),\n        'se': np.abs(np.random.normal(0.3, 0.05, 9)),\n    }),\n    'Imputation': ImpDF({\n        'period': periods_m,\n        'estimate': base + np.random.normal(0, 0.1, 9),\n        'se': np.abs(np.random.normal(0.25, 0.05, 9)),\n    }),\n}\n\nggiplot_multi_annotated(\n    model_results_annotated,\n    geom_style='errorbar',\n    reference_period=-1,\n    annotate='both',\n    title='Multi-Model Comparison with Auto-Annotations',\n)\n\n\n\n9.4. Design Decision: How to Annotate\n\n\n\n\n\n\n\n\n\nApproach\nPros\nCons\nRecommendation\n\n\n\n\nText box on plot\nAlways visible, travels with saved figure\nCan be cluttered\nUse for reference period info (compact)\n\n\nPrinted warning\nMore detail, doesn’t clutter\nUser might miss it\nUse for construction-type warnings\n\n\nannotate='auto'\nQuiet when safe, loud when dangerous\nMight suppress info\nDefault behavior\n\n\n\nOur recommendation: annotate='auto' as default: - Always show a small text box with reference period per model - Print detailed warnings only when issues are detected (asymmetric construction, different references, etc.) - Return the comparability report dict so users can inspect programmatically\nThis is a genuinely novel contribution – no existing tool (ggfixest, did2s, fixest) provides any form of automatic comparability checking for multi-model event study plots."
  },
  {
    "objectID": "notebooks/honest_inference_exploration.html#freyaldenhoven-et-al.-plot-featuresthree-specific-recommendations-from-freyaldenhoven-et-al.-2021-that-we-should-add-as-options-10.1.-parenthetical-outcome-label-suggestion-2the-y-axis-label-0-41.94-means-the-normalized-coefficient-of-0-corresponds-to-an-average-outcome-of-41.94-in-levels.-this-helps-readers-interpret-the-economic-magnitude-of-the-effects.for-example-if-hatbeta_3--0.8-and-the-baseline-is-41.94-then-the-treatment-reduced-the-outcome-by-approximately-0.841.94-approx-1.9-by-period-3.-10.2.-joint-wald-test-for-pre-trendsinstead-of-eyeballing-whether-pre-treatment-coefficients-are-close-to-zero-run-a-formal-joint-testh_0-beta_-k-beta_-k1-cdots-beta_-2-0this-is-an-f-test-wald-test-using-the-joint-variance-covariance-matrix.diff-diff-status-imputationdid-already-has-pretrend_test-returning-an-f-stat-and-p-value.-but-its-not-available-for-other-estimators-or-as-a-general-utility.-10.3.-leveling-off-testtests-whether-the-post-treatment-effects-are-constant-over-timeh_0-beta_0-beta_1-cdots-beta_kif-we-fail-to-reject-we-can-summarize-with-a-single-average-att.-the-smoothest-path-constant-effects-p-value-in-the-third-figure-from-the-paper-shows-this.diff-diff-status-not-implemented-anywhere.",
    "href": "notebooks/honest_inference_exploration.html#freyaldenhoven-et-al.-plot-featuresthree-specific-recommendations-from-freyaldenhoven-et-al.-2021-that-we-should-add-as-options-10.1.-parenthetical-outcome-label-suggestion-2the-y-axis-label-0-41.94-means-the-normalized-coefficient-of-0-corresponds-to-an-average-outcome-of-41.94-in-levels.-this-helps-readers-interpret-the-economic-magnitude-of-the-effects.for-example-if-hatbeta_3--0.8-and-the-baseline-is-41.94-then-the-treatment-reduced-the-outcome-by-approximately-0.841.94-approx-1.9-by-period-3.-10.2.-joint-wald-test-for-pre-trendsinstead-of-eyeballing-whether-pre-treatment-coefficients-are-close-to-zero-run-a-formal-joint-testh_0-beta_-k-beta_-k1-cdots-beta_-2-0this-is-an-f-test-wald-test-using-the-joint-variance-covariance-matrix.diff-diff-status-imputationdid-already-has-pretrend_test-returning-an-f-stat-and-p-value.-but-its-not-available-for-other-estimators-or-as-a-general-utility.-10.3.-leveling-off-testtests-whether-the-post-treatment-effects-are-constant-over-timeh_0-beta_0-beta_1-cdots-beta_kif-we-fail-to-reject-we-can-summarize-with-a-single-average-att.-the-smoothest-path-constant-effects-p-value-in-the-third-figure-from-the-paper-shows-this.diff-diff-status-not-implemented-anywhere.",
    "title": "Honest Inference for Event Study Plots",
    "section": "10. Freyaldenhoven et al. Plot FeaturesThree specific recommendations from Freyaldenhoven et al. (2021) that we should add as options:### 10.1. Parenthetical Outcome Label (Suggestion 2)The y-axis label “0 (41.94)” means: the normalized coefficient of 0 corresponds to an average outcome of 41.94 in levels. This helps readers interpret the economic magnitude of the effects.For example, if \\(\\hat{\\beta}_3 = -0.8\\) and the baseline is 41.94, then the treatment reduced the outcome by approximately \\(0.8/41.94 \\approx 1.9\\%\\) by period 3.### 10.2. Joint Wald Test for Pre-TrendsInstead of eyeballing whether pre-treatment coefficients are “close to zero”, run a formal joint test:\\[H_0: \\beta_{-K} = \\beta_{-K+1} = \\cdots = \\beta_{-2} = 0\\]This is an F-test (Wald test) using the joint variance-covariance matrix.diff-diff status: ImputationDiD already has pretrend_test() returning an F-stat and p-value. But it’s not available for other estimators or as a general utility.### 10.3. Leveling-Off TestTests whether the post-treatment effects are constant over time:\\[H_0: \\beta_0 = \\beta_1 = \\cdots = \\beta_K\\]If we fail to reject, we can summarize with a single average ATT. The “smoothest path” + “Constant effects p-value” in the third figure from the paper shows this.diff-diff status: Not implemented anywhere.",
    "text": "10. Freyaldenhoven et al. Plot FeaturesThree specific recommendations from Freyaldenhoven et al. (2021) that we should add as options:### 10.1. Parenthetical Outcome Label (Suggestion 2)The y-axis label “0 (41.94)” means: the normalized coefficient of 0 corresponds to an average outcome of 41.94 in levels. This helps readers interpret the economic magnitude of the effects.For example, if \\(\\hat{\\beta}_3 = -0.8\\) and the baseline is 41.94, then the treatment reduced the outcome by approximately \\(0.8/41.94 \\approx 1.9\\%\\) by period 3.### 10.2. Joint Wald Test for Pre-TrendsInstead of eyeballing whether pre-treatment coefficients are “close to zero”, run a formal joint test:\\[H_0: \\beta_{-K} = \\beta_{-K+1} = \\cdots = \\beta_{-2} = 0\\]This is an F-test (Wald test) using the joint variance-covariance matrix.diff-diff status: ImputationDiD already has pretrend_test() returning an F-stat and p-value. But it’s not available for other estimators or as a general utility.### 10.3. Leveling-Off TestTests whether the post-treatment effects are constant over time:\\[H_0: \\beta_0 = \\beta_1 = \\cdots = \\beta_K\\]If we fail to reject, we can summarize with a single average ATT. The “smoothest path” + “Constant effects p-value” in the third figure from the paper shows this.diff-diff status: Not implemented anywhere.\n\ndef pretrend_wald_test(estimates, vcov, pre_indices):    \"\"\"    Joint Wald F-test for pre-treatment coefficients = 0.        H0: beta_{pre} = 0 (parallel trends hold)        Parameters    ----------    estimates : array-like (K,)        All event study coefficients.    vcov : array-like (K, K)        Variance-covariance matrix.    pre_indices : array-like        Indices of pre-treatment periods (excluding reference).        Returns    -------    dict with f_stat, p_value, df_num, df_denom    \"\"\"    gamma = np.array(estimates)[pre_indices]    V = np.array(vcov)[np.ix_(pre_indices, pre_indices)]    n = len(gamma)        # Wald statistic: gamma' V^{-1} gamma    V_inv = np.linalg.inv(V)    wald_stat = gamma @ V_inv @ gamma    f_stat = wald_stat / n        # F-distribution p-value (conservative: use n as df_denom)    # In practice, df_denom depends on clustering; here we use chi2 / n    p_value = scipy_stats.chi2.sf(wald_stat, df=n)        return {        'f_stat': f_stat,        'wald_stat': wald_stat,        'p_value': p_value,        'df': n,        'pre_coefficients': gamma,    }def leveling_off_test(estimates, vcov, post_indices):    \"\"\"    Test whether post-treatment effects are constant (leveling off).        H0: beta_0 = beta_1 = ... = beta_K (all post-treatment effects equal)        Implemented as a Wald test on the contrasts:    beta_1 - beta_0 = 0, beta_2 - beta_0 = 0, ..., beta_K - beta_0 = 0        Parameters    ----------    estimates : array-like (K,)        All event study coefficients.    vcov : array-like (K, K)        Variance-covariance matrix.    post_indices : array-like        Indices of post-treatment periods.        Returns    -------    dict with f_stat, p_value, df    \"\"\"    betas = np.array(estimates)[post_indices]    V_post = np.array(vcov)[np.ix_(post_indices, post_indices)]    n_post = len(betas)        if n_post &lt; 2:        return {'f_stat': np.nan, 'p_value': np.nan, 'df': 0}        # Contrast matrix R: each row tests beta_j - beta_0 = 0    # R is (n_post-1) x n_post    R = np.zeros((n_post - 1, n_post))    for j in range(n_post - 1):        R[j, 0] = -1        R[j, j + 1] = 1        # Contrasts    r = R @ betas  # should be ~0 under H0    V_r = R @ V_post @ R.T  # variance of contrasts        # Wald statistic    V_r_inv = np.linalg.inv(V_r)    wald_stat = r @ V_r_inv @ r    df = n_post - 1    p_value = scipy_stats.chi2.sf(wald_stat, df=df)        return {        'f_stat': wald_stat / df,        'wald_stat': wald_stat,        'p_value': p_value,        'df': df,        'contrasts': r,    }# Demo with our synthetic data# Use the existing es_df and vcov_full from earlierest_arr = es_df['estimate'].valuesper_arr = es_df['period'].valuespre_idx = np.where((per_arr &lt; -1))[0]  # exclude referencepost_idx = np.where(per_arr &gt;= 0)[0]pre_test = pretrend_wald_test(est_arr, vcov_full, pre_idx)lev_test = leveling_off_test(est_arr, vcov_full, post_idx)print('=== Joint Pre-Trend Test ===')print(f'  H0: all pre-treatment coefficients = 0')print(f'  Wald stat = {pre_test[\"wald_stat\"]:.3f}, df = {pre_test[\"df\"]}')print(f'  p-value = {pre_test[\"p_value\"]:.4f}')print(f'  Pre-coefficients: {pre_test[\"pre_coefficients\"].round(3)}')print()print('=== Leveling-Off Test ===')print(f'  H0: all post-treatment effects are equal')print(f'  Wald stat = {lev_test[\"wald_stat\"]:.3f}, df = {lev_test[\"df\"]}')print(f'  p-value = {lev_test[\"p_value\"]:.4f}')\n\n\n10.4. Prototype: Freyaldenhoven-Style Event Study PlotPutting it all together: parenthetical outcome label, sup-t bands, p-values below the plot, and optional smoothest path with constant-effects test.\n\ndef ggiplot_freyaldenhoven(    results,    *,    ci_level=0.95,    vcov=None,    reference_period=None,    # Freyaldenhoven features    baseline_outcome=None,       # average Y at reference period (for parenthetical label)    show_supt=True,              # show sup-t bands alongside pointwise    show_pretrend_pval=True,     # print pre-trend Wald test p-value    show_leveloff_pval=True,     # print leveling-off test p-value    show_smoothest_path=False,   # overlay smoothest confounding path    show_constant_fit=False,     # overlay constant effects fit (step function)    # Aesthetics    color='#006400',             # dark green (like the Freyaldenhoven plots)    figsize=(10, 6.5),    title='Event Study',    xlabel='Event time',    ylabel='Coefficient',    ax=None,    show=True,):    \"\"\"    Event study plot following Freyaldenhoven et al. (2021) recommendations.        Features:    - Parenthetical outcome level label on y-axis: \"0 (41.94)\"    - Pointwise CIs + sup-t simultaneous bands    - Pre-trend Wald test p-value below the plot    - Leveling-off test p-value below the plot    - Optional smoothest confounding path overlay    - Optional constant-effects step function overlay    \"\"\"    if ax is None:        fig, ax = plt.subplots(figsize=figsize)    else:        fig = ax.get_figure()        # Extract data    if isinstance(results, pd.DataFrame):        df = results.copy()        if 'effect' in df.columns:            df = df.rename(columns={'effect': 'estimate'})        periods = df['period'].values        estimates = df['estimate'].values        se_vals = df['se'].values    else:        raise TypeError('Pass a DataFrame with period, estimate, se columns')        ref = reference_period if reference_period is not None else -1    z_pw = scipy_stats.norm.ppf(1 - (1 - ci_level) / 2)        pre_mask = periods &lt; ref    post_mask = periods &gt;= 0    pre_idx = np.where(pre_mask)[0]    post_idx = np.where(post_mask)[0]        # --- Pointwise CIs (thinner, inner) ---    ci_pw_low = estimates - z_pw * se_vals    ci_pw_high = estimates + z_pw * se_vals        ax.errorbar(periods, estimates, yerr=[estimates - ci_pw_low, ci_pw_high - estimates],                fmt='o', color=color, capsize=3, markersize=7, linewidth=1.2,                capthick=1.0, zorder=3, label=f'{ci_level:.0%} Pointwise CI')        # --- Sup-t bands (wider, outer) ---    if show_supt and vcov is not None:        c_supt = compute_supt_critical_value(vcov, alpha=1-ci_level)        ci_supt_low = estimates - c_supt * se_vals        ci_supt_high = estimates + c_supt * se_vals                # Draw sup-t as wider caps (like the Freyaldenhoven paper style)        for i, p in enumerate(periods):            ax.plot([p, p], [ci_supt_low[i], ci_supt_high[i]],                    color='gray', linewidth=0.8, zorder=2)            ax.plot([p - 0.15, p + 0.15], [ci_supt_low[i], ci_supt_low[i]],                    color='gray', linewidth=0.8, zorder=2)            ax.plot([p - 0.15, p + 0.15], [ci_supt_high[i], ci_supt_high[i]],                    color='gray', linewidth=0.8, zorder=2)        # Invisible line for legend        ax.plot([], [], color='gray', linewidth=0.8, label=f'{ci_level:.0%} Sup-t band')        # --- Smoothest confounding path ---    if show_smoothest_path:        pre_est_mask = (periods &lt; ref) & (periods != ref)        if pre_est_mask.sum() &gt;= 2:            sp_periods, sp_path = compute_smoothest_path(                estimates[pre_est_mask], periods[pre_est_mask],                periods[post_mask], ref_period=ref            )            ax.plot(sp_periods, sp_path, '-', color='#003366', linewidth=3,                    alpha=0.8, label='Smoothest path', zorder=4)        # --- Constant effects fit (step function) ---    if show_constant_fit and post_mask.any():        mean_post = estimates[post_mask].mean()        # Step function: 0 for pre, mean_post for post        ax.plot([periods[pre_mask].min() - 0.5, ref + 0.5], [0, 0],                '-', color='#003366', linewidth=3, alpha=0.8, zorder=4)        ax.plot([ref + 0.5, ref + 0.5], [0, mean_post],                '-', color='#003366', linewidth=3, alpha=0.8, zorder=4)        ax.plot([ref + 0.5, periods[post_mask].max() + 0.5], [mean_post, mean_post],                '-', color='#003366', linewidth=3, alpha=0.8,                label=f'Constant effect = {mean_post:.2f}', zorder=4)        # --- Reference lines ---    ax.axhline(0, color='#006400', linestyle='--', linewidth=1.2, alpha=0.6)        # --- Parenthetical outcome label ---    if baseline_outcome is not None:        ax.set_ylabel(ylabel, fontsize=11)        # Add the baseline label at y=0        ax.annotate(f'0 ({baseline_outcome:.2f})',                    xy=(periods.min() - 1.2, 0),                    fontsize=9, color='#006400', ha='right', va='center')    else:        ax.set_ylabel(ylabel, fontsize=11)        # --- P-values below the plot ---    pval_texts = []    if show_pretrend_pval and vcov is not None and len(pre_idx) &gt;= 2:        pt = pretrend_wald_test(estimates, vcov, pre_idx)        pval_texts.append(f'Pretrends p-value = {pt[\"p_value\"]:.2f}')        if show_leveloff_pval and vcov is not None and len(post_idx) &gt;= 2:        lt = leveling_off_test(estimates, vcov, post_idx)        pval_texts.append(f'Leveling off p-value = {lt[\"p_value\"]:.2f}')        if show_constant_fit and vcov is not None and len(post_idx) &gt;= 2:        ct = leveling_off_test(estimates, vcov, post_idx)        # Replace leveling off with constant effects label        pval_texts = [t for t in pval_texts if 'Leveling' not in t]        pval_texts.append(f'Constant effects p-value = {ct[\"p_value\"]:.2f}')        if pval_texts:        pval_str = '  --  '.join(pval_texts)        ax.text(0.5, -0.12, pval_str, transform=ax.transAxes,                fontsize=9, ha='center', va='top', style='italic')        ax.set_title(title, fontsize=13)    ax.set_xlabel(xlabel, fontsize=11)    ax.legend(fontsize=9, loc='best')    ax.grid(True, alpha=0.15)        if show:        plt.tight_layout()        plt.show()        return axprint('ggiplot_freyaldenhoven() defined')\n\n\n# Demo 1: \"Smooth\" event-time trend (like Figure (a) in the paper)# Pointwise + sup-t bands, with p-values belowggiplot_freyaldenhoven(    es_df,    vcov=vcov_full,    reference_period=-1,    baseline_outcome=41.94,    show_supt=True,    show_pretrend_pval=True,    show_leveloff_pval=True,    title='\"Smooth\" event-time trend (Freyaldenhoven style)',)\n\n\n# Demo 2: With smoothest path + constant effects step function# (like the third figure from the paper)ggiplot_freyaldenhoven(    es_df,    vcov=vcov_full,    reference_period=-1,    baseline_outcome=5.12,    show_supt=True,    show_smoothest_path=True,    show_constant_fit=True,    show_pretrend_pval=False,    show_leveloff_pval=False,    title='Smoothest path + Constant effects fit (Freyaldenhoven style)',)\n\n\n# Demo 3: Side by side — all three Freyaldenhoven figure stylesfig, axes = plt.subplots(1, 3, figsize=(20, 6), sharey=True)# (a) Smooth event-time trend: pointwise + sup-tggiplot_freyaldenhoven(    es_df, vcov=vcov_full, reference_period=-1,    baseline_outcome=41.94,    show_supt=True, show_pretrend_pval=True, show_leveloff_pval=True,    title='(a) Pointwise + Sup-t\\n+ p-values',    ax=axes[0], show=False,)# (b) Jump at event: same but different datanp.random.seed(99)jump_est = np.where(es_df['period'] &gt;= 0, 0.5, 0) + np.random.normal(0, 0.15, len(es_df))jump_est[es_df['period'].values == -1] = 0.0jump_df = es_df.copy()jump_df['estimate'] = jump_estggiplot_freyaldenhoven(    jump_df, vcov=vcov_full, reference_period=-1,    baseline_outcome=41.94,    show_supt=True, show_pretrend_pval=True, show_leveloff_pval=True,    title='(b) \"Jump\" at event\\n+ p-values',    ax=axes[1], show=False,)# (c) Smoothest path + constant effectsggiplot_freyaldenhoven(    jump_df, vcov=vcov_full, reference_period=-1,    baseline_outcome=5.12,    show_supt=True, show_smoothest_path=True, show_constant_fit=True,    show_pretrend_pval=False, show_leveloff_pval=False,    title='(c) Smoothest path\\n+ constant effects fit',    ax=axes[2], show=False,)plt.tight_layout()plt.subplots_adjust(bottom=0.15)plt.show()print('These replicate the three figure styles from Freyaldenhoven et al. (2021).')\n\n\n\n10.5. What diff-diff Already Has vs. What We Need| Feature | diff-diff status | Our contribution ||———|—————–|——————|| Parenthetical outcome label | Not available | baseline_outcome param in ggiplot() || Pre-trend Wald test | Only in ImputationDiD.pretrend_test() | Generalize to pretrend_wald_test() for any estimator || Leveling-off test | Not available | leveling_off_test() — new || Constant effects p-value | Not available | leveling_off_test() + step function overlay || Sup-t bands on same plot | Not available | show_supt=True — outer bands in gray || Smoothest path overlay | Not available | show_smoothest_path=True || VCV matrix | Stored in MultiPeriodDiDResults.vcov | Already available for real results |Key design choice: The Freyaldenhoven-style features should be composable options in our ggiplot(), not a separate function. The ggiplot_freyaldenhoven() above is a prototype — in the final version, these would be parameters of the unified ggiplot()."
  },
  {
    "objectID": "notebooks/honest_inference_exploration.html#comprehensive-roadmap-contribution-architecturethe-contribution-to-diff-diff-spans-four-domains-a-plotting-improvements-b-honest-inference-c-multi-model-fairness-and-d-freyaldenhoven-best-practices.diff-diff-contribution---a.-plotting-from-plotting_exploration-notebook----iplot_data-ggiplot----geom_style-ci_level-multi-model-aggr_eff---b.-honest-inference-this-notebook-sections-3-8----sup-t-simultaneous-bands----smoothest-confounding-path----functional-scbs-fdid----integration-with-existing-honestdid---c.-multi-model-fairness-section-9-novel----get_estimator_metadata----check_multi_model_comparability----auto-annotation-annotateauto----kink-artifact-demo---d.-freyaldenhoven-best-practices-section-10----parenthetical-outcome-label-baseline_outcome----joint-pre-trend-wald-test-generalized----leveling-off-test-new----constant-effects-step-function-overlay----sup-t-bands-alongside-pointwise-implementation-priority-updated-priority-feature-source-difficulty-impact---1-iplot_data-with-ci_type-all-low-high-2-ggiplot-with-geom_style-ggfixest-medium-high-3-sup-t-simultaneous-bands-eventstudyr-low-high-4-multi-model-comparability-check-roth-2026-low-high-5-auto-annotation-for-multi-model-novel-medium-high-6-generalized-pre-trend-wald-test-freyaldenhoven-low-high-7-leveling-off-test-freyaldenhoven-low-medium-8-parenthetical-outcome-label-freyaldenhoven-low-medium-9-smoothest-confounding-path-eventstudyr-medium-medium-10-constant-effects-step-function-freyaldenhoven-low-medium-11-functional-scbs-infsup-fdid-medium-high-12-equivalence-region-overlay-fdid-low-medium-13-multi-model-comparison-dodgefacet-ggfixest-medium-high-14-aggregate-effects-overlay-ggfixest-low-medium-15-honestdid-integration-diff-diff-medium-medium-16-roth-2026-kink-artifact-demo-paper-low-medium-vcov-requirements-feature-needs-vcov-diff-diff-status---pointwise-cis-no-se-only-default-sup-t-bands-yes-multiperioddidresults.vcov-available-fdid-scbs-yes-same-pre-trend-wald-test-yes-available-interaction_indices-leveling-off-test-yes-available-smoothest-path-no-uses-point-estimates-only-honestdid-sensitivity-no-uses-se-already-supported-multi-model-check-no-uses-result-object-metadata-file-structure-for-prdiff_diff-plotting-__init__.py-iplot_data.py-tidy-data-extraction-ci_type-ggiplot.py-enhanced-event-study-plot-unified-honest_bands.py-sup-t-scbs-smoothest-path-multi_model.py-comparability-check-annotation-tests_on_plot.py-wald-tests-leveling-off-p-values-utils.py-shared-utilities-tests-test_iplot_data.py-test_ggiplot.py-test_honest_bands.py-test_multi_model.py-test_tests_on_plot.py",
    "href": "notebooks/honest_inference_exploration.html#comprehensive-roadmap-contribution-architecturethe-contribution-to-diff-diff-spans-four-domains-a-plotting-improvements-b-honest-inference-c-multi-model-fairness-and-d-freyaldenhoven-best-practices.diff-diff-contribution---a.-plotting-from-plotting_exploration-notebook----iplot_data-ggiplot----geom_style-ci_level-multi-model-aggr_eff---b.-honest-inference-this-notebook-sections-3-8----sup-t-simultaneous-bands----smoothest-confounding-path----functional-scbs-fdid----integration-with-existing-honestdid---c.-multi-model-fairness-section-9-novel----get_estimator_metadata----check_multi_model_comparability----auto-annotation-annotateauto----kink-artifact-demo---d.-freyaldenhoven-best-practices-section-10----parenthetical-outcome-label-baseline_outcome----joint-pre-trend-wald-test-generalized----leveling-off-test-new----constant-effects-step-function-overlay----sup-t-bands-alongside-pointwise-implementation-priority-updated-priority-feature-source-difficulty-impact---1-iplot_data-with-ci_type-all-low-high-2-ggiplot-with-geom_style-ggfixest-medium-high-3-sup-t-simultaneous-bands-eventstudyr-low-high-4-multi-model-comparability-check-roth-2026-low-high-5-auto-annotation-for-multi-model-novel-medium-high-6-generalized-pre-trend-wald-test-freyaldenhoven-low-high-7-leveling-off-test-freyaldenhoven-low-medium-8-parenthetical-outcome-label-freyaldenhoven-low-medium-9-smoothest-confounding-path-eventstudyr-medium-medium-10-constant-effects-step-function-freyaldenhoven-low-medium-11-functional-scbs-infsup-fdid-medium-high-12-equivalence-region-overlay-fdid-low-medium-13-multi-model-comparison-dodgefacet-ggfixest-medium-high-14-aggregate-effects-overlay-ggfixest-low-medium-15-honestdid-integration-diff-diff-medium-medium-16-roth-2026-kink-artifact-demo-paper-low-medium-vcov-requirements-feature-needs-vcov-diff-diff-status---pointwise-cis-no-se-only-default-sup-t-bands-yes-multiperioddidresults.vcov-available-fdid-scbs-yes-same-pre-trend-wald-test-yes-available-interaction_indices-leveling-off-test-yes-available-smoothest-path-no-uses-point-estimates-only-honestdid-sensitivity-no-uses-se-already-supported-multi-model-check-no-uses-result-object-metadata-file-structure-for-prdiff_diff-plotting-__init__.py-iplot_data.py-tidy-data-extraction-ci_type-ggiplot.py-enhanced-event-study-plot-unified-honest_bands.py-sup-t-scbs-smoothest-path-multi_model.py-comparability-check-annotation-tests_on_plot.py-wald-tests-leveling-off-p-values-utils.py-shared-utilities-tests-test_iplot_data.py-test_ggiplot.py-test_honest_bands.py-test_multi_model.py-test_tests_on_plot.py",
    "title": "Honest Inference for Event Study Plots",
    "section": "11. Comprehensive Roadmap### Contribution ArchitectureThe contribution to diff-diff spans four domains: (A) plotting improvements, (B) honest inference, (C) multi-model fairness, and (D) Freyaldenhoven best practices.diff-diff contribution|+-- A. PLOTTING (from plotting_exploration notebook)|   +-- iplot_data(), ggiplot()|   +-- geom_style, ci_level, multi-model, aggr_eff|+-- B. HONEST INFERENCE (this notebook, sections 3-8)|   +-- Sup-t simultaneous bands|   +-- Smoothest confounding path|   +-- Functional SCBs (fdid)|   +-- Integration with existing HonestDiD|+-- C. MULTI-MODEL FAIRNESS (section 9)  *** NOVEL ***|   +-- get_estimator_metadata()|   +-- check_multi_model_comparability()|   +-- Auto-annotation (annotate='auto')|   +-- Kink artifact demo|+-- D. FREYALDENHOVEN BEST PRACTICES (section 10)    +-- Parenthetical outcome label (baseline_outcome)    +-- Joint pre-trend Wald test (generalized)    +-- Leveling-off test (new)    +-- Constant effects step function overlay    +-- Sup-t bands alongside pointwise### Implementation Priority (Updated)| Priority | Feature | Source | Difficulty | Impact ||———-|———|——–|————|——–|| 1 | iplot_data() with ci_type | All | Low | HIGH || 2 | ggiplot() with geom_style | ggfixest | Medium | HIGH || 3 | Sup-t simultaneous bands | eventstudyr | Low | HIGH || 4 | Multi-model comparability check | Roth (2026) | Low | HIGH || 5 | Auto-annotation for multi-model | Novel | Medium | HIGH || 6 | Generalized pre-trend Wald test | Freyaldenhoven | Low | HIGH || 7 | Leveling-off test | Freyaldenhoven | Low | MEDIUM || 8 | Parenthetical outcome label | Freyaldenhoven | Low | MEDIUM || 9 | Smoothest confounding path | eventstudyr | Medium | MEDIUM || 10 | Constant effects step function | Freyaldenhoven | Low | MEDIUM || 11 | Functional SCBs (inf/sup) | fdid | Medium | HIGH || 12 | Equivalence region overlay | fdid | Low | MEDIUM || 13 | Multi-model comparison (dodge/facet) | ggfixest | Medium | HIGH || 14 | Aggregate effects overlay | ggfixest | Low | MEDIUM || 15 | HonestDiD integration | diff-diff | Medium | MEDIUM || 16 | Roth (2026) kink artifact demo | Paper | Low | MEDIUM |### vcov Requirements| Feature | Needs vcov? | diff-diff status ||———|————-|——————|| Pointwise CIs | No (SE only) | Default || Sup-t bands | Yes | MultiPeriodDiDResults.vcov available || fdid SCBs | Yes | Same || Pre-trend Wald test | Yes | Available + interaction_indices || Leveling-off test | Yes | Available || Smoothest path | No | Uses point estimates only || HonestDiD sensitivity | No (uses SE) | Already supported || Multi-model check | No | Uses result object metadata |### File Structure for PRdiff_diff/  plotting/    __init__.py    iplot_data.py          # Tidy data extraction + ci_type    ggiplot.py             # Enhanced event study plot (unified)    honest_bands.py        # Sup-t, SCBs, smoothest path    multi_model.py         # Comparability check + annotation    tests_on_plot.py       # Wald tests, leveling-off, p-values    utils.py               # Shared utilities  tests/    test_iplot_data.py    test_ggiplot.py    test_honest_bands.py    test_multi_model.py    test_tests_on_plot.py",
    "text": "11. Comprehensive Roadmap### Contribution ArchitectureThe contribution to diff-diff spans four domains: (A) plotting improvements, (B) honest inference, (C) multi-model fairness, and (D) Freyaldenhoven best practices.diff-diff contribution|+-- A. PLOTTING (from plotting_exploration notebook)|   +-- iplot_data(), ggiplot()|   +-- geom_style, ci_level, multi-model, aggr_eff|+-- B. HONEST INFERENCE (this notebook, sections 3-8)|   +-- Sup-t simultaneous bands|   +-- Smoothest confounding path|   +-- Functional SCBs (fdid)|   +-- Integration with existing HonestDiD|+-- C. MULTI-MODEL FAIRNESS (section 9)  *** NOVEL ***|   +-- get_estimator_metadata()|   +-- check_multi_model_comparability()|   +-- Auto-annotation (annotate='auto')|   +-- Kink artifact demo|+-- D. FREYALDENHOVEN BEST PRACTICES (section 10)    +-- Parenthetical outcome label (baseline_outcome)    +-- Joint pre-trend Wald test (generalized)    +-- Leveling-off test (new)    +-- Constant effects step function overlay    +-- Sup-t bands alongside pointwise### Implementation Priority (Updated)| Priority | Feature | Source | Difficulty | Impact ||———-|———|——–|————|——–|| 1 | iplot_data() with ci_type | All | Low | HIGH || 2 | ggiplot() with geom_style | ggfixest | Medium | HIGH || 3 | Sup-t simultaneous bands | eventstudyr | Low | HIGH || 4 | Multi-model comparability check | Roth (2026) | Low | HIGH || 5 | Auto-annotation for multi-model | Novel | Medium | HIGH || 6 | Generalized pre-trend Wald test | Freyaldenhoven | Low | HIGH || 7 | Leveling-off test | Freyaldenhoven | Low | MEDIUM || 8 | Parenthetical outcome label | Freyaldenhoven | Low | MEDIUM || 9 | Smoothest confounding path | eventstudyr | Medium | MEDIUM || 10 | Constant effects step function | Freyaldenhoven | Low | MEDIUM || 11 | Functional SCBs (inf/sup) | fdid | Medium | HIGH || 12 | Equivalence region overlay | fdid | Low | MEDIUM || 13 | Multi-model comparison (dodge/facet) | ggfixest | Medium | HIGH || 14 | Aggregate effects overlay | ggfixest | Low | MEDIUM || 15 | HonestDiD integration | diff-diff | Medium | MEDIUM || 16 | Roth (2026) kink artifact demo | Paper | Low | MEDIUM |### vcov Requirements| Feature | Needs vcov? | diff-diff status ||———|————-|——————|| Pointwise CIs | No (SE only) | Default || Sup-t bands | Yes | MultiPeriodDiDResults.vcov available || fdid SCBs | Yes | Same || Pre-trend Wald test | Yes | Available + interaction_indices || Leveling-off test | Yes | Available || Smoothest path | No | Uses point estimates only || HonestDiD sensitivity | No (uses SE) | Already supported || Multi-model check | No | Uses result object metadata |### File Structure for PRdiff_diff/  plotting/    __init__.py    iplot_data.py          # Tidy data extraction + ci_type    ggiplot.py             # Enhanced event study plot (unified)    honest_bands.py        # Sup-t, SCBs, smoothest path    multi_model.py         # Comparability check + annotation    tests_on_plot.py       # Wald tests, leveling-off, p-values    utils.py               # Shared utilities  tests/    test_iplot_data.py    test_ggiplot.py    test_honest_bands.py    test_multi_model.py    test_tests_on_plot.py"
  },
  {
    "objectID": "notebooks/honest_inference_exploration.html#summary-what-we-prototyped-16-features-across-4-domainsa.-plotting-from-previous-notebook--iplot_data-ggiplot-with-ribbonerrorbarpointrange-nested-cis-multi-model-aggregate-effectsb.-honest-inference1.-compute_supt_critical_value-simulation-based-sup-t-critical-values2.-compute_scb_critical_values-infimum-supremum-scbs-fdid3.-compute_smoothest_path-minimum-curvature-confounding-path4.-enhanced-iplot_data-with-ci_typepointwisesup-tfdid5.-enhanced-ggiplot-with-composable-honest-inference-layersc.-multi-model-fairness-novel6.-get_estimator_metadata-introspects-reference-period-construction-type-symmetry7.-check_multi_model_comparability-detects-mismatches-warns-automatically8.-ggiplot_multi_annotated-auto-annotated-multi-model-plots9.-roth-2026-kink-artifact-visual-demo10.-long-vs.-short-difference-definitions-concrete-numerical-exampled.-freyaldenhoven-best-practices11.-pretrend_wald_test-generalized-joint-pre-trend-f-test-diff-diff-only-has-it-for-imputation12.-leveling_off_test-post-treatment-constant-effects-wald-test-new13.-baseline_outcome-parenthetical-outcome-level-label-on-y-axis14.-constant-effects-step-function-overlay15.-sup-t-bands-drawn-alongside-pointwise-cis-outer-gray-bars16.-ggiplot_freyaldenhoven-replicates-all-three-figure-styles-from-the-paper-whats-genuinely-novel-no-r-or-python-package-does-this-feature-why-novel-multi-model-comparability-check-no-tool-warns-about-reference-period-construction-differences-auto-annotation-no-tool-annotates-which-base-period-each-estimator-uses-generalized-pre-trend-test-diff-diff-only-has-it-for-imputation-not-general-leveling-off-test-not-implemented-anywhere-in-diff-diff-composable-honest-inference-layers-no-python-package-combines-sup-t-smoothest-path-fdid-scbs-in-one-function-next-steps1.-run-this-notebook-end-to-end-to-verify-all-prototypes-work2.-check-if-diff-diff-exposes-vcov-from-callawaysantanna-and-sunabraham-not-just-multiperioddid3.-write-unit-tests-for-all-new-functions4.-integrate-all-features-into-one-unified-ggiplot-function5.-package-as-a-clean-pr-to-diff-diff",
    "href": "notebooks/honest_inference_exploration.html#summary-what-we-prototyped-16-features-across-4-domainsa.-plotting-from-previous-notebook--iplot_data-ggiplot-with-ribbonerrorbarpointrange-nested-cis-multi-model-aggregate-effectsb.-honest-inference1.-compute_supt_critical_value-simulation-based-sup-t-critical-values2.-compute_scb_critical_values-infimum-supremum-scbs-fdid3.-compute_smoothest_path-minimum-curvature-confounding-path4.-enhanced-iplot_data-with-ci_typepointwisesup-tfdid5.-enhanced-ggiplot-with-composable-honest-inference-layersc.-multi-model-fairness-novel6.-get_estimator_metadata-introspects-reference-period-construction-type-symmetry7.-check_multi_model_comparability-detects-mismatches-warns-automatically8.-ggiplot_multi_annotated-auto-annotated-multi-model-plots9.-roth-2026-kink-artifact-visual-demo10.-long-vs.-short-difference-definitions-concrete-numerical-exampled.-freyaldenhoven-best-practices11.-pretrend_wald_test-generalized-joint-pre-trend-f-test-diff-diff-only-has-it-for-imputation12.-leveling_off_test-post-treatment-constant-effects-wald-test-new13.-baseline_outcome-parenthetical-outcome-level-label-on-y-axis14.-constant-effects-step-function-overlay15.-sup-t-bands-drawn-alongside-pointwise-cis-outer-gray-bars16.-ggiplot_freyaldenhoven-replicates-all-three-figure-styles-from-the-paper-whats-genuinely-novel-no-r-or-python-package-does-this-feature-why-novel-multi-model-comparability-check-no-tool-warns-about-reference-period-construction-differences-auto-annotation-no-tool-annotates-which-base-period-each-estimator-uses-generalized-pre-trend-test-diff-diff-only-has-it-for-imputation-not-general-leveling-off-test-not-implemented-anywhere-in-diff-diff-composable-honest-inference-layers-no-python-package-combines-sup-t-smoothest-path-fdid-scbs-in-one-function-next-steps1.-run-this-notebook-end-to-end-to-verify-all-prototypes-work2.-check-if-diff-diff-exposes-vcov-from-callawaysantanna-and-sunabraham-not-just-multiperioddid3.-write-unit-tests-for-all-new-functions4.-integrate-all-features-into-one-unified-ggiplot-function5.-package-as-a-clean-pr-to-diff-diff",
    "title": "Honest Inference for Event Study Plots",
    "section": "Summary### What We Prototyped (16 features across 4 domains)A. Plotting (from previous notebook):- iplot_data(), ggiplot() with ribbon/errorbar/pointrange, nested CIs, multi-model, aggregate effectsB. Honest Inference:1. compute_supt_critical_value() – simulation-based sup-t critical values2. compute_scb_critical_values() – infimum + supremum SCBs (fdid)3. compute_smoothest_path() – minimum curvature confounding path4. Enhanced iplot_data() with ci_type='pointwise'|'sup-t'|'fdid'5. Enhanced ggiplot() with composable honest inference layersC. Multi-Model Fairness (Novel):6. get_estimator_metadata() – introspects reference period, construction type, symmetry7. check_multi_model_comparability() – detects mismatches, warns automatically8. ggiplot_multi_annotated() – auto-annotated multi-model plots9. Roth (2026) kink artifact visual demo10. Long vs. short difference definitions + concrete numerical exampleD. Freyaldenhoven Best Practices:11. pretrend_wald_test() – generalized joint pre-trend F-test (diff-diff only has it for Imputation)12. leveling_off_test() – post-treatment constant effects Wald test (new)13. baseline_outcome – parenthetical outcome level label on y-axis14. Constant effects step function overlay15. Sup-t bands drawn alongside pointwise CIs (outer gray bars)16. ggiplot_freyaldenhoven() – replicates all three figure styles from the paper### What’s Genuinely Novel (no R or Python package does this)| Feature | Why novel ||———|———–|| Multi-model comparability check | No tool warns about reference period / construction differences || Auto-annotation | No tool annotates which base period each estimator uses || Generalized pre-trend test | diff-diff only has it for Imputation, not general || Leveling-off test | Not implemented anywhere in diff-diff || Composable honest inference layers | No Python package combines sup-t + smoothest path + fdid SCBs in one function |### Next Steps1. Run this notebook end-to-end to verify all prototypes work2. Check if diff-diff exposes vcov from CallawaySantAnna and SunAbraham (not just MultiPeriodDiD)3. Write unit tests for all new functions4. Integrate all features into one unified ggiplot() function5. Package as a clean PR to diff-diff",
    "text": "Summary### What We Prototyped (16 features across 4 domains)A. Plotting (from previous notebook):- iplot_data(), ggiplot() with ribbon/errorbar/pointrange, nested CIs, multi-model, aggregate effectsB. Honest Inference:1. compute_supt_critical_value() – simulation-based sup-t critical values2. compute_scb_critical_values() – infimum + supremum SCBs (fdid)3. compute_smoothest_path() – minimum curvature confounding path4. Enhanced iplot_data() with ci_type='pointwise'|'sup-t'|'fdid'5. Enhanced ggiplot() with composable honest inference layersC. Multi-Model Fairness (Novel):6. get_estimator_metadata() – introspects reference period, construction type, symmetry7. check_multi_model_comparability() – detects mismatches, warns automatically8. ggiplot_multi_annotated() – auto-annotated multi-model plots9. Roth (2026) kink artifact visual demo10. Long vs. short difference definitions + concrete numerical exampleD. Freyaldenhoven Best Practices:11. pretrend_wald_test() – generalized joint pre-trend F-test (diff-diff only has it for Imputation)12. leveling_off_test() – post-treatment constant effects Wald test (new)13. baseline_outcome – parenthetical outcome level label on y-axis14. Constant effects step function overlay15. Sup-t bands drawn alongside pointwise CIs (outer gray bars)16. ggiplot_freyaldenhoven() – replicates all three figure styles from the paper### What’s Genuinely Novel (no R or Python package does this)| Feature | Why novel ||———|———–|| Multi-model comparability check | No tool warns about reference period / construction differences || Auto-annotation | No tool annotates which base period each estimator uses || Generalized pre-trend test | diff-diff only has it for Imputation, not general || Leveling-off test | Not implemented anywhere in diff-diff || Composable honest inference layers | No Python package combines sup-t + smoothest path + fdid SCBs in one function |### Next Steps1. Run this notebook end-to-end to verify all prototypes work2. Check if diff-diff exposes vcov from CallawaySantAnna and SunAbraham (not just MultiPeriodDiD)3. Write unit tests for all new functions4. Integrate all features into one unified ggiplot() function5. Package as a clean PR to diff-diff"
  },
  {
    "objectID": "posts/ab_test_bayesian/bayesian_ab_test.html",
    "href": "posts/ab_test_bayesian/bayesian_ab_test.html",
    "title": "Bayesian Approach to A/B Testing",
    "section": "",
    "text": "Bayesian A/B testing Analysis using Python.\nAs data scientists, we are frequently confronted with questions like: “Does X affects Y?” Here, Y is the outcome that we care about, while X could be a new feature, product, policy or experience. How do we go about answering such causal questions? For online platforms, the answer lies in experimentation.\nBy randomly assigning some kind of treatment (new interface design, new product, etc.) to a subset of users (Group A) and comparing their behavior or outcomes (such as revenue, visits, clicks, etc.) to those who did not receive the treatment (Group B), we can effectively isolate the causal impact of the change. This is at the core of A/B testing (also known as Randomized Control Trials), which is considered the workhorse method of experimentation for platforms like Spotify, Uber, Netflix, an more.\nIn my previous article, we learned the basics of A/B testing using Python, employing traditional traditional methods. However, one distinguishing feature of platforms like Spotify is their execution of thousands of simultaneous tests. With so many experiments available, is there a way to leverage information from previous tests and improve the inferences we make?\nIn this article, I aim to address this question by introducing the Bayesian approach to A/B testing. The Bayesian framework proves to be highly suitable for this task as it inherently facilitates the integration of prior knowledge with new data, enabling more robust conclusions. Let’s see how!"
  },
  {
    "objectID": "posts/ab_test_bayesian/bayesian_ab_test.html#classical-approaches-normal-distribution-vs-nonparametric-bootstrap",
    "href": "posts/ab_test_bayesian/bayesian_ab_test.html#classical-approaches-normal-distribution-vs-nonparametric-bootstrap",
    "title": "Bayesian Approach to A/B Testing",
    "section": "2.1. Classical Approaches: Normal Distribution vs Nonparametric Bootstrap",
    "text": "2.1. Classical Approaches: Normal Distribution vs Nonparametric Bootstrap\nWe want to know the uncertainty of the response rates p. One standard way to measure uncertainty of the possible values of the response rate is the classical framework of inference, which is to assume that p comes from a normal distribution.\nWe start by calculating the mean and standard deviation. Then, we’ll use those parameters to plot confidence intervals (CI) and get a sense of the lay of the land:\n\n\\text{CI}_{\\text{lower}} = \\bar{x} - 1.96 \\times \\frac{\\text{SE}_{\\bar{x}}}{\\sqrt{n}}\n\nIt’s classic statistical inference!\n\n# Calculate the mean\nxbar = df['respmail'].mean()\nprint(\"Sample Response Rate:\", xbar.round(3))\n\n# Calculate the standard error of the mean (SE)\nxbse = np.sqrt(df['respmail'].var() / len(df))\n\n# Calculate the confidence interval\nci_lower = xbar - 1.96 * xbse\nci_upper = xbar + 1.96 * xbse\n\nprint(\"95% Confidence Interval:\")\nprint(\"Lower bound:\", ci_lower.round(3))\nprint(\"Upper bound:\", ci_upper.round(3))\n\nSample Response Rate: 0.124\n95% Confidence Interval:\nLower bound: 0.118\nUpper bound: 0.131\n\n\nOur estimated response rate for the catalog campaign falls within a confidence interval ranging from 11.8% to 13.1% with 95% confidence. This means that, assuming normality, we’re pretty confident that the true response rate lies somewhere in this range. Now, let’s visualize these bounds:\n\n\nCode\n# Generate xx values\nxx = np.linspace(df['respmail'].min(), df['respmail'].max(), 10000)\n\n# Calculate normal density\nnorm_density = pd.DataFrame({'xx': xx, 'm': np.exp(-(xx - xbar) ** 2 / (2 * xbse ** 2)) / (xbse * np.sqrt(2 * np.pi))})\n\n# Plot using Seaborn with confidence interval\nplt.figure(figsize=(8, 6))\nsns.lineplot(data=norm_density, x='xx', y='m', color=\"#00A087B2\")\nplt.axvline(xbar, color=\"black\", linestyle=\"--\", label=\"Mean\")\nplt.axvline(ci_lower, color=\"#481f70\", linestyle=\"--\", label=\"95% CI Lower Bound\")\nplt.axvline(ci_upper, color=\"#481f70\", linestyle=\"--\", label=\"95% CI Upper Bound\")\nplt.xlim(0.108, 0.14)  # Limit x-axis range\nplt.xlabel(\"Response Rate (p)\")\nplt.ylabel(\"Density\")\nplt.title(\"Normal Distribution\")\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\nIf we don’t want to make any distributional assumptions, we can give the nonparametric bootstrap method a shot. This approach allows us to leverage bootstrap sampling techniques for robust estimates of response rates. Here is the code to do re-sampling with replacement:\n\n# Define the number of bootstrap samples\nB = 10000\n\n# Define a function to calculate response rate\ndef calculate_response_rate(data):\n    return data.mean()\n\n# Initialize an array to store bootstrap sample response rates\nbootstrap_response_rates = np.zeros(B)\n\n# Generate bootstrap samples and calculate response rates\nfor i in range(B):\n    bootstrap_sample = df['respmail'].sample(frac=1, replace=True)  # Sampling with replacement\n    bootstrap_response_rates[i] = calculate_response_rate(bootstrap_sample)\n\n# Calculate mean and standard error of bootstrap response rates\nbootstrap_mean = bootstrap_response_rates.mean()\n\n# Calculate 95% confidence interval\nconfidence_interval = np.percentile(bootstrap_response_rates, [2.5, 97.5])\nprint(\"Bootstrap Mean:\", bootstrap_mean.round(3))\nprint(\"95% Confidence Interval:\", confidence_interval.round(3))\n\nBootstrap Mean: 0.124\n95% Confidence Interval: [0.115 0.134]\n\n\n\n\nCode\n# Plot using Seaborn\nplt.figure(figsize=(8, 6))\nsns.histplot(bootstrap_response_rates, bins=100, kde=True, color=\"#00A087B2\", edgecolor=\"black\", fill=True)\nplt.axvline(np.mean(bootstrap_response_rates), color=\"black\", linestyle=\"--\", linewidth=1.5)\nplt.axvline(ci_lower, color=\"#481f70\", linestyle=\"--\", label=\"95% CI Lower Bound\")\nplt.axvline(ci_upper, color=\"#481f70\", linestyle=\"--\", label=\"95% CI Upper Bound\")\nplt.xlabel(\"Response Rate (p)\")\nplt.ylabel(\"Density\")\nplt.title(\"Bootstrap Distribution\")\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\nUpon running the resampling, we obtain a 95% confidence interval of [0.115, 0.134]. Interestingly, we notice that the uncertainty level is slightly higher in the nonparametric case (more uncertainty). This is a common trade-off when opting for fewer assumptions in our analysis."
  },
  {
    "objectID": "posts/ab_test_bayesian/bayesian_ab_test.html#bayesian-approach",
    "href": "posts/ab_test_bayesian/bayesian_ab_test.html#bayesian-approach",
    "title": "Bayesian Approach to A/B Testing",
    "section": "2.3. Bayesian Approach",
    "text": "2.3. Bayesian Approach\nSo far, everything has been very standard. But let’s consider this: what if our catalog campaign isn’t the only experiment we’ve conducted to boost our browser (and ultimately ad revenue)? Among the thousands of experiments we’ve run in the past, this campaign is just one idea in the mix. Can we efficiently leverage this wealth of additional information?\nThis is how Bayesian statistics offers a powerful advantage over the frequentist approach: the seamless integration of additional information into our model. The idea directly follows from the main results behind all Bayesian statistics: Bayes Theorem. By inverting the inference problem, Bayes’ Theorem allows us to transition from the probability of the model given the data to the probability of the data given the model:\n\n\\underbrace{ \\Pr \\big( \\text{model} \\ \\big| \\ \\text{data} \\big) }_{\\text{posterior}} = \\underbrace{ \\Pr(\\text{model}) }_{\\text{prior}} \\ \\underbrace{ \\frac{ \\Pr \\big( \\text{data} \\ \\big| \\ \\text{model} \\big) }{ \\Pr(\\text{data}) } }_{\\text{likelihood}}\n\nWe can split the right-hand side of Bayes Theorem (or Rule) into two components: the prior and the likelihood. The likelihood is the information about the model that comes from the data, the prior instead is any additional information about the model.\n\nPriors\nAs the name suggests, priors contain information that was available even before looking at the data. This leads to one of the most relevant questions in Bayesian statistics: How do you choose a prior? In practice, there are several considerations to keep in mind:\n\nBeta Distribution Assumption: Under the Bayesian framework, we assume that the true conversion rates follow a Beta distribution. This particular distribution is chosen for its flexibility and ability to capture uncertainty.\nUninformative or Weakly Informative Priors: these priors are characterized by setting low values for alpha and beta. For example, alpha = 1, beta = 1 leads to a uniform distribution as a prior. If we were considering one distribution in isolation, setting this prior is a statement that we don’t know anything about the value of the parameter, nor our confidence around it.\nStrong Priors: Conversely, strong priors are characterized by high values for alpha and beta, implying that the relative uplift distribution is thin, i.e. our prior belief is that the variants are not very different from each other.\n\nGoing back to our emailing campaign, let’s assume our response rate p comes from a beta distribution. The mean of the beta distribution is:\n\n\\text{E}[p|a,b] = \\frac{a}{a+b}\n\nDrawing insights from past experiments, we’ve observed that p tends to hover around 12.3%. To reflect this knowledge, we set the parameters a and b of the beta distribution accordingly: a = 9 and b = 64.\n\nprior_a = 9\nprior_b = 64\n\n# Mean\np_hat = prior_a / (prior_a + prior_b)\nprint(round(p_hat,3))\n\n0.123\n\n\n\n\nCode\n# Create DataFrame with samples\nB = 10000  # number draws from distribution\nsamples = np.random.beta(prior_a, prior_b, size=B)\nbeta_df = pd.DataFrame({'Samples': samples})\n\n# Plot using Seaborn\nplt.figure(figsize=(8, 6))\nsns.histplot(beta_df, bins=100, kde=True, color=\"#00A087B2\", edgecolor=\"black\", fill=True)\nplt.axvline(p_hat, color=\"black\", linestyle=\"--\", linewidth=1.5)\nplt.xlabel(\"Response Rate (p)\")\nplt.ylabel(\"Density\")\nplt.title(\"Prior Distribution\")\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\nIt’s worth noting that the beta distribution exhibits a slight leftward skewness, suggesting that lower response rates may be more likely than higher response rates, which is a common pattern in scenarios where there are factors such as limited customer engagement or market saturation.\n\n\nPosterior\nBayes rule tells you how you should update your beliefs after you see some data:\n \\textrm{posterior} \\propto \\textrm{likelihood} \\times \\textrm{prior} \nNow, we already assume some prior and now we run the experiment and we see our data results. With this, we can update our prior belief by calculating the posterior. Our observed experiment had the following characteristics:\n\n\nCode\nprint('Number Emails sent:', df.mailing.sum() )\nprint('Number of responses:', df[df['mailing'] == 1].respmail.sum() )\nprint('Response Rate', df[df['mailing'] == 1].respmail.mean().round(4)*100 )\n\n\nNumber Emails sent: 4952\nNumber of responses: 616.0\nResponse Rate 12.44\n\n\nIf we observe s responses in n observations, we can update our data by using the following equations:\n\nf(p)=p^{a+x-1}(1-p)^{b+n-s-1}\\approx\\text{beta}[a+s,b+n-s]\n\n\n# Define parameters\nB = 10000\nn = df.mailing.sum()                       # number in test sample\ns = df[df['mailing'] == 1].respmail.sum()  # number of responses\n\npost_a = prior_a + s\npost_b = prior_b + n - s\n\n\nfrom scipy.stats import beta\n\n# Calculate mean posterior response rate\np_hat1 = post_a / (post_a + post_b)\n\n# Print mean response rate\nprint(\"The Posterior Mean response rate is\", round(p_hat1, 3))\n\n# Calculate prior and posterior confidence intervals\nprior_ci = beta.ppf([0.025, 0.975], prior_a, prior_b)\nposterior_ci = beta.ppf([0.025, 0.975], post_a, post_b)\n\n# Print confidence intervals\nprint(\"Prior Response Rate 95% CI:\", prior_ci.round(3))\nprint(\"Posterior Response Rate 95% CI:\", posterior_ci.round(3))\n\nThe Posterior Mean response rate is 0.124\nPrior Response Rate 95% CI: [0.059 0.207]\nPosterior Response Rate 95% CI: [0.115 0.134]\n\n\n\n\nCode\n# Generate posterior density samples\nposterior_density = np.random.beta(a=post_a, b=post_b, size=B)\n\n# Plot using Seaborn\nplt.figure(figsize=(8, 6))\nsns.histplot(posterior_density, bins=100, kde=True, color=\"#00A087B2\", edgecolor=\"black\", fill=True)\nplt.axvline(p_hat1, color=\"black\", linestyle=\"--\", linewidth=1.5)\nplt.xlabel(\"Response Rate (p)\")\nplt.ylabel(\"Density\")\nplt.title(\"Posterior Distribution\")\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\nThe Bayesian approach provides a posterior mean response rate of 0.124, along with a 95% confidence interval (CI) for the response rate ranging from 0.115 to 0.134. In this case, the standard approaches reported similar confidence intervals.\n\n\nBreakeven Point\nPlatforms often seek to determine whether the response rate from a campaign surpasses a minimum threshold necessary to cover the costs incurred. This minimum threshold is known as the breakeven point. For instance, let’s consider a scenario where the cost per mailing (c) amounts to $5.75, while the profits generated per response (m) stand at $50.\nWe can calculate the probability that the posterior is above the breakeven point, which in this case is:  p &gt;\\frac{c}{\\pi} \\equiv \\frac{5.75}{50} = 0.115 \nSo that we are interested in:  \\Pr(p&lt;0.115)  We estimate the probability by drawing from a beta distribution with above parameters.\n\nc = 5.75   # cost per mailing\nm = 50   # profit if respond\n\n# Calculate breakeven point\nbrk = c / m\nprint(\"Breakeven point:\", brk)\n\nBreakeven point: 0.115\n\n\n\n# Set random seed\nnp.random.seed(19312)\n\n# Generate random deviates\npost_draws = np.random.beta(post_a, post_b, size=B)\n\n# Calculate probability of passing breakeven point\nprob_passing = np.sum(post_draws &lt; brk) / B\n\n# Print probability\nprint(\"The Probability that the response rate is below the breakeven point is\", round(prob_passing, 3))\n\nThe Probability that the response rate is below the breakeven point is 0.02\n\n\nWith only a 2% probability of the response rate falling below the breakeven point, the results indicate a favorable outcome, indicating that the campaign’s effectiveness in generating responses is promising in relation to its profitability."
  },
  {
    "objectID": "posts/ab_test_bayesian/bayesian_ab_test.html#classical-approach-linear-regression",
    "href": "posts/ab_test_bayesian/bayesian_ab_test.html#classical-approach-linear-regression",
    "title": "Bayesian Approach to A/B Testing",
    "section": "3.1. Classical Approach: Linear Regression",
    "text": "3.1. Classical Approach: Linear Regression\nAs explored in my previous post on A/B Testing, we can quantify the impact of our catalog campaign using a standard linear regression model. In this case, I would like to know the impact of the campaign on Purchase Value M of customers on the E-beer store during a given period:\n\nY_{i} = \\alpha + \\beta D_i + \\varepsilon_i\n\nwhere D_i is the treatment indicator (equal to 1 for treated units and 0 for control units), \\alpha is the intercept term and \\beta is our coefficient of interest representing the Average Treatment Effect.\n\nimport statsmodels.formula.api as smf\n\nsmf.ols('M ~ mailing', data=df).fit().summary().tables[1]\n\n\n\nTable 3: A/B Regression Results I\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n92.2839\n1.066\n86.589\n0.000\n90.195\n94.373\n\n\nmailing\n2.9149\n1.512\n1.928\n0.054\n-0.049\n5.878\n\n\n\n\n\n\n\n\nAccording to Table 3, while the coefficient associated with the treatment variable (mailing) suggests a positive impact of the catalog campaign, these effects do not achieve statistical significance. The 95% Confidence Intervals for the coefficient range from -0.049 to 5.878, indicating substantial uncertainty surrounding the estimated treatment effect.\nCan we do better? Let’s leverage information from previous tests with the Bayesian framework."
  },
  {
    "objectID": "posts/ab_test_bayesian/bayesian_ab_test.html#bayesian-approach-uninformative-prior",
    "href": "posts/ab_test_bayesian/bayesian_ab_test.html#bayesian-approach-uninformative-prior",
    "title": "Bayesian Approach to A/B Testing",
    "section": "3.2. Bayesian Approach: Uninformative Prior",
    "text": "3.2. Bayesian Approach: Uninformative Prior\n\nPrior\nWhen choosing a prior, one analytically appealing restriction is to have a prior distribution such that the posterior belongs to the same family. For example, in the case of a continuous dependent variable (M), I can assume my treatment effect is normally distributed and I would like it to be normally distributed also after incorporating the information contained in the data.\nNow, before observing any data, I knew nothing about how much people might spend on beers online. They might spend 2 euros or they might stay for 200 euros. Formally, I can describe my weakly informative prior beliefs with a prior distribution:\n\\textrm{mean Purchase-Value by group} \\sim N(0, 100^2)\nHere is how the distribution looks like:\n\n\nCode\n# Prior parameters\nprior_mean = 0\nprior_sd = 100\n\n# Generate x-values for plotting\nxx = np.linspace(-300, 300, num=1000)\n\n# Calculate prior density\nprior_density = (1 / (prior_sd * np.sqrt(2 * np.pi))) * np.exp(-0.5 * ((xx - prior_mean) / prior_sd) ** 2)\n\n# Plot prior density using Seaborn\nplt.figure(figsize=(10, 6))\nsns.lineplot(x=xx, y=prior_density, color='#00A087B2')\nplt.axvline(prior_mean, color=\"black\", linestyle=\"--\", linewidth=1.5)\nplt.title('Prior Density')\nplt.xlabel('Mean Purchase Value (M)')\nplt.ylabel('Density')\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nPosterior\nThen Bayes rule tells us that the posterior distribution for mean purchase-value for each group should be:\n\n\\textrm{mean time-on-site (m)} \\sim \\mathcal{N}\\left(\\mu, \\sigma^2\\right)\nwhere\n\n\\sigma = \\left(\\frac{1}{\\sigma_{0}^2} + \\frac{n}{s^2}\\right)^{-1}\n\nand\n\n\\mu = \\sigma^2 \\left(\\frac{\\mu_0}{\\sigma_{0}^2} + \\frac{n \\bar{y}}{s^2}\\right)\n\n\n# Posterior parameters\nn_A = df.shape[0] - df.mailing.sum()\nn_B = df.mailing.sum()\ns = df.M.std()  # Standard deviation of data (approx==2)\nybar_A = df[df['mailing'] == 0].M.mean()\nybar_B = df[df['mailing'] == 1].M.mean()\n\n# Posterior standard deviation\nposterior_sd_A = (1 / (prior_sd ** 2) + n_A / (s ** 2)) ** (-0.5)\nposterior_sd_B = (1 / (prior_sd ** 2) + n_B / (s ** 2)) ** (-0.5)\n\n# Posterior mean\nposterior_mean_A = posterior_sd_A ** 2 * ((prior_mean / (prior_sd ** 2)) + (n_A * ybar_A / (s ** 2)))\nposterior_mean_B = posterior_sd_B ** 2 * ((prior_mean / (prior_sd ** 2)) + (n_B * ybar_B / (s ** 2)))\n\n\nprint(\"Posterior Mean Estimate for Group A:\", posterior_mean_A.round(1))\nprint(\"Posterior Mean Estimate for Group B:\", posterior_mean_B.round(1))\n\nPosterior Mean Estimate for Group A: 92.3\nPosterior Mean Estimate for Group B: 95.2\n\n\nWe can plot each posterior distribution to compare:\n\n\nCode\n# Plot posterior distributions\nxx = np.linspace(min(df['M']), max(df['M']), 1000)\nposterior_distribution_A = (1 / (posterior_sd_A * np.sqrt(2 * np.pi))) * np.exp(-0.5 * ((xx - posterior_mean_A) / posterior_sd_A) ** 2)\nposterior_distribution_B = (1 / (posterior_sd_B * np.sqrt(2 * np.pi))) * np.exp(-0.5 * ((xx - posterior_mean_B) / posterior_sd_B) ** 2)\n\nplt.figure(figsize=(10, 6))\nsns.lineplot(x=xx, y=posterior_distribution_A, label='Group A', color='#440154')\nsns.lineplot(x=xx, y=posterior_distribution_B, label='Group B', color='#00A087B2')\nplt.xlim(85, 102)  # Limit x-axis range\n\nplt.title('Posterior Distributions of Monetary Value (M)')\nplt.xlabel('Monetary Value (M)')\nplt.ylabel('Density')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\nExcellent! With the Bayesian approach, we’ve estimated the posterior mean purchase value for Group A to be $92.3 and for Group B to be $95.2. Once we have these distribution for the difference in the mean, we can compute the probability that the mean of B is greater than the mean of A and Confidence Intervals:\n\nfrom scipy.stats import norm\n\n# Calculate the difference in means\npost_mean_diff = posterior_mean_B - posterior_mean_A\nprint(\"Posterior Average Treatment Effect:\", post_mean_diff.round(3))\n\n# Calculate the standard deviation of the difference\npost_sd_diff = np.sqrt(posterior_sd_B ** 2 + posterior_sd_A ** 2)\n\n# Compute the probability\nprob = 1 - norm.cdf(0, loc=post_mean_diff, scale=post_sd_diff)\nprint(\"Probability that mean of B is greater than mean of A:\", prob.round(3))\n\n# Calculate the 95% confidence interval\nci_lower = norm.ppf(0.025, loc=post_mean_diff, scale=post_sd_diff)\nci_upper = norm.ppf(0.975, loc=post_mean_diff, scale=post_sd_diff)\nprint(\"95% Confidence Interval for the Difference in Means:\", (ci_lower.round(3), ci_upper.round(3)))\n\nPosterior Average Treatment Effect: 2.914\nProbability that mean of B is greater than mean of A: 0.973\n95% Confidence Interval for the Difference in Means: (-0.049, 5.878)\n\n\nOn average, the treatment (catalog campaign) leads to an increase in purchase value by approximately $2.914 compared to the control group. The confidence interval for the difference in means ranges from -0.049 to 5.878. Since this interval includes zero, we cannot conclude with 95% confidence that there is a statistically significant difference in purchase values between the treated and control groups.\nThis suggests that the catalog campaign may not have a significant impact on purchase behavior compared to the control group, based on the observed data.\n\n\nCode\n# Generate x-values for plotting\nxx_diff = np.linspace(-2, 7, num=100)\n\n# Calculate the posterior density of the difference\npost_density_diff = norm.pdf(xx_diff, loc=post_mean_diff, scale=post_sd_diff)\n\n# Plot the posterior distribution of the difference using Seaborn\nplt.figure(figsize=(10, 6))\nsns.lineplot(x=xx_diff, y=post_density_diff, color='#00A087B2')\nplt.title('Posterior Distribution of Difference in Means (B - A)')\nplt.axvline(x=0, color='black', linestyle='--', label='Zero difference')\nplt.xlabel('Difference in Means')\nplt.ylabel('Density')\nplt.grid(True)\n\n# Shade the area to the right of 0 to calculate the probability\nplt.fill_between(xx_diff, post_density_diff, where=(xx_diff &gt;= 0), color='#00A087B2', alpha=0.2)\n\nplt.show()"
  },
  {
    "objectID": "posts/ab_test_bayesian/bayesian_ab_test.html#bayesian-approach-student-t-prior",
    "href": "posts/ab_test_bayesian/bayesian_ab_test.html#bayesian-approach-student-t-prior",
    "title": "Bayesian Approach to A/B Testing",
    "section": "3.3. Bayesian Approach: Student-t Prior",
    "text": "3.3. Bayesian Approach: Student-t Prior\nWhat if we have a more informative prior? What if past experiments tell us that the distribution is actually more similar to a t-student distribution, for example?\nWhile we’ve previously assumed a normal distribution for our prior, which is appropriate for continuous data with a large sample size, we can explore also the t-student distribution, which is more robust to outliers and has heavier tails compared to the normal distribution. This is a setting that happens often in the industry.\n\nfrom scipy.stats import t\n\n# Assuming you have a DataFrame df with columns 'mailing' and 'M'\n# Filter the data for group A and group B\ngroup_A = df[df['mailing'] == 0]['M']\ngroup_B = df[df['mailing'] == 1]['M']\n\n# Prior parameters for Student's t-distribution\nprior_df = 1.3  # degrees of freedom\nprior_loc = 0  # mean\nprior_scale = 100  # scale (similar to standard deviation)\n\n# Likelihood function (assuming a normal likelihood)\ndef likelihood(data, mean):\n    sd = data.std()  # Standard deviation from the data\n    return np.prod(1 / (sd * np.sqrt(2 * np.pi)) * np.exp(-0.5 * ((data - mean) / sd) ** 2))\n\n# Posterior parameters for group A\nposterior_mean_A = group_A.mean()\nposterior_sd_A = np.sqrt(prior_scale ** 2 / (prior_scale ** 2 + (group_A.std() ** 2) / len(group_A)))\n\n# Posterior parameters for group B\nposterior_mean_B = group_B.mean()\nposterior_sd_B = np.sqrt(prior_scale ** 2 / (prior_scale ** 2 + (group_B.std() ** 2) / len(group_B)))\n\n\n\nCode\nfrom scipy.stats import norm\n\n# Prior parameters\nprior_mean = 0\nprior_sd = 100\n\n# Generate x-values for plotting\nxx = np.linspace(-300, 300, num=1000)\n\n# Calculate prior density\nprior_density = (1 / (prior_sd * np.sqrt(2 * np.pi))) * np.exp(-0.5 * ((xx - prior_mean) / prior_sd) ** 2)\n\n# Calculate the difference in means\npost_mean_diff = posterior_mean_B - posterior_mean_A\n\n# Calculate the standard deviation of the difference\npost_sd_diff = np.sqrt(posterior_sd_B ** 2 + posterior_sd_A ** 2)\n\n# Generate x-values for plotting\nxx_diff = np.linspace(-2, 7, num=100)\n\n# Calculate the posterior density of the difference using Normal prior\npost_density_diff_normal = norm.pdf(xx_diff, loc=post_mean_diff, scale=post_sd_diff)\n\n# Plot prior density and posterior distribution of the difference using Seaborn\nplt.figure(figsize=(10, 6))\n\n# Plot prior density\nsns.lineplot(x=xx, y=prior_density, color='#00A087B2', label='Normal Prior ')\nplt.axvline(prior_mean, color=\"black\", linestyle=\"--\", linewidth=1.5)\n\n# Plot posterior distribution of the difference\nsns.lineplot(x=xx_diff, y=post_density_diff_normal, color='#440154', label='t-Student Prior')\n\nplt.title('Comparison of Distributions')\nplt.xlabel('Mean Purchase Value (M)')\nplt.ylabel('Density')\nplt.grid(True)\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\nAs we can see from this plot, all distributions are centered on zero, but they have very different shapes. Let’s estimate our Confidence Intervals:\n\nprint(\"Posterior Average Treatment Effect:\", post_mean_diff.round(3))\n\n# Calculate the 95% confidence interval using the t-distribution\nci_lower = t.ppf(0.025, df=(len(group_A) + len(group_B)) - 2, loc=post_mean_diff, scale=post_sd_diff)\nci_upper = t.ppf(0.975, df=(len(group_A) + len(group_B)) - 2, loc=post_mean_diff, scale=post_sd_diff)\nprint(\"95% Confidence Interval for the Difference in Means:\", (ci_lower.round(3), ci_upper.round(3)))\n\nPosterior Average Treatment Effect: 2.915\n95% Confidence Interval for the Difference in Means: (0.143, 5.687)\n\n\n\n\nCode\nfrom scipy.stats import t\n\n# Generate x-values for plotting\nxx_diff = np.linspace(-2, 7, num=100)\n\n# Calculate the posterior density of the difference using Student's t-distribution\npost_density_diff = t.pdf(xx_diff, df=(len(group_A) + len(group_B)) - 2, loc=post_mean_diff, scale=post_sd_diff)\n\n\n# Plot the posterior distribution of the difference using Seaborn\nplt.figure(figsize=(10, 6))\nsns.lineplot(x=xx_diff, y=post_density_diff, color='#00A087B2')\nplt.title('Posterior Distribution of Difference in Means (B - A) with Student-t Prior')\nplt.xlabel('Difference in Means')\nplt.ylabel('Density')\nplt.grid(True)\n\n# Shade the area to the right of 0 to calculate the probability\nplt.fill_between(xx_diff, post_density_diff, where=(xx_diff &gt;= 0), color='#00A087B2', alpha=0.2)\n\n# Add a vertical line at the point where the difference in means is zero\nplt.axvline(x=0, color='black', linestyle='--', label='Zero difference')\n\n# Add confidence interval lines\nplt.axvline(x=ci_lower, color='blue', linestyle='--', label='95% CI')\nplt.axvline(x=ci_upper, color='blue', linestyle='--')\n\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\nAt first glance, the Posterior Average Treatment Effect (ATE) appears to be similar to our previous findings. But now, these results are statistically significant. Unlike our previous analysis, the 95% Confidence Interval for the Difference in Means now ranges from 0.143 to 5.687. The change in significance might be attributed to the adoption of a more informative prior distribution, which has provided greater precision in our estimates."
  },
  {
    "objectID": "posts/causal_forests/causal_forests.html",
    "href": "posts/causal_forests/causal_forests.html",
    "title": "Causal Forests: Heterogeneous Treatment Effects",
    "section": "",
    "text": "Estimating heterogeneous treatment effects using Causal Forests and Meta-Learners in Python.\nIn causal inference, we often focus on estimating the Average Treatment Effect (ATE) — the mean impact of a treatment across an entire population. But in practice, treatment effects are rarely uniform. A marketing campaign might boost sales for price-sensitive customers while having no effect on loyal ones. A medical treatment might work well for younger patients but not for older ones.\nThe Conditional Average Treatment Effect (CATE) captures this heterogeneity:\n\\tau(x) = \\mathbb{E}[Y_i(1) - Y_i(0) \\mid X_i = x]\nwhere Y_i(1) and Y_i(0) are the potential outcomes under treatment and control, and X_i is a vector of covariates. Estimating \\tau(x) allows us to answer: who benefits most from the treatment?\nIn this post, we explore several modern approaches to CATE estimation:\nWe start with simulated data where the true CATE is known, allowing us to visually compare how well each method recovers the treatment effect surface. Later, we apply these methods to a real-world dataset."
  },
  {
    "objectID": "posts/causal_forests/causal_forests.html#setup",
    "href": "posts/causal_forests/causal_forests.html#setup",
    "title": "Causal Forests: Heterogeneous Treatment Effects",
    "section": "1.1. Setup",
    "text": "1.1. Setup\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\nimport seaborn as sns\nfrom scipy.interpolate import griddata\n\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, GradientBoostingClassifier\nfrom sklearn.model_selection import cross_val_score\n\nfrom econml.dml import CausalForestDML\nfrom econml.metalearners import SLearner, TLearner, XLearner\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nnp.random.seed(42)"
  },
  {
    "objectID": "posts/causal_forests/causal_forests.html#data-generating-process",
    "href": "posts/causal_forests/causal_forests.html#data-generating-process",
    "title": "Causal Forests: Heterogeneous Treatment Effects",
    "section": "1.2. Data Generating Process",
    "text": "1.2. Data Generating Process\nWe follow a simulation design inspired by Athey & Imbens (2016) and Aeturrell. The setup is:\n\nCovariates: X_i \\in \\mathbb{R}^6, drawn from \\text{Uniform}[0, 1]^6. Only the first two covariates (X_0, X_1) drive the treatment effect; the remaining four are noise.\nTreatment assignment: W_i \\sim \\text{Bernoulli}(e(X_i)) where the propensity score is:\n\n\ne(x) = 0.25 \\cdot (1 + \\beta_{2,4}(x_0))\n\nwith \\beta_{2,4} denoting the CDF of a \\text{Beta}(2, 4) distribution. This creates a non-uniform propensity that depends on X_0, making the problem more realistic.\n\nBaseline outcome (control potential outcome):\n\n\n\\mu_0(x) = 2X_0 - 1\n\n\nTreatment effect (CATE):\n\n\n\\tau(x) = \\zeta(X_0) \\cdot \\zeta(X_1)\n\nwhere \\zeta is a sigmoid-like function:\n\n\\zeta(x) = 1 + \\frac{1}{1 + e^{-20(x - 1/3)}}\n\nThis creates a nonlinear CATE surface that ranges from approximately 1 (when both X_0 and X_1 are small) to 4 (when both are large), with a sharp transition around x = 1/3.\n\nObserved outcome:\n\n\nY_i = \\mu_0(X_i) + \\tau(X_i) \\cdot W_i + \\varepsilon_i, \\quad \\varepsilon_i \\sim \\mathcal{N}(0, 1)\n\n\nfrom scipy.stats import beta as beta_dist\n\ndef zeta(x):\n    \"\"\"Sigmoid-like function for CATE surface.\"\"\"\n    return 1 + 1 / (1 + np.exp(-20 * (x - 1/3)))\n\ndef true_cate(X):\n    \"\"\"True CATE: tau(x) = zeta(X0) * zeta(X1).\"\"\"\n    return zeta(X[:, 0]) * zeta(X[:, 1])\n\ndef generate_data(n=5000, d=6, seed=42):\n    \"\"\"Generate simulation data with heterogeneous treatment effects.\"\"\"\n    rng = np.random.default_rng(seed)\n    \n    # Covariates: Uniform[0,1]^d\n    X = rng.uniform(0, 1, size=(n, d))\n    \n    # Propensity score: depends on X0\n    e_x = 0.25 * (1 + beta_dist.cdf(X[:, 0], 2, 4))\n    \n    # Treatment assignment\n    W = rng.binomial(1, e_x)\n    \n    # Baseline outcome\n    mu_0 = 2 * X[:, 0] - 1\n    \n    # True treatment effect\n    tau = true_cate(X)\n    \n    # Observed outcome\n    epsilon = rng.normal(0, 1, n)\n    Y = mu_0 + tau * W + epsilon\n    \n    return X, W, Y, tau, e_x\n\n# Generate data\nn = 5000\nX, W, Y, tau_true, e_x = generate_data(n=n)\n\nprint(f\"Sample size: {n}\")\nprint(f\"Treated: {W.sum()} ({W.mean():.1%})\")\nprint(f\"Control: {n - W.sum()} ({1 - W.mean():.1%})\")\nprint(f\"True ATE: {tau_true.mean():.3f}\")\n\nSample size: 5000\nTreated: 2067 (41.3%)\nControl: 2933 (58.7%)\nTrue ATE: 2.774"
  },
  {
    "objectID": "posts/causal_forests/causal_forests.html#true-cate-surface",
    "href": "posts/causal_forests/causal_forests.html#true-cate-surface",
    "title": "Causal Forests: Heterogeneous Treatment Effects",
    "section": "1.3. True CATE Surface",
    "text": "1.3. True CATE Surface\nBefore estimating anything, let’s visualize the true treatment effect surface. Since \\tau(x) only depends on X_0 and X_1, we can plot it as a heatmap:\n\n\nCode\n# Create grid for true CATE\ngrid_n = 200\nx0_grid = np.linspace(0, 1, grid_n)\nx1_grid = np.linspace(0, 1, grid_n)\nX0_mesh, X1_mesh = np.meshgrid(x0_grid, x1_grid)\n\n# Compute true CATE on grid\nX_grid = np.column_stack([X0_mesh.ravel(), X1_mesh.ravel()])\ntau_grid = zeta(X_grid[:, 0]) * zeta(X_grid[:, 1])\ntau_mesh = tau_grid.reshape(grid_n, grid_n)\n\n# Plot\nfig, ax = plt.subplots(figsize=(8, 6))\nim = ax.pcolormesh(X0_mesh, X1_mesh, tau_mesh, cmap='viridis', shading='auto')\nax.set_xlabel('$X_0$', fontsize=13)\nax.set_ylabel('$X_1$', fontsize=13)\nax.set_title('True CATE: $\\\\tau(x) = \\\\zeta(X_0) \\\\cdot \\\\zeta(X_1)$', fontsize=14)\ncbar = fig.colorbar(im, ax=ax)\ncbar.set_label('$\\\\tau(x)$', fontsize=12)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nFigure 1: True CATE surface \\tau(x) = \\zeta(X_0) \\cdot \\zeta(X_1). The treatment effect ranges from ~1 (low X_0, low X_1) to ~4 (high X_0, high X_1), with a sharp transition around x = 1/3.\n\n\n\n\n\nThe heatmap reveals the key feature of our DGP: the treatment effect is strongly heterogeneous. Individuals in the upper-right region (X_0 &gt; 1/3, X_1 &gt; 1/3) experience a treatment effect close to 4, while those in the lower-left region see an effect close to 1. The sigmoid function creates a sharp “elbow” around X_0 = X_1 = 1/3.\nThis is the surface that our estimators will try to recover."
  },
  {
    "objectID": "posts/causal_forests/causal_forests.html#s-learner",
    "href": "posts/causal_forests/causal_forests.html#s-learner",
    "title": "Causal Forests: Heterogeneous Treatment Effects",
    "section": "2.1. S-Learner",
    "text": "2.1. S-Learner\nThe S-Learner (Single Learner) is the simplest meta-learner. It fits a single model \\hat{\\mu}(X, W) on the full dataset, including the treatment indicator as a feature:\n\n\\hat{\\tau}^S(x) = \\hat{\\mu}(x, 1) - \\hat{\\mu}(x, 0)\n\nThe CATE estimate is the difference in predictions when we toggle the treatment indicator. The main drawback is regularization bias: if the treatment effect is small relative to the main effects, the model may learn to ignore W entirely.\n\n# S-Learner\ns_learner = SLearner(\n    overall_model=GradientBoostingRegressor(\n        n_estimators=200, max_depth=4, learning_rate=0.1, random_state=42\n    )\n)\ns_learner.fit(Y, W, X=X)\ntau_hat_s = s_learner.effect(X).flatten()\n\nprint(f\"S-Learner — Estimated ATE: {tau_hat_s.mean():.3f} (True: {tau_true.mean():.3f})\")\n\nS-Learner — Estimated ATE: 2.755 (True: 2.774)"
  },
  {
    "objectID": "posts/causal_forests/causal_forests.html#t-learner",
    "href": "posts/causal_forests/causal_forests.html#t-learner",
    "title": "Causal Forests: Heterogeneous Treatment Effects",
    "section": "2.2. T-Learner",
    "text": "2.2. T-Learner\nThe T-Learner (Two Learner) fits separate models for the treated and control groups:\n\n\\hat{\\mu}_1(x) = \\mathbb{E}[Y \\mid X = x, W = 1], \\quad \\hat{\\mu}_0(x) = \\mathbb{E}[Y \\mid X = x, W = 0]\n\n\n\\hat{\\tau}^T(x) = \\hat{\\mu}_1(x) - \\hat{\\mu}_0(x)\n\nThis avoids the regularization bias of the S-Learner but can suffer from high variance when sample sizes are small, since each model only sees half the data.\n\n# T-Learner\nt_learner = TLearner(\n    models=[\n        GradientBoostingRegressor(n_estimators=200, max_depth=4, learning_rate=0.1, random_state=42),\n        GradientBoostingRegressor(n_estimators=200, max_depth=4, learning_rate=0.1, random_state=42),\n    ]\n)\nt_learner.fit(Y, W, X=X)\ntau_hat_t = t_learner.effect(X).flatten()\n\nprint(f\"T-Learner — Estimated ATE: {tau_hat_t.mean():.3f} (True: {tau_true.mean():.3f})\")\n\nT-Learner — Estimated ATE: 2.778 (True: 2.774)"
  },
  {
    "objectID": "posts/causal_forests/causal_forests.html#x-learner",
    "href": "posts/causal_forests/causal_forests.html#x-learner",
    "title": "Causal Forests: Heterogeneous Treatment Effects",
    "section": "2.3. X-Learner",
    "text": "2.3. X-Learner\nThe X-Learner (Künzel et al., 2019) improves on the T-Learner by using cross-imputation: it imputes the missing potential outcomes for each group using the other group’s model, then fits a second-stage model on these imputed treatment effects:\nStep 1: Fit \\hat{\\mu}_0 and \\hat{\\mu}_1 as in the T-Learner.\nStep 2: Impute individual treatment effects: \n\\tilde{\\tau}_1^i = Y_i^1 - \\hat{\\mu}_0(X_i^1), \\quad \\tilde{\\tau}_0^i = \\hat{\\mu}_1(X_i^0) - Y_i^0\n\nStep 3: Fit models \\hat{\\tau}_1(x) and \\hat{\\tau}_0(x) on these imputed effects.\nStep 4: Combine using the propensity score: \n\\hat{\\tau}^X(x) = e(x) \\cdot \\hat{\\tau}_0(x) + (1 - e(x)) \\cdot \\hat{\\tau}_1(x)\n\nThe X-Learner is particularly effective when one group is much larger than the other.\n\n# X-Learner\nx_learner = XLearner(\n    models=[\n        GradientBoostingRegressor(n_estimators=200, max_depth=4, learning_rate=0.1, random_state=42),\n        GradientBoostingRegressor(n_estimators=200, max_depth=4, learning_rate=0.1, random_state=42),\n    ],\n    cate_models=[\n        GradientBoostingRegressor(n_estimators=200, max_depth=4, learning_rate=0.1, random_state=42),\n        GradientBoostingRegressor(n_estimators=200, max_depth=4, learning_rate=0.1, random_state=42),\n    ],\n    propensity_model=GradientBoostingClassifier(n_estimators=200, max_depth=4, learning_rate=0.1, random_state=42),\n)\nx_learner.fit(Y, W, X=X)\ntau_hat_x = x_learner.effect(X).flatten()\n\nprint(f\"X-Learner — Estimated ATE: {tau_hat_x.mean():.3f} (True: {tau_true.mean():.3f})\")\n\nX-Learner — Estimated ATE: 2.780 (True: 2.774)"
  },
  {
    "objectID": "posts/causal_forests/causal_forests.html#causal-forest",
    "href": "posts/causal_forests/causal_forests.html#causal-forest",
    "title": "Causal Forests: Heterogeneous Treatment Effects",
    "section": "2.4. Causal Forest",
    "text": "2.4. Causal Forest\nCausal Forests (Wager & Athey, 2018) are an extension of random forests specifically designed for treatment effect estimation. The key innovation is that trees split to maximize heterogeneity in the treatment effect, not just to improve prediction.\nWe use the CausalForestDML implementation from econml, which combines the causal forest with double/debiased machine learning (DML). This involves:\n\nFirst stage: Estimate nuisance parameters \\hat{\\mu}(x) = \\mathbb{E}[Y|X] and \\hat{e}(x) = \\mathbb{E}[W|X] using flexible ML models.\nSecond stage: Fit a causal forest on the residualized outcome \\tilde{Y} = Y - \\hat{\\mu}(X) and residualized treatment \\tilde{W} = W - \\hat{e}(X).\n\nThis orthogonalization step provides doubly robust estimates: the CATE estimate is consistent if either the outcome model or the propensity model is correctly specified.\n\n# Causal Forest (DML)\ncf = CausalForestDML(\n    model_y=GradientBoostingRegressor(n_estimators=200, max_depth=4, learning_rate=0.1, random_state=42),\n    model_t=GradientBoostingRegressor(n_estimators=200, max_depth=4, learning_rate=0.1, random_state=42),\n    n_estimators=500,\n    min_samples_leaf=5,\n    max_depth=None,\n    random_state=42,\n)\ncf.fit(Y, W, X=X)\ntau_hat_cf = cf.effect(X).flatten()\n\nprint(f\"Causal Forest — Estimated ATE: {tau_hat_cf.mean():.3f} (True: {tau_true.mean():.3f})\")\n\nCausal Forest — Estimated ATE: 2.710 (True: 2.774)"
  },
  {
    "objectID": "posts/causal_forests/causal_forests.html#estimated-cate-surfaces",
    "href": "posts/causal_forests/causal_forests.html#estimated-cate-surfaces",
    "title": "Causal Forests: Heterogeneous Treatment Effects",
    "section": "3.1. Estimated CATE Surfaces",
    "text": "3.1. Estimated CATE Surfaces\nWe now compare how each method recovers the true CATE surface. For each estimator, we interpolate the predicted \\hat{\\tau}(x) onto a grid over (X_0, X_1) and plot the heatmap alongside the ground truth:\n\n\nCode\ndef interpolate_cate(X, tau_hat, grid_n=100):\n    \"\"\"Interpolate CATE estimates onto a regular grid over (X0, X1).\"\"\"\n    x0_grid = np.linspace(0, 1, grid_n)\n    x1_grid = np.linspace(0, 1, grid_n)\n    X0_mesh, X1_mesh = np.meshgrid(x0_grid, x1_grid)\n    tau_mesh = griddata(\n        X[:, :2], tau_hat, (X0_mesh, X1_mesh), method='cubic', fill_value=np.nan\n    )\n    return X0_mesh, X1_mesh, tau_mesh\n\n# Compute true CATE on finer grid\ngrid_n = 100\nx0_g = np.linspace(0, 1, grid_n)\nx1_g = np.linspace(0, 1, grid_n)\nX0_g, X1_g = np.meshgrid(x0_g, x1_g)\nX_g = np.column_stack([X0_g.ravel(), X1_g.ravel()])\ntau_true_grid = (zeta(X_g[:, 0]) * zeta(X_g[:, 1])).reshape(grid_n, grid_n)\n\n# Interpolate estimated CATEs\nestimates = {\n    'True CATE': (X0_g, X1_g, tau_true_grid),\n    'S-Learner': interpolate_cate(X, tau_hat_s, grid_n),\n    'T-Learner': interpolate_cate(X, tau_hat_t, grid_n),\n    'X-Learner': interpolate_cate(X, tau_hat_x, grid_n),\n    'Causal Forest': interpolate_cate(X, tau_hat_cf, grid_n),\n}\n\n# Global color limits\nvmin = min(tau_true.min(), tau_hat_s.min(), tau_hat_t.min(), tau_hat_x.min(), tau_hat_cf.min())\nvmax = max(tau_true.max(), tau_hat_s.max(), tau_hat_t.max(), tau_hat_x.max(), tau_hat_cf.max())\n\n# Plot: 2x3 grid (5 panels + 1 empty for colorbar)\nfig, axes = plt.subplots(2, 3, figsize=(16, 10))\naxes_flat = axes.flatten()\n\nfor idx, (title, (X0_m, X1_m, tau_m)) in enumerate(estimates.items()):\n    ax = axes_flat[idx]\n    im = ax.pcolormesh(X0_m, X1_m, tau_m, cmap='viridis', shading='auto', vmin=vmin, vmax=vmax)\n    ax.set_xlabel('$X_0$', fontsize=12)\n    ax.set_ylabel('$X_1$', fontsize=12)\n    ax.set_title(title, fontsize=14)\n    ax.set_aspect('equal')\n\n# Remove empty subplot, use it for colorbar\naxes_flat[5].axis('off')\nfig.colorbar(im, ax=axes_flat[5], shrink=0.85, label='$\\\\hat{\\\\tau}(x)$', location='left')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nFigure 2: Estimated CATE surfaces from each method compared to the true CATE. The S-Learner and Causal Forest best capture the sharp transition in the treatment effect surface.\n\n\n\n\n\nFigure 2 reveals striking differences across methods. The S-Learner and the Causal Forest produce the smoothest and most faithful reconstructions of the true CATE surface — both clearly recover the characteristic “L-shaped” transition around X_0 = X_1 = 1/3.\nIn contrast, the T-Learner surface is visibly noisy. Because the T-Learner fits entirely separate models for the treated and control groups, each model only trains on a subset of the data (~2,000 and ~3,000 observations respectively). The CATE is then obtained by subtracting two noisy predictions, which amplifies estimation error — a well-known variance issue with this approach.\nThe X-Learner falls in between: it captures the general shape of the CATE surface better than the T-Learner, but introduces some noise through its cross-imputation and propensity-weighting steps. Its advantage — leveraging the larger control group to improve treated-group estimates — is partially offset by the complexity of its multi-stage procedure."
  },
  {
    "objectID": "posts/causal_forests/causal_forests.html#quantitative-comparison",
    "href": "posts/causal_forests/causal_forests.html#quantitative-comparison",
    "title": "Causal Forests: Heterogeneous Treatment Effects",
    "section": "3.2. Quantitative Comparison",
    "text": "3.2. Quantitative Comparison\nBeyond visual inspection, we evaluate each estimator using two metrics:\n\nMSE: \\frac{1}{n}\\sum_i (\\hat{\\tau}(X_i) - \\tau(X_i))^2 — measures the average squared error in CATE estimation.\nR^2 (CATE): 1 - \\frac{\\sum_i(\\hat{\\tau}(X_i) - \\tau(X_i))^2}{\\sum_i(\\tau(X_i) - \\bar{\\tau})^2} — measures how much of the treatment effect heterogeneity is captured.\n\n\ndef cate_metrics(tau_true, tau_hat, name):\n    mse = np.mean((tau_hat - tau_true) ** 2)\n    ss_res = np.sum((tau_hat - tau_true) ** 2)\n    ss_tot = np.sum((tau_true - tau_true.mean()) ** 2)\n    r2 = 1 - ss_res / ss_tot\n    ate_bias = np.abs(tau_hat.mean() - tau_true.mean())\n    return {'Method': name, 'MSE': round(mse, 4), 'R² (CATE)': round(r2, 4), 'ATE Bias': round(ate_bias, 4)}\n\nresults = pd.DataFrame([\n    cate_metrics(tau_true, tau_hat_s, 'S-Learner'),\n    cate_metrics(tau_true, tau_hat_t, 'T-Learner'),\n    cate_metrics(tau_true, tau_hat_x, 'X-Learner'),\n    cate_metrics(tau_true, tau_hat_cf, 'Causal Forest'),\n]).set_index('Method')\n\nresults\n\n\n\nTable 1: Performance of CATE Estimators (Simulation)\n\n\n\n\n\n\n\n\n\n\nMSE\nR² (CATE)\nATE Bias\n\n\nMethod\n\n\n\n\n\n\n\nS-Learner\n0.0519\n0.9475\n0.0185\n\n\nT-Learner\n0.2939\n0.7022\n0.0038\n\n\nX-Learner\n0.1401\n0.8580\n0.0065\n\n\nCausal Forest\n0.0461\n0.9533\n0.0641\n\n\n\n\n\n\n\n\n\n\nTable 1 confirms the visual impression. Two key takeaways:\n\nCausal Forest achieves the best overall CATE recovery (R^2 = 0.95, MSE = 0.046), closely followed by the S-Learner (R^2 = 0.95, MSE = 0.052). Both methods effectively capture over 95% of the variation in the true treatment effect surface.\nThere is a bias-variance tradeoff across methods. The T-Learner has the lowest ATE bias (0.004) — it gets the average treatment effect almost exactly right — but performs worst on individual-level CATE estimation (MSE = 0.29, R^2 = 0.70). In contrast, the Causal Forest has slightly higher ATE bias (0.064) but is far more precise at estimating who benefits most from the treatment. This highlights an important point: getting the ATE right does not guarantee good CATE estimation, and vice versa."
  },
  {
    "objectID": "posts/causal_forests/causal_forests.html#distribution-of-estimated-cates",
    "href": "posts/causal_forests/causal_forests.html#distribution-of-estimated-cates",
    "title": "Causal Forests: Heterogeneous Treatment Effects",
    "section": "3.3. Distribution of Estimated CATEs",
    "text": "3.3. Distribution of Estimated CATEs\nWe can also compare the full distribution of estimated treatment effects across methods. A good estimator should produce a distribution that closely matches the true CATE distribution:\n\n\nCode\nfig, axes = plt.subplots(2, 2, figsize=(12, 9), sharey=True, sharex=True)\naxes = axes.flatten()\n\nestimator_results = [\n    ('S-Learner', tau_hat_s),\n    ('T-Learner', tau_hat_t),\n    ('X-Learner', tau_hat_x),\n    ('Causal Forest', tau_hat_cf),\n]\n\nfor ax, (name, tau_hat) in zip(axes, estimator_results):\n    ax.hist(tau_true, bins=50, alpha=0.4, color='gray', label='True CATE', density=True)\n    ax.hist(tau_hat, bins=50, alpha=0.6, color='#440154', label=f'{name}', density=True)\n    ax.axvline(tau_true.mean(), color='black', linestyle='--', linewidth=1.2, label='True ATE')\n    ax.set_xlabel('$\\\\tau(x)$', fontsize=12)\n    ax.set_title(name, fontsize=14)\n    ax.legend(fontsize=10)\n\naxes[0].set_ylabel('Density', fontsize=12)\naxes[2].set_ylabel('Density', fontsize=12)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nFigure 3: Distribution of estimated CATEs vs. the true CATE distribution. The vertical dashed line marks the true ATE.\n\n\n\n\n\nThe distribution plots in Figure 3 further illustrate the differences. The true CATE has a distinctive bimodal shape — a mass of observations near \\tau \\approx 1 (low X_0 or X_1) and another mass near \\tau \\approx 4 (both covariates above 1/3).\nThe S-Learner and Causal Forest both recover this bimodal structure, though the Causal Forest distribution appears slightly more concentrated. The T-Learner produces the widest spread, with substantial probability mass outside the true range [1, 4] — a consequence of its high variance. The X-Learner distribution shows an intermediate pattern, capturing the general shape but with heavier tails than the top-performing methods."
  },
  {
    "objectID": "posts/causal_forests/causal_forests.html#sorted-treatment-effects",
    "href": "posts/causal_forests/causal_forests.html#sorted-treatment-effects",
    "title": "Causal Forests: Heterogeneous Treatment Effects",
    "section": "3.4. Sorted Treatment Effects",
    "text": "3.4. Sorted Treatment Effects\nFinally, a useful way to evaluate CATE estimators is the sorted treatment effect plot: we sort individuals by their estimated CATE and compare against the true ranking. A well-calibrated estimator should produce a monotonically increasing curve that closely tracks the true sorted CATEs:\n\n\nCode\nfig, ax = plt.subplots(figsize=(10, 6))\n\ncolors = ['#440154', '#31688e', '#35b779', '#fde725']\n\nfor (name, tau_hat), color in zip(estimator_results, colors):\n    sort_idx = np.argsort(tau_hat)\n    ax.plot(\n        np.arange(len(tau_hat)) / len(tau_hat),\n        tau_true[sort_idx],\n        label=name, color=color, alpha=0.8, linewidth=1.5\n    )\n\n# Perfect ranking (sorted by true CATE)\nsort_true = np.argsort(tau_true)\nax.plot(\n    np.arange(len(tau_true)) / len(tau_true),\n    tau_true[sort_true],\n    label='Perfect ranking', color='black', linestyle='-', linewidth=2, alpha=0.5\n)\n\nax.axhline(tau_true.mean(), color='gray', linestyle='--', linewidth=1, label='True ATE')\nax.set_xlabel('Percentile (sorted by estimated CATE)', fontsize=12)\nax.set_ylabel('True CATE', fontsize=12)\nax.set_title('Sorted Treatment Effects', fontsize=14)\nax.legend(fontsize=10)\nax.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nFigure 4: Sorted treatment effects: individuals ranked by estimated CATE (x-axis) vs. their true CATE (colored lines). The gray dashed line shows the true ATE.\n\n\n\n\n\nFigure 4 provides perhaps the most policy-relevant evaluation. In practice, we often want to use CATE estimates to prioritize treatment — e.g., targeting a marketing campaign at the customers who would benefit most. What matters, then, is not just the accuracy of the point estimates but whether the estimator correctly ranks individuals by their treatment effect.\nThe black line shows the perfect ranking — what we would get if we could observe \\tau(x) directly. A good estimator should produce a curve that closely tracks this oracle ranking. The S-Learner and Causal Forest both produce steep, monotonically increasing curves that nearly overlap with the perfect ranking, confirming their strong discriminative ability. The T-Learner curve is flatter and noisier, suggesting that its individual-level rankings are less reliable — a practitioner who used T-Learner estimates for targeting would misallocate treatment to a meaningful degree."
  },
  {
    "objectID": "posts/causal_forests/causal_forests.html#data",
    "href": "posts/causal_forests/causal_forests.html#data",
    "title": "Causal Forests: Heterogeneous Treatment Effects",
    "section": "4.1. Data",
    "text": "4.1. Data\n\nfrom sklift.datasets import fetch_hillstrom\n\n# Load data\nbundle = fetch_hillstrom()\ndf = bundle['data'].copy()\ndf['visit'] = bundle['target'].values\ndf['segment'] = bundle['treatment'].values\n\n# Binarize treatment: Email (Men's or Women's) vs. No Email\ndf['W'] = (df['segment'] != 'No E-Mail').astype(int)\n\n# Encode categorical variables\ndf['channel_web'] = (df['channel'] == 'Web').astype(int)\ndf['channel_multichannel'] = (df['channel'] == 'Multichannel').astype(int)\ndf['zip_suburban'] = (df['zip_code'] == 'Surburban').astype(int)\ndf['zip_rural'] = (df['zip_code'] == 'Rural').astype(int)\n\n# Feature matrix\nfeature_cols = ['recency', 'history', 'mens', 'womens', 'newbie',\n                'channel_web', 'channel_multichannel', 'zip_suburban', 'zip_rural']\n\nX_h = df[feature_cols].values\nW_h = df['W'].values\nY_h = df['visit'].values\n\nprint(f\"Sample size: {len(df):,}\")\nprint(f\"Treated (any email): {W_h.sum():,} ({W_h.mean():.1%})\")\nprint(f\"Control (no email):  {(1-W_h).sum():,.0f} ({1-W_h.mean():.1%})\")\nprint(f\"Visit rate — Email: {Y_h[W_h==1].mean():.3f} | No Email: {Y_h[W_h==0].mean():.3f}\")\nprint(f\"Naive ATE: {Y_h[W_h==1].mean() - Y_h[W_h==0].mean():.4f}\")\nprint()\ndf[feature_cols + ['W', 'visit']].head(8)\n\nSample size: 64,000\nTreated (any email): 42,694 (66.7%)\nControl (no email):  21,306 (33.3%)\nVisit rate — Email: 0.167 | No Email: 0.106\nNaive ATE: 0.0609\n\n\n\n\n\nTable 2: Hillstrom Email Marketing Dataset\n\n\n\n\n\n\n\n\n\n\nrecency\nhistory\nmens\nwomens\nnewbie\nchannel_web\nchannel_multichannel\nzip_suburban\nzip_rural\nW\nvisit\n\n\n\n\n0\n10\n142.44\n1\n0\n0\n0\n0\n1\n0\n1\n0\n\n\n1\n6\n329.08\n1\n1\n1\n1\n0\n0\n1\n0\n0\n\n\n2\n7\n180.65\n0\n1\n1\n1\n0\n1\n0\n1\n0\n\n\n3\n9\n675.83\n1\n0\n1\n1\n0\n0\n1\n1\n0\n\n\n4\n2\n45.34\n1\n0\n0\n1\n0\n0\n0\n1\n0\n\n\n5\n6\n134.83\n0\n1\n0\n0\n0\n1\n0\n1\n1\n\n\n6\n9\n280.20\n1\n0\n1\n0\n0\n1\n0\n1\n0\n\n\n7\n9\n46.42\n0\n1\n0\n0\n0\n0\n0\n1\n0\n\n\n\n\n\n\n\n\n\n\nThe naive ATE tells us that the email campaign increases website visits by about 6 percentage points on average. Since this is a randomized experiment, this estimate is unbiased. But is it the same for all customers? If heterogeneity is strong enough, some customers may actually respond negatively to the email — and we would be better off not emailing them at all."
  },
  {
    "objectID": "posts/causal_forests/causal_forests.html#cate-estimation",
    "href": "posts/causal_forests/causal_forests.html#cate-estimation",
    "title": "Causal Forests: Heterogeneous Treatment Effects",
    "section": "4.2. CATE Estimation",
    "text": "4.2. CATE Estimation\nWe apply the Causal Forest to the Hillstrom data. Since this performed best in our simulation study and — unlike the meta-learners — provides valid confidence intervals through the DML framework, it is the natural choice for a real-world application:\n\n# Causal Forest on Hillstrom data\ncf_h = CausalForestDML(\n    model_y=GradientBoostingRegressor(n_estimators=200, max_depth=4, learning_rate=0.1, random_state=42),\n    model_t=GradientBoostingRegressor(n_estimators=200, max_depth=4, learning_rate=0.1, random_state=42),\n    n_estimators=500,\n    min_samples_leaf=10,\n    random_state=42,\n)\ncf_h.fit(Y_h, W_h, X=X_h)\ntau_hat_h = cf_h.effect(X_h).flatten()\n\n# Inference\nate_inference = cf_h.ate_inference(X=X_h)\nci_low, ci_high = ate_inference.conf_int_mean()\nprint(f\"Causal Forest ATE: {np.asarray(ate_inference.mean_point).item():.4f}\")\nprint(f\"95% CI: [{np.asarray(ci_low).item():.4f}, {np.asarray(ci_high).item():.4f}]\")\nprint(f\"CATE range: [{tau_hat_h.min():.4f}, {tau_hat_h.max():.4f}]\")\nprint(f\"CATE std: {tau_hat_h.std():.4f}\")\n\nCausal Forest ATE: 0.0611\n95% CI: [-0.0260, 0.1481]\nCATE range: [-0.1221, 0.2448]\nCATE std: 0.0444"
  },
  {
    "objectID": "posts/causal_forests/causal_forests.html#who-responds-most",
    "href": "posts/causal_forests/causal_forests.html#who-responds-most",
    "title": "Causal Forests: Heterogeneous Treatment Effects",
    "section": "4.3. Who Responds Most?",
    "text": "4.3. Who Responds Most?\nA key advantage of CATE estimation is that we can inspect which customer characteristics drive treatment effect heterogeneity. We do this in two ways: first, by looking at the distribution of estimated CATEs, and second, by examining how CATEs vary across observable subgroups.\n\n\nCode\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Histogram\naxes[0].hist(tau_hat_h, bins=60, color='#440154', alpha=0.7, edgecolor='white', density=True)\naxes[0].axvline(tau_hat_h.mean(), color='black', linestyle='--', linewidth=1.5, label=f'Mean CATE = {tau_hat_h.mean():.3f}')\naxes[0].axvline(0, color='red', linestyle='-', linewidth=1, alpha=0.7, label='Zero effect')\naxes[0].set_xlabel('Estimated CATE (pp)', fontsize=12)\naxes[0].set_ylabel('Density', fontsize=12)\naxes[0].set_title('Distribution of Estimated CATEs', fontsize=13)\naxes[0].legend(fontsize=10)\n\n# CATE by quintile\ndf['cate'] = tau_hat_h\ndf['cate_quintile'] = pd.qcut(df['cate'], 5, labels=['Q1\\n(Lowest)', 'Q2', 'Q3', 'Q4', 'Q5\\n(Highest)'])\n\nquintile_stats = df.groupby('cate_quintile', observed=True).agg(\n    mean_cate=('cate', 'mean'),\n    visit_treated=('visit', lambda x: x[df.loc[x.index, 'W'] == 1].mean()),\n    visit_control=('visit', lambda x: x[df.loc[x.index, 'W'] == 0].mean()),\n).reset_index()\nquintile_stats['observed_lift'] = quintile_stats['visit_treated'] - quintile_stats['visit_control']\n\nbars = axes[1].bar(quintile_stats['cate_quintile'], quintile_stats['observed_lift'],\n                    color=['#440154', '#31688e', '#21918c', '#5ec962', '#fde725'],\n                    edgecolor='white', alpha=0.85)\naxes[1].axhline(0, color='black', linewidth=0.8)\naxes[1].axhline(tau_hat_h.mean(), color='black', linestyle='--', linewidth=1, alpha=0.5, label='Overall ATE')\naxes[1].set_xlabel('CATE Quintile', fontsize=12)\naxes[1].set_ylabel('Observed Visit Rate Lift (Treated - Control)', fontsize=12)\naxes[1].set_title('Observed Lift by Estimated CATE Quintile', fontsize=13)\naxes[1].legend(fontsize=10)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nFigure 5: Distribution of estimated CATEs from the Causal Forest. Most customers experience a positive treatment effect, but there is meaningful heterogeneity.\n\n\n\n\n\nThe results are telling. The left panel of Figure 5 shows that the estimated CATEs range from about -0.12 to +0.24 — a wide spread around the mean of 0.061. Notably, a meaningful fraction of customers have negative estimated treatment effects: for these individuals, the email appears to reduce the probability of visiting the site (perhaps by triggering annoyance or unsubscribes).\nThe right panel provides a crucial validation check. We split customers into quintiles based on their estimated CATE, then compute the observed treatment effect within each quintile using the raw experimental data. If the Causal Forest is capturing real heterogeneity, we should see a monotonically increasing pattern — and the results are striking. Customers in Q1 (lowest estimated CATE) show a negative observed lift of roughly -15 pp, while those in Q5 experience an observed lift of nearly +30 pp. This strong monotonic gradient confirms that the Causal Forest is detecting genuine heterogeneity, not just fitting noise.\nNow let’s examine which features drive these differences:\n\n\nCode\nfig, axes = plt.subplots(2, 3, figsize=(16, 9))\naxes = axes.flatten()\npalette = ['#440154', '#35b779']\n\n# 1. Newbie vs Returning\nax = axes[0]\ngroups = df.groupby('newbie')['cate'].mean()\nax.bar(['Returning', 'New Customer'], groups.values, color=palette, edgecolor='white')\nax.set_title('Newbie Status', fontsize=13)\nax.set_ylabel('Mean Estimated CATE', fontsize=11)\n\n# 2. Men's vs Women's past purchases\nax = axes[1]\nlabels = ['Neither', 'Womens Only', 'Mens Only', 'Both']\nvals = [\n    df[(df['mens']==0) & (df['womens']==0)]['cate'].mean(),\n    df[(df['mens']==0) & (df['womens']==1)]['cate'].mean(),\n    df[(df['mens']==1) & (df['womens']==0)]['cate'].mean(),\n    df[(df['mens']==1) & (df['womens']==1)]['cate'].mean(),\n]\nax.bar(labels, vals, color=['#440154', '#31688e', '#35b779', '#fde725'], edgecolor='white')\nax.set_title('Past Purchase Category', fontsize=13)\nax.set_ylabel('Mean Estimated CATE', fontsize=11)\n\n# 3. Channel\nax = axes[2]\nch_groups = df.groupby('channel')['cate'].mean().sort_values()\nax.barh(ch_groups.index, ch_groups.values, color=['#440154', '#31688e', '#35b779'], edgecolor='white')\nax.set_title('Purchase Channel', fontsize=13)\nax.set_xlabel('Mean Estimated CATE', fontsize=11)\n\n# 4. Recency\nax = axes[3]\nrec_groups = df.groupby('recency')['cate'].mean()\nax.plot(rec_groups.index, rec_groups.values, 'o-', color='#440154', linewidth=2, markersize=5)\nax.set_xlabel('Recency (months since last purchase)', fontsize=11)\nax.set_ylabel('Mean Estimated CATE', fontsize=11)\nax.set_title('Recency', fontsize=13)\nax.grid(True, alpha=0.3)\n\n# 5. History (spending)\nax = axes[4]\ndf['history_bin'] = pd.qcut(df['history'], 5, labels=['Q1\\n(Lowest)', 'Q2', 'Q3', 'Q4', 'Q5\\n(Highest)'])\nhist_groups = df.groupby('history_bin', observed=True)['cate'].mean()\nax.bar(hist_groups.index, hist_groups.values,\n       color=['#440154', '#31688e', '#21918c', '#5ec962', '#fde725'], edgecolor='white')\nax.set_xlabel('Past Spending Quintile', fontsize=11)\nax.set_ylabel('Mean Estimated CATE', fontsize=11)\nax.set_title('Purchase History', fontsize=13)\n\n# 6. Zip Code\nax = axes[5]\nzip_groups = df.groupby('zip_code')['cate'].mean().sort_values()\nax.barh(zip_groups.index, zip_groups.values, color=['#440154', '#31688e', '#35b779'], edgecolor='white')\nax.set_title('Zip Code Type', fontsize=13)\nax.set_xlabel('Mean Estimated CATE', fontsize=11)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nFigure 6: Average estimated CATE by customer subgroup. Cross-category shoppers, new customers, and recent buyers respond most to the email campaign.\n\n\n\n\n\nFigure 6 reveals several patterns in who responds most to the email:\n\nPast purchase category is the strongest driver of heterogeneity. Customers who previously bought from both men’s and women’s categories show a CATE nearly 2.5x higher than men’s-only buyers (~10 pp vs. ~4 pp). This makes intuitive sense: cross-category shoppers may be more engaged with the brand overall, making them more receptive to marketing.\nNew customers respond slightly more than returning ones, suggesting the email may be especially effective at re-engaging recent acquirers.\nRecency matters: customers who purchased most recently (1–2 months ago) show the highest CATEs, with a declining trend for more dormant customers. This aligns with the marketing intuition that recency is one of the strongest predictors of re-engagement.\nChannel, zip code, and spending history show more modest differences, with the highest spenders (Q5) and multichannel buyers slightly more responsive.\n\nTogether, these patterns suggest a clear customer profile for email targeting: recently acquired, cross-category shoppers with high engagement."
  },
  {
    "objectID": "posts/causal_forests/causal_forests.html#targeting-policy-who-should-receive-the-email",
    "href": "posts/causal_forests/causal_forests.html#targeting-policy-who-should-receive-the-email",
    "title": "Causal Forests: Heterogeneous Treatment Effects",
    "section": "4.4. Targeting Policy: Who Should Receive the Email?",
    "text": "4.4. Targeting Policy: Who Should Receive the Email?\nThe ultimate value of CATE estimation is in designing optimal targeting policies. Instead of sending the email to everyone (which is what the firm did in the experiment), we can use the estimated CATEs to selectively target only those customers whose estimated treatment effect exceeds a threshold.\nThis is particularly relevant when the email has a cost (even if just an opportunity cost or risk of unsubscribes). The following plot shows what happens if we target only the top k\\% of customers ranked by their estimated CATE, and compare the expected lift against the “treat everyone” baseline:\n\n\nCode\n# Sort by estimated CATE (descending)\nsort_idx = np.argsort(-tau_hat_h)\ntau_sorted = tau_hat_h[sort_idx]\n\n# Cumulative mean CATE as we include more customers\npct = np.arange(1, len(tau_sorted) + 1) / len(tau_sorted)\ncumulative_mean = np.cumsum(tau_sorted) / np.arange(1, len(tau_sorted) + 1)\n\nfig, ax = plt.subplots(figsize=(10, 6))\nax.plot(pct * 100, cumulative_mean, color='#440154', linewidth=2.5, label='Targeted (top $k$%)')\nax.axhline(tau_hat_h.mean(), color='gray', linestyle='--', linewidth=1.5, label=f'Treat Everyone (ATE = {tau_hat_h.mean():.4f})')\nax.axhline(0, color='red', linestyle='-', linewidth=0.8, alpha=0.5)\n\n# Highlight optimal region\nax.fill_between(pct[:int(0.4*len(pct))] * 100, cumulative_mean[:int(0.4*len(pct))],\n                tau_hat_h.mean(), alpha=0.15, color='#35b779', label='Gain from targeting')\n\nax.set_xlabel('Percentage of Customers Targeted (%)', fontsize=12)\nax.set_ylabel('Average CATE Among Targeted Customers', fontsize=12)\nax.set_title('Targeting Policy: Expected Lift by Coverage', fontsize=14)\nax.legend(fontsize=11)\nax.grid(True, alpha=0.3)\nax.set_xlim(0, 100)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nFigure 7: Targeting policy curve: expected treatment effect when emailing only the top k\\% of customers, ranked by estimated CATE. Targeting the top 40% yields a higher average effect while reducing email volume by 60%.\n\n\n\n\n\nFigure 7 illustrates the value of personalization. The gray dashed line shows the ATE from the “email everyone” strategy (6.1 pp). The purple curve shows the average CATE among the top k\\% of customers. By targeting only the top 20% most responsive customers, the firm could achieve an average lift of roughly 15 pp — more than double the blanket approach — while sending 80% fewer emails.\nPerhaps more importantly, the Q1 quintile in Figure 5 showed a negative observed effect. By excluding the bottom ~20% of customers from the campaign, the firm avoids actively harming its relationship with customers who respond negatively to the email. This is the core promise of heterogeneity-aware targeting: not just finding who to treat, but also who to leave alone."
  },
  {
    "objectID": "posts/event_study_comparison/event_study_comparison.html",
    "href": "posts/event_study_comparison/event_study_comparison.html",
    "title": "Interpreting Event Studies Across Modern DiD Methods",
    "section": "",
    "text": "Modern event study plots are the go-to visualization for Difference-in-Differences analysis. But when you overlay results from multiple estimators — Dynamic TWFE, Sun-Abraham, Callaway-Sant’Anna, Imputation DiD — the coefficients may not mean the same thing, even if they look the same visually.\nMulti-model comparisons can go wrong in several ways. In this post, we focus on three practical questions:\nWe use the diff-diff Python package throughout."
  },
  {
    "objectID": "posts/event_study_comparison/event_study_comparison.html#the-landscape",
    "href": "posts/event_study_comparison/event_study_comparison.html#the-landscape",
    "title": "Interpreting Event Studies Across Modern DiD Methods",
    "section": "2.1 The Landscape",
    "text": "2.1 The Landscape\n\n\n\n\n\n\n\n\n\n\nEstimator\nApproach\nHandles Heterogeneous Effects?\nEfficiency\nBest For\n\n\n\n\nDynamic TWFE\nSingle regression with time dummies\nNo — biased under heterogeneity\nHigh (if valid)\nSimultaneous adoption only\n\n\nCallaway-Sant’Anna\n2$$2 DiD aggregation\nYes\nModerate\nPrimary staggered estimator; flexible aggregation\n\n\nSun-Abraham\nInteraction-weighted regression\nYes\nModerate\nRobustness check alongside CS\n\n\nImputation / BJS\nImpute counterfactuals, then aggregate\nYes\nHighest (~50% shorter CIs)\nPreferred when efficiency matters\n\n\nTwo-Stage DiD (Gardner)\nResidualize on untreated, then regress\nYes\nHighest (same point estimates as BJS)\nAlternative to BJS with GMM variance\n\n\nSynthetic DiD\nReweight controls to match treated pre-trends\nYes\nVaries\nFew treated units; questionable parallel trends"
  },
  {
    "objectID": "posts/event_study_comparison/event_study_comparison.html#decision-tree",
    "href": "posts/event_study_comparison/event_study_comparison.html#decision-tree",
    "title": "Interpreting Event Studies Across Modern DiD Methods",
    "section": "2.2 Decision Tree",
    "text": "2.2 Decision Tree\nIs treatment adopted at the same time by all units?\n\nYes \\rightarrow Dynamic TWFE is fine (no staggered-adoption bias). Use MultiPeriodDiD.\nNo (staggered) \\rightarrow continue below.\n\nDo you want the most efficient estimates?\n\nYes \\rightarrow Imputation DiD (or equivalently Two-Stage DiD). These produce the shortest confidence intervals under homogeneous treatment effects. Use as your primary estimator.\nNo / want robustness \\rightarrow continue below.\n\nPrimary analysis or robustness check?\n\nPrimary \\rightarrow Callaway-Sant’Anna. Most flexible: supports doubly robust estimation, covariates, and multiple aggregation schemes.\nRobustness \\rightarrow Sun-Abraham. If CS and SA agree, results are more credible. If they disagree, investigate heterogeneity.\n\nVery few treated units or poor parallel trends?\n\nYes \\rightarrow Consider Synthetic DiD or TROP. These reweight controls to match the treated unit’s pre-treatment trajectory."
  },
  {
    "objectID": "posts/event_study_comparison/event_study_comparison.html#when-results-should-agree",
    "href": "posts/event_study_comparison/event_study_comparison.html#when-results-should-agree",
    "title": "Interpreting Event Studies Across Modern DiD Methods",
    "section": "2.3 When Results Should Agree",
    "text": "2.3 When Results Should Agree\nUnder parallel trends and homogeneous treatment effects, all estimators should produce similar point estimates. When they diverge, it typically signals:\n\nTreatment effect heterogeneity across cohorts or over time\nParallel trends violations (different estimators are more or less sensitive)\nDifferent comparison populations (never-treated vs. not-yet-treated — more on this in Section 7)"
  },
  {
    "objectID": "posts/event_study_comparison/event_study_comparison.html#numerical-example",
    "href": "posts/event_study_comparison/event_study_comparison.html#numerical-example",
    "title": "Interpreting Event Studies Across Modern DiD Methods",
    "section": "3.1 Numerical Example",
    "text": "3.1 Numerical Example\nLet’s make this concrete. Suppose the treated-control gap \\bar{Y}_t^D grows linearly in the pre-treatment period, then jumps at t=0 due to a treatment effect:\n\n# Concrete example: gap grows linearly, then treatment kicks in\nperiods = np.array([-5, -4, -3, -2, -1, 0, 1, 2, 3])\nY_gap   = np.array([1.0, 1.5, 2.0, 2.5, 3.0, 6.0, 7.0, 8.0, 9.0])\n\nref = -1\nref_idx = np.where(periods == ref)[0][0]\n\n# Long differences: each period vs fixed reference (t=-1)\nbeta_long = Y_gap - Y_gap[ref_idx]\n\n# Short differences: each period vs its immediate predecessor\nbeta_short = np.full(len(periods), np.nan)\nfor i in range(1, len(periods)):\n    beta_short[i] = Y_gap[i] - Y_gap[i-1]\n\nexample = pd.DataFrame({\n    'period': periods,\n    'Y_gap': Y_gap,\n    'beta_long (vs t=-1)': beta_long,\n    'beta_short (vs t-1)': beta_short,\n})\nexample\n\n\n\n\n\n\n\n\nperiod\nY_gap\nbeta_long (vs t=-1)\nbeta_short (vs t-1)\n\n\n\n\n0\n-5\n1.0\n-2.0\nNaN\n\n\n1\n-4\n1.5\n-1.5\n0.5\n\n\n2\n-3\n2.0\n-1.0\n0.5\n\n\n3\n-2\n2.5\n-0.5\n0.5\n\n\n4\n-1\n3.0\n0.0\n0.5\n\n\n5\n0\n6.0\n3.0\n3.0\n\n\n6\n1\n7.0\n4.0\n1.0\n\n\n7\n2\n8.0\n5.0\n1.0\n\n\n8\n3\n9.0\n6.0\n1.0\n\n\n\n\n\n\n\nNotice the key difference:\n\nLong diffs (pre-treatment): -2.0, -1.5, -1.0, -0.5, 0 — a clear downward-to-zero slope revealing the linear pre-trend violation\nShort diffs (pre-treatment): 0.5, 0.5, 0.5, 0.5 — constant and seemingly harmless\n\nBoth representations contain the same information, but the visual impression is very different. A linear pre-trend violation is immediately visible with long differences (the coefficients trend toward the reference), but hidden with short differences (the coefficients look flat).\nThis distinction becomes critical when we mix estimators that use different constructions on the same plot.\n\n\nCode\nfig, axes = plt.subplots(1, 2, figsize=(14, 5), sharey=False)\n\n# Long differences\nax = axes[0]\nax.plot(periods, beta_long, 'o-', color='#2563eb', markersize=8, linewidth=2, zorder=3)\nax.fill_between(periods, beta_long - 0.3, beta_long + 0.3, alpha=0.15, color='#2563eb')\nax.axhline(0, color='gray', linestyle='--', linewidth=1)\nax.axvline(-0.5, color='gray', linestyle=':', linewidth=1.5, alpha=0.7)\nax.set_title('Long Differences (vs. $t=-1$)', fontsize=13, fontweight='bold')\nax.set_xlabel('Period', fontsize=11)\nax.set_ylabel('$\\\\hat{\\\\beta}_t$', fontsize=12)\nax.text(-4.5, -1.5, 'Pre-trend\\nvisible as slope', fontsize=9, style='italic', color='#2563eb')\nax.grid(True, alpha=0.2)\n\n# Short differences\nax = axes[1]\nax.plot(periods[1:], beta_short[1:], 'o-', color='#dc2626', markersize=8, linewidth=2, zorder=3)\nax.fill_between(periods[1:], beta_short[1:] - 0.3, beta_short[1:] + 0.3, alpha=0.15, color='#dc2626')\nax.axhline(0, color='gray', linestyle='--', linewidth=1)\nax.axvline(-0.5, color='gray', linestyle=':', linewidth=1.5, alpha=0.7)\nax.set_title('Short Differences (vs. $t-1$)', fontsize=13, fontweight='bold')\nax.set_xlabel('Period', fontsize=11)\nax.set_ylabel('$\\\\hat{\\\\beta}_t$', fontsize=12)\nax.text(-4.5, 2.5, 'Pre-trend\\nhidden (flat!)', fontsize=9, style='italic', color='#dc2626')\nax.grid(True, alpha=0.2)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nFigure 1: Long differences (left) reveal the pre-trend violation as a slope; short differences (right) flatten it into a constant."
  },
  {
    "objectID": "posts/event_study_comparison/event_study_comparison.html#dynamic-twfe",
    "href": "posts/event_study_comparison/event_study_comparison.html#dynamic-twfe",
    "title": "Interpreting Event Studies Across Modern DiD Methods",
    "section": "4.1 Dynamic TWFE",
    "text": "4.1 Dynamic TWFE\nReference: Last pre-period (e=-1), dropped from the regression.\nMethod: OLS regression with period dummies:\nY_{it} = \\alpha_i + \\gamma_t + \\sum_{e \\neq -1} \\beta_e \\cdot \\mathbb{1}[t - g_i = e] + \\varepsilon_{it}\nAll \\beta_e coefficients (both pre and post) are long differences relative to e=-1.\nPre-treatment \\hat{\\beta}_{-3} answers: “how different was the gap at e=-3 vs. e=-1?”"
  },
  {
    "objectID": "posts/event_study_comparison/event_study_comparison.html#sun-abraham",
    "href": "posts/event_study_comparison/event_study_comparison.html#sun-abraham",
    "title": "Interpreting Event Studies Across Modern DiD Methods",
    "section": "4.2 Sun-Abraham",
    "text": "4.2 Sun-Abraham\nReference: Also e=-1, also dropped from the regression.\nMethod: Same regression structure as Dynamic TWFE, but with cohort-specific interactions to handle treatment effect heterogeneity:\nY_{it} = \\alpha_i + \\gamma_t + \\sum_{g} \\sum_{e \\neq -1} \\delta_{g,e} \\cdot \\mathbb{1}[G_i = g] \\cdot \\mathbb{1}[t - g = e] + \\varepsilon_{it}\nThen aggregates: \\hat{\\beta}_e = \\sum_g w_{g,e} \\cdot \\hat{\\delta}_{g,e} (interaction-weighted average).\nAll \\beta_e are still long differences relative to e=-1, just with proper heterogeneity handling.\nBottom line: Sun-Abraham is TWFE done right. Same reference, same type of coefficient, same interpretation. “Dropping from regression” = normalizing to zero."
  },
  {
    "objectID": "posts/event_study_comparison/event_study_comparison.html#callaway-santanna",
    "href": "posts/event_study_comparison/event_study_comparison.html#callaway-santanna",
    "title": "Interpreting Event Studies Across Modern DiD Methods",
    "section": "4.3 Callaway-Sant’Anna",
    "text": "4.3 Callaway-Sant’Anna\nReference: e=-1 (the period before treatment for each cohort g).\nMethod: Builds group-time ATTs \\widehat{ATT}(g,t) via 2$$2 DiD comparisons, then aggregates to an event study.\nHere is where it gets tricky: CS has two modes for the base period:\n\n4.3.1 base_period='universal'\nALL coefficients (pre and post) use t = g-1 as the comparison period:\n\\hat{\\beta}_e = \\widehat{ATT}(g, g+e) \\quad \\text{where each ATT compares to } t=g-1\nThis is a long difference — same as TWFE / Sun-Abraham. Comparable: Yes.\n\n\n4.3.2 base_period='varying' (the default)\n\nPost-treatment: Uses t=g-1 as comparison \\rightarrow long difference\nPre-treatment: Uses t-1 as comparison for each t \\rightarrow short difference\n\nThis means pre- and post-treatment coefficients are constructed with different formulas. The pre-treatment coefficients answer a different question than the post-treatment ones.\nComparable: Only post-treatment. Pre-treatment coefficients are a fundamentally different quantity."
  },
  {
    "objectID": "posts/event_study_comparison/event_study_comparison.html#imputation-did-borusyak-jaravel-spiess-and-two-stage-did-gardner",
    "href": "posts/event_study_comparison/event_study_comparison.html#imputation-did-borusyak-jaravel-spiess-and-two-stage-did-gardner",
    "title": "Interpreting Event Studies Across Modern DiD Methods",
    "section": "4.4 Imputation DiD (Borusyak-Jaravel-Spiess) and Two-Stage DiD (Gardner)",
    "text": "4.4 Imputation DiD (Borusyak-Jaravel-Spiess) and Two-Stage DiD (Gardner)\nThis one works completely differently from the others. No regression with period dummies at all.\nMethod (two steps):\n\nEstimate the counterfactual: Using only untreated observations (units before they get treated + never-treated), fit unit and time fixed effects: \\hat{Y}_{it}(0) = \\hat{\\alpha}_i + \\hat{\\gamma}_t This gives a prediction of what Y would have been without treatment for every unit-time.\nCompute individual treatment effects: For each treated observation: \\hat{\\tau}_{it} = Y_{it} - \\hat{Y}_{it}(0) = Y_{it} - \\hat{\\alpha}_i - \\hat{\\gamma}_t\nAggregate by horizon: Group the \\hat{\\tau}_{it} by relative time h = t - g_i and average: \\hat{\\beta}_h = \\frac{1}{N_h} \\sum_{i,t: t-g_i = h} \\hat{\\tau}_{it}\n\nReference period: BJS doesn’t naturally produce a coefficient at h=-1. Since at h=-1 no unit is yet treated, there are no \\hat{\\tau}_{it} to aggregate. The code manually sets \\hat{\\beta}_{-1} = 0 for plotting.\nPre-treatment coefficients (h &lt; -1): These are placebo tests — they show Y - \\hat{Y}(0) for periods before treatment. If the model is correctly specified, these should be \\approx 0. They are not “differences relative to a reference period” — they are direct residuals from the counterfactual model.\nComparable: Post-treatment yes (same estimand: ATT at each relative time). Pre-treatment: similar scale but different interpretation (model specification test, not parallel trends relative to a reference).\n\n\n\n\n\n\nGardner’s Two-Stage DiD\n\n\n\nGardner (2022) proposes an equivalent two-step procedure: (1) residualize outcomes using FE estimated on untreated observations, (2) regress residuals on treatment indicators. The point estimates are identical to Imputation DiD. The only difference is variance estimation: Gardner uses a GMM sandwich estimator (accounting for first-stage estimation error), while BJS uses a conservative variance (Theorem 3). For event study comparison purposes, they are interchangeable."
  },
  {
    "objectID": "posts/event_study_comparison/event_study_comparison.html#rules-of-thumb",
    "href": "posts/event_study_comparison/event_study_comparison.html#rules-of-thumb",
    "title": "Interpreting Event Studies Across Modern DiD Methods",
    "section": "6.1 Rules of Thumb",
    "text": "6.1 Rules of Thumb\nSafe multi-model overlays:\n\nTWFE + Sun-Abraham + CS (universal): Fully comparable, provided CS and SA use the same control_group. All use long differences relative to the same reference period.\nAdding Imputation (BJS) or Two-Stage DiD: Post-treatment coefficients are comparable (same estimand). Pre-treatment coefficients are on a similar scale but have a different interpretation. Note that BJS always uses all untreated observations — closest to not_yet_treated.\n\nComparisons that require caution:\n\nCS (varying) with any other estimator: Only post-treatment coefficients are comparable. Pre-treatment coefficients are short differences that will look different from everyone else’s long differences.\nMismatched control groups: If one estimator uses never_treated and another uses not_yet_treated, divergent results may reflect different comparison populations, not estimator disagreement.\n\nPractical checklist before overlaying event studies:\n\nSet base_period='universal' for Callaway-Sant’Anna\nMatch control_group across all estimators (CS, SA)\nNote that BJS/Two-Stage DiD always uses all untreated observations\nIf using CS with base_period='varying', annotate the plot and do not visually compare pre-treatment coefficients\n\n\n\nCode\n# Overlay three estimators under a linear pre-trend violation\nnp.random.seed(123)\nperiods_overlay = np.arange(-5, 6)\nref = -1\ndelta = 0.3 * periods_overlay  # linear confound, no treatment\n\n# TWFE: long diffs\ntwfe = delta - delta[periods_overlay == ref][0] + np.random.normal(0, 0.1, len(periods_overlay))\ntwfe[periods_overlay == ref] = 0.0\ntwfe_se = np.full(len(periods_overlay), 0.2)\n\n# Sun-Abraham: long diffs (similar to TWFE, slightly different noise)\nsa = delta - delta[periods_overlay == ref][0] + np.random.normal(0, 0.1, len(periods_overlay))\nsa[periods_overlay == ref] = 0.0\nsa_se = np.full(len(periods_overlay), 0.22)\n\n# CS (varying): short pre, long post\ncs_var = np.zeros(len(periods_overlay))\nfor i, p in enumerate(periods_overlay):\n    if p &lt; ref:\n        cs_var[i] = 0.3 + np.random.normal(0, 0.1)\n    elif p == ref:\n        cs_var[i] = 0.0\n    else:\n        cs_var[i] = 0.3 * (p - ref) + np.random.normal(0, 0.1)\ncs_se = np.full(len(periods_overlay), 0.25)\n\nfig, axes = plt.subplots(1, 2, figsize=(16, 6))\n\n# Left: The misleading overlay\nax = axes[0]\ndodge = 0.12\nax.errorbar(periods_overlay - dodge, twfe, yerr=1.96*twfe_se, fmt='o-',\n            color='#2563eb', capsize=3, markersize=6, linewidth=1.5, label='TWFE')\nax.errorbar(periods_overlay, sa, yerr=1.96*sa_se, fmt='s-',\n            color='#8b5cf6', capsize=3, markersize=6, linewidth=1.5, label='Sun-Abraham')\nax.errorbar(periods_overlay + dodge, cs_var, yerr=1.96*cs_se, fmt='^-',\n            color='#dc2626', capsize=3, markersize=6, linewidth=1.5, label='CS (varying)')\nax.axhline(0, color='gray', linestyle='--', linewidth=1)\nax.axvline(-0.5, color='gray', linestyle=':', linewidth=1.5, alpha=0.7)\nax.set_title('Multi-Model Overlay (Misleading!)', fontsize=13, fontweight='bold')\nax.set_xlabel('Period Relative to Treatment', fontsize=11)\nax.set_ylabel('Estimated Coefficient', fontsize=12)\nax.legend(fontsize=9, loc='upper left')\nax.grid(True, alpha=0.2)\n\n# Annotation box at bottom-right to avoid legend overlap\nax.text(0.98, 0.02,\n        'Base reference:\\nTWFE: ref=e\\u22121 (long diffs)\\nSun-Abraham: ref=e\\u22121 (long diffs)\\nCS (varying): ref=e\\u22121 [ASYMMETRIC]',\n        transform=ax.transAxes, fontsize=7.5,\n        verticalalignment='bottom', horizontalalignment='right',\n        bbox=dict(boxstyle='round,pad=0.4', facecolor='lightyellow',\n                  edgecolor='orange', alpha=0.85), family='monospace')\n\n# Right: What a reader might wrongly conclude\nax = axes[1]\nax.fill_between([-5.5, -0.5], [-3, -3], [4, 4], alpha=0.06, color='green')\nax.fill_between([-0.5, 5.5], [-3, -3], [4, 4], alpha=0.06, color='red')\nax.text(-3, 3.3, 'Pre-treatment', fontsize=10, color='green', alpha=0.7, ha='center')\nax.text(2.5, 3.3, 'Post-treatment', fontsize=10, color='red', alpha=0.7, ha='center')\n\nax.errorbar(periods_overlay, twfe, yerr=1.96*twfe_se, fmt='o-',\n            color='#2563eb', capsize=3, markersize=5, linewidth=1, alpha=0.4, label='TWFE')\nax.errorbar(periods_overlay, cs_var, yerr=1.96*cs_se, fmt='^-',\n            color='#dc2626', capsize=3, markersize=6, linewidth=2, label='CS (varying)')\nax.axhline(0, color='gray', linestyle='--', linewidth=1)\nax.axvline(-0.5, color='gray', linestyle=':', linewidth=1.5, alpha=0.7)\n\nax.annotate('\"Flat pre-trends\" +\\n\"treatment effect\"?',\n            xy=(0, cs_var[periods_overlay == 0][0]), xytext=(2, -1.5),\n            fontsize=10, color='#dc2626', fontweight='bold',\n            arrowprops=dict(arrowstyle='-&gt;', color='#dc2626', lw=1.5),\n            ha='center')\nax.annotate('Actually: just\\na pre-trend!',\n            xy=(-3, twfe[periods_overlay == -3][0]), xytext=(-4.5, 1.5),\n            fontsize=10, color='#2563eb', fontweight='bold',\n            arrowprops=dict(arrowstyle='-&gt;', color='#2563eb', lw=1.5),\n            ha='center')\n\nax.set_title('True Effect = 0 Everywhere', fontsize=13, fontweight='bold')\nax.set_xlabel('Period Relative to Treatment', fontsize=11)\nax.legend(fontsize=9, loc='upper left')\nax.grid(True, alpha=0.2)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nFigure 3: Multi-model overlay: CS (varying) produces visually misleading pre-trends when compared against TWFE and Sun-Abraham, both of which correctly show the pre-trend violation."
  },
  {
    "objectID": "posts/event_study_comparison/event_study_comparison.html#why-this-matters-for-multi-model-comparisons",
    "href": "posts/event_study_comparison/event_study_comparison.html#why-this-matters-for-multi-model-comparisons",
    "title": "Interpreting Event Studies Across Modern DiD Methods",
    "section": "7.1 Why This Matters for Multi-Model Comparisons",
    "text": "7.1 Why This Matters for Multi-Model Comparisons\nIf you overlay two estimators that use different control groups, the estimates may diverge even if everything else is the same — because they are estimating treatment effects relative to different comparison populations:\n\n\n\n\n\n\n\n\nEstimator A\nEstimator B\nComparable?\n\n\n\n\nCS (never_treated)\nSA (never_treated)\nYes\n\n\nCS (not_yet_treated)\nSA (not_yet_treated)\nYes\n\n\nCS (never_treated)\nSA (not_yet_treated)\nCaution — different comparison populations\n\n\nAny staggered estimator\nImputation DiD / Two-Stage DiD\nCaution — BJS always uses all untreated observations (analogous to not_yet_treated)"
  },
  {
    "objectID": "posts/event_study_comparison/event_study_comparison.html#when-does-the-choice-matter",
    "href": "posts/event_study_comparison/event_study_comparison.html#when-does-the-choice-matter",
    "title": "Interpreting Event Studies Across Modern DiD Methods",
    "section": "7.2 When Does the Choice Matter?",
    "text": "7.2 When Does the Choice Matter?\nThe two control groups give similar results when:\n\nThere are many never-treated units (so the not_yet_treated pool is dominated by them)\nTreatment effects are homogeneous across cohorts\nThere are no anticipation effects\n\nThey diverge when:\n\nFew never-treated units: never_treated has a small, potentially unrepresentative control pool\nHeterogeneous effects by cohort: Future-treated units may have different outcome dynamics than never-treated units\nAnticipation: If units respond before formal treatment, not_yet_treated controls are contaminated"
  },
  {
    "objectID": "posts/event_study_comparison/event_study_comparison.html#practical-recommendation",
    "href": "posts/event_study_comparison/event_study_comparison.html#practical-recommendation",
    "title": "Interpreting Event Studies Across Modern DiD Methods",
    "section": "7.3 Practical Recommendation",
    "text": "7.3 Practical Recommendation\nWhen comparing across estimators, always match the control group:\n# Consistent control group across estimators\ncs = CallawaySantAnna(control_group='never_treated')\nsa = SunAbraham(control_group='never_treated')\n\n# BJS always uses all untreated obs — note this when comparing\nbjs = ImputationDiD()  # no control_group parameter\nIf your results are sensitive to the control group choice, that itself is informative — it suggests treatment effect heterogeneity or anticipation effects that warrant further investigation."
  },
  {
    "objectID": "posts/merger_simulation/Assignment1.html",
    "href": "posts/merger_simulation/Assignment1.html",
    "title": "Merger Simulation of Broadband Internet Services",
    "section": "",
    "text": "Assessing merger competitive effects through merger simulation.\nLet’s assume our industry concerns providers of broadband internet services. Some providers offer multiple products (e.g. multiple download speeds). In this case, the industry is best described by a heterogeneous Bertrand competition with 5 firms offering 11 products. Demand follows a simple logit model.\nProviders A and B want to merge and we were asked to evaluate the effects of the merger. In this document, we will assess the potential anticompetitive effects that the proposed merger might carry."
  },
  {
    "objectID": "posts/merger_simulation/Assignment1.html#quantities-and-prices",
    "href": "posts/merger_simulation/Assignment1.html#quantities-and-prices",
    "title": "Merger Simulation of Broadband Internet Services",
    "section": "\n1.1 Quantities and Prices",
    "text": "1.1 Quantities and Prices\n\ndata &lt;- data.frame(provider = c(rep(\"A\", 3),\n                                rep(LETTERS[seq(from = 2, to = 5)], \n                                    each = 2)),\n                   product = c(\"100 Mbit/s\", \n                               rep(c(\"400 Mbit/s\", \"1000 Mbit/s\"), 3),\n                               rep(c(\"200 Mbit/s\", \"400 Mbit/s\"), 2)),\n                   price = c(40, 65, 95,#Provider A\n                             50, 70,     #Provider B\n                             70, 98,     #Provider C\n                             40, 50,     #Provider D\n                             35, 45),    #Provider E\n                   share = c(0.26, 0.2, 0.05, #Provider A\n                             0.1, 0.09,       #Provider B\n                             0.08, 0.03,          #Provider C\n                             0.03, 0.05,          #Provider D\n                             0.06, 0.05))         #Provider E \n\n# We assume 9 million households, all of them have internet\nM = 9e6\n# Volume Quantity\ndata &lt;- data %&gt;% mutate(quantity = share*M)\n# vectors of prices and quantities\nQQ &lt;- as.matrix(data$share*M)\nPP &lt;- as.matrix(data$price)"
  },
  {
    "objectID": "posts/merger_simulation/Assignment1.html#demand",
    "href": "posts/merger_simulation/Assignment1.html#demand",
    "title": "Merger Simulation of Broadband Internet Services",
    "section": "\n1.2 Demand",
    "text": "1.2 Demand\nThe consumer utility for product i:\n\nU_i=V_i+\\varepsilon_i\n\nWhere V_i is the observable part of the utility. We assume here that V_i takes the following form:\n\nV_i= \\delta_i -\\alpha p_i+\\text{asc}_i\n\nWe can rearrange the market in such that we end up with:\n\n\\ln s_i-\\ln s_0 = \\delta_i -\\alpha p_i+\\text{asc}_i\n\nThe price sensitivity is estimated to be -0.1.\n\nalpha &lt;- -0.1\n\nThe logit demand contains alternative-specific constants (asc) that set the prediction equal to observed market shares. They can be found using the observed prices, quantities and the price sensitivity parameter (if the shares sum to 1, then no outside good is included and by default \\delta_i is normalized to 0).\n\n#intercepts are relative to reference (let's pick product 1)\ndata$asc &lt;- log(data$share) - log(data$share[1]) - \n  alpha*(data$price - data$price[1])"
  },
  {
    "objectID": "posts/merger_simulation/Assignment1.html#firm-ownership",
    "href": "posts/merger_simulation/Assignment1.html#firm-ownership",
    "title": "Merger Simulation of Broadband Internet Services",
    "section": "\n1.3 Firm Ownership",
    "text": "1.3 Firm Ownership\nTable 1 presents the ownership matrix of which products belong to which providers.\n\n\n\nTable 1: Pre-Merger Ownership Structure"
  },
  {
    "objectID": "posts/merger_simulation/Assignment1.html#relevant-market",
    "href": "posts/merger_simulation/Assignment1.html#relevant-market",
    "title": "Merger Simulation of Broadband Internet Services",
    "section": "\n2.1 Relevant Market",
    "text": "2.1 Relevant Market\nTo assess the potential competitive effects of the proposed merger, we will start by discussing briefly the relevant market.\nAs already mentioned, providers of internet usually offer multiple products with certain degree of differentiation. In particular, the various internet offerings with various speeds could be a plausible segmentation of the market for the retail provision of internet services. According to our data, the actual providers offers four different internet speeds: 100 Megabits per second (Mbps), 200 Mbps, 400 Mbps and 1000 Mbps.\nIn recent cases regarding the retail market for internet access, the Dutch competition agency, Authority for Consumers & Markets (ACM), had considered that a market segmentation based on download speeds is not appropriate, primarily because there is substitution both at the demand side and at the supply-side of the market (“Marktanalyse ontbundelde toegang“, 2015). In his view, “the fact that an internet connection, regardless of speed, can be used for the same applications, a distinction based on usage in the market for fixed internet access based on certain speeds is unlikely” (pp. 156-157).\nThe European Commission had followed the a similar approach by either disregarding the possibility of further market segmentation based on internet speed, or by considering it a “left open” issue (DGCOMP/Case M.7978, DGCOMP/Case M.5532, DGCOMP/Case M.6990).\nAs we will see, the quantitative analysis of elasticities and diversion ratios confirms the presence of relevant product substitution between different internet speeds, such that products with different speeds exert competitive pressure on each other.\nTherefore, and also in line with previous European jurispudence, we will take the broadband of internet services as the product relevant market, without further distinctions based on internet speed. The geographic relevant market is at the national level."
  },
  {
    "objectID": "posts/merger_simulation/Assignment1.html#structural-analysis",
    "href": "posts/merger_simulation/Assignment1.html#structural-analysis",
    "title": "Merger Simulation of Broadband Internet Services",
    "section": "\n2.2 Structural Analysis",
    "text": "2.2 Structural Analysis\nOnce defined the relevant market, we can now study their structure in terms of market concentration. The left-hand side of Figure 1 reports the pre-merger market shares measured in terms of units sold (quantity):\n\n\n\n\n\n\n\nFigure 1: Market Shares\n\n\n\n\nFirst, we observe a clear leadership of firm A in the market, with an overall market share of 51%. In contrast, the individual market shares of their rivals are between 11% and 19%, with firm B being the second largest provider of internet.\nNote also that firm A is the only firm that still offers low-speed broadband services (100 Mbps), accounting for the 26% of the overall market. On the other hand, all providers participate on the “400 Mbps” segment. More to the point, the Parties (A and B) offers internet with speed of 400 Mbps and 1,000 Mbps, but do not offer 200 Mbps.\nThe right-hand side of Figure 1 plots the combined market share of the Parties.1 This figure suggests a plausible degree of dominance of the New Entity (AB) as a result of the merger, as the combined market share is now 70%.\nAccording to Commission’s Guidelines on Horizontal Mergers (see Commission 2004), market shares of 50% or more might be evidence of the existence of a dominant market position. However, smaller competitors may act as a sufficient constraining influence if, for example, they have the ability and incentive to increase their supplies.\nThe overall concentration level in a market may also provide useful information about the competitive situation. Table 1 shows the overall concentration levels in terms of Herfindahl-Hirschman Index (HHI).\n\nHerfindahl-Hirschman Index (HHI)\n\nHHI\nValue\n\n\n\nPre-Merger\n3,268\n\n\n\n\\Delta HHI\n1,938\n\n\nPost-Merger\n5,206\n\n\n\nTable 1 also seems to suggest high market concentration (above 2,000).2 While these results give an initial indication of the competitive concern, further investigation is needed."
  },
  {
    "objectID": "posts/merger_simulation/Assignment1.html#unilateral-effects",
    "href": "posts/merger_simulation/Assignment1.html#unilateral-effects",
    "title": "Merger Simulation of Broadband Internet Services",
    "section": "\n2.3 Unilateral Effects",
    "text": "2.3 Unilateral Effects\nWe will now study the potential unilateral effects that may arise due to the merger. By removing one competitor from the market, the merger also reduces competitive constraints that in turn lead to price increases in the relevant market.\n\n2.3.1 Price-Elasticities: Product Substitution\nThe price-elasticity matrix provides a good overview of the competitive pressure each product exerts to each other. On the one hand, both own and cross-elasticities allows us to understand the disciplinary pressure that firms A and B faces from their smaller rivals. On the other hand, elasticities give us information to assess closeness of competition between firm A and B. The underlying idea is that the closer competitors the merging parties are3, the more likely it is that a merger will give rise to significant unilateral effects.\nWe will write a function to calculate elasticities using the given data and price sensitivity. The assumptions underlying Logit demand imply that the probability that a consumer purchases product i is given by:\n\n\\Pr(U_{ij}&gt;U_{ik})=s_i=\\frac{\\exp(V_i)}{\\sum_{k} \\exp(V_k)}\n\nwith V_i=\\delta_i+\\alpha p_i+\\text{asc}_i (Berry 1994). The implies elasticities:\n\nThe own-price elasticity for product i is defined by:\n\n\n\\epsilon_i^{i}=\\alpha p_i(1-s_i)\n\n\nThe cross-price elasticity for product i in reaction to change in price for product j:\n\n\n\\epsilon_j^{i}=-\\alpha p_js_j\n\n\nelast &lt;- function(products, alpha) {\n  own &lt;- alpha*products$price*(1-Pr)\n  cross &lt;- -alpha*products$price %o% Pr\n  E &lt;- cross\n  diag(E) &lt;- own\n  return(E)\n}\n\nFigure 2 shows the matrix of own and cross-price elasticities between the products involved in the relevant market:\n\n\n\n\n\n\n\nFigure 2: Own and Cross-Price Elasticities\n\n\n\n\nRecall that when the cross elasticity of demand \\epsilon_{ij} is greater than 1, the cross elasticity of demand is elastic: a change in price of product j results in a more than proportionate change in quantity demanded for product i.\nFrom Figure 2 we can see high substitution between the Parties, specially from the products offered by B towards the products of A. For example, a 1% price increase in 400 Mbps internet offered by B increases by 1.3% the demand for 100 Mbps internet services of firm A. For the 1000 Mbps service offered by B, the demand increase is 2.45% towards firm A (100 Mbps). On the other hand, the elasticity from firm A towards B is less elastic (between 0.6 and 0.95 in the segments where they overlap).\nFinally, it’s important to note that firm C (400 and 1000 Mbps) and to some extent D (200 and 400 Mbps) exerts quite an important competitive pressure on various of the products offered by A (100 Mbps and 400 Mbps). This implies that, even post-merger, there is still high competitive pressure coming from other products from rivals.\nTherefore, we confirm the presence of high substitution across the different products of internet services."
  },
  {
    "objectID": "posts/merger_simulation/Assignment1.html#calibrate-marginal-costs",
    "href": "posts/merger_simulation/Assignment1.html#calibrate-marginal-costs",
    "title": "Merger Simulation of Broadband Internet Services",
    "section": "\n3.1 Calibrate Marginal Costs",
    "text": "3.1 Calibrate Marginal Costs\nWe will first calibrate the marginal costs.\n\n#calculate elasticities\ndata$PCand &lt;- data$price # candidate price is current price\ndata$s &lt;- data$share # idem\n\nelas &lt;- elast(data, alpha)\n\n#drop columns\ndata$PCand &lt;- data$s &lt;- NULL\n\n#calculate matrix of marginal effects\npart &lt;- E*(QQ%*%t(1/PP))\n\n#calculate implied marginal costs\nCC &lt;- PP + diag(own_pre)*solve(own_pre * part) %*% QQ"
  },
  {
    "objectID": "posts/merger_simulation/Assignment1.html#first-order-conditions-foc",
    "href": "posts/merger_simulation/Assignment1.html#first-order-conditions-foc",
    "title": "Merger Simulation of Broadband Internet Services",
    "section": "\n3.2 First-Order Conditions (FOC)",
    "text": "3.2 First-Order Conditions (FOC)\nSecond, we define a function calculating the first order conditions. The function will give a vector corresponding to FOC value for each product. In equilibrium, we should get a vector of zeros.\nThe part calculating the matrix of marginal effects \\dfrac{\\partial Q_i}{\\partial p_j} can be calculated by using the following relationship:\n\\epsilon_{ij}=\\dfrac{\\dfrac{\\partial Q_i}{\\partial p_j}}{\\dfrac{Q_i}{p_j}}\n\n\nFOC &lt;- function(delta, products, EIG, C, M) { \n  products$PCand = products$price*delta\n  products$s = exp(products$asc + \n                     alpha*products$PCand)/sum(exp(products$asc + \n                                                     alpha*products$PCand))\n  #quantities\n  Q &lt;- as.matrix(products$s*M)\n  #prices\n  P &lt;- as.matrix(products$PCand)\n  #calculate elasticities\n  E &lt;- elast(products, alpha)\n  #marginal effects\n  PART &lt;- E*(Q%*%t(1/P))\n  #FOCs\n  thisFOC &lt;- diag(EIG)*Q + (EIG * PART) %*% (P - C)\n  return(as.vector(thisFOC))\n}\n\nAll we need to do now is to adjust the ownership matrix to reflect the change in market structure due to the merger, as shown in Table 2 .\n\n\n\nTable 2: Post-Merger Ownership Structure\n\n\n\n\n\n\n\n\n\nAnd we now solve the FOC’s with the new ownership structure with the following function:\n\n#post-merger equilibrium\nresult &lt;- BBsolve(rep(1, 11),\n                       function(x){FOC(x,\n                                        products = data,\n                                        EIG = own_post,\n                                        C = CC,\n                                        M = M)})\n\n  Successful convergence."
  },
  {
    "objectID": "posts/merger_simulation/Assignment1.html#post-merger-effects",
    "href": "posts/merger_simulation/Assignment1.html#post-merger-effects",
    "title": "Merger Simulation of Broadband Internet Services",
    "section": "\n3.3 Post-Merger Effects",
    "text": "3.3 Post-Merger Effects\n\n\n\n\n\n\na) What is the average estimated price effect?\n\n\n\nTable 3 reports the average price effects of the merger according to our simulation exercise. We observe that the the loss of a competitor in the relevant market due to the merger leads to a new equilibrium with higher prices on all products. In particular, we found a weighted average price effect4 of around 1.54 euros, that is, a 3 percentage point increase approximately relative to the pre-merger scenario. Figure 3 reports the post-merger effects of each product that resulted from our simulation:\n\n\n\nTable 3: Post-Merger Price Effects\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 3: Post-Merger Price Effects\n\n\n\n\nFigure 3 shows that most of the price increase is primarily driven by the behavior of the New Entity. The simulation predicts a price-increase of around 4.5% for the 100 Mbps service offered by firm A and 4.3% for the 400 Mbps of firm B.\nTo put this in perspective, we can compare it to the results found in Grzybowski and Pereira (2007), were the authors carried a merger simulation in the Portuguese mobile telephony market. They found an average price increase of 7–10% without cost efficiencies. One firm could increase the price by as much as 13–22%.\nAccording to these results, the competitive pressure exerted from rivals is not enough to prevent the New Entity to raise their prices. These results, though modest, suggests competitive risk in the relevant market.\n\n\n\n\n\n\n\nFigure 4: Predicted Market Shares\n\n\n\n\nFinally, we can also estimate the new market shares, as shown in Figure 4. Here we see that practically the same results found in our initial assessment based on simple combined market share between the Parties (Figure 1). Taken all into account, we conclude the existence of unilateral price effects on the relevant market."
  },
  {
    "objectID": "posts/merger_simulation/Assignment1.html#entry-conditions",
    "href": "posts/merger_simulation/Assignment1.html#entry-conditions",
    "title": "Merger Simulation of Broadband Internet Services",
    "section": "\n4.1 Entry Conditions",
    "text": "4.1 Entry Conditions\nTypically, antitrust agencies evaluate whether entry will be likely, timely, and of sufficient magnitude as to discipline post-merger exercise of market power (see Commission 2004). When entering a market is sufficiently easy, a merger is unlikely to pose any significant anti-competitive risk.\nPotential barriers to entry in the broadband internet market could be related to technical advantages that incumbents enjoy, for example, access to essential facilities (network infrastructure, local loops, etc.), which make it difficult for potential entrants to compete successfully. All these investments presents high sunk costs for entrants. This could also implies the presence of large economies of scale and scope, that may also constitute barriers to entry.\nFor example, in Xiao and Orazem (2011), the authors study the importance of sunk costs in determining entry conditions. In the conventional framework, entrants incur sunk costs to enter, while incumbents disregard these costs in deciding on continuation or exit. The authors apply this framework to study entry and competition in the local U.S. broadband markets and find that entry costs for early entrants are smaller than for later entrants, implying the existence of early mover advantages in this market. They also find that entry conditions for the 4th firm and subsequent entrants are stable. This implies that, once the market has between one to three incumbent firms, the fourth entrant has little effect on competitive conduct in the local broadband market.\nThere is also literature that suggests that mergers create new incentives on incumbents to actually create entry barriers in a profitable way. For example, Das Varma and De Stefano (2022) introduced the idea that entry deterrence is a public good amongst incumbents, having the incentive to free-ride on each other’s efforts to deter entry. According to authors, a merger between two incumbents eliminates free-riding between the merging firms, leading to an increase in post-merger investment in entry deterrence.\nOn the other hand, we need to recall that the relevant market in our case is at the retail level. Because of the unbundling regulation that generally applies in the network industries in the Netherlands (CPB, 2005), new entrants can compete with incumbents (such as KPN) on the market for service provision without needing to invest in the essential network facilities. In this case, unbundling means that other firms have access to the local loop, which requires incumbents to allow competitors to install their own equipment on both sides of the loop to provide their own services (CPB, 2005).\nThis is an important factor to consider when assessing entry barriers in the market of broadband internet services, as it would mean lower barriers for entrants in the services market or retail market. An interesting aspect to analyze is whether cable network providers can provide easy access to their connections or if other technical or compatibility barriers still exist."
  },
  {
    "objectID": "posts/merger_simulation/Assignment1.html#efficiency-claims",
    "href": "posts/merger_simulation/Assignment1.html#efficiency-claims",
    "title": "Merger Simulation of Broadband Internet Services",
    "section": "\n4.2 Efficiency Claims",
    "text": "4.2 Efficiency Claims\nIts likely that the Parties had presented efficiencies that may counteract the effects on competition and in particular the potential harm to consumers that it might otherwise have. According to the Guidelines, for the Commission to take account of efficiency claims, the efficiencies have to benefit consumers, be merger-specific and be verifiable. These conditions are cumulative.\nSome theoretical research on the effects of mergers in the telecommunications industry has found the presence of relevant cost-reduction efficiencies. For example, Houngbonon and Jeanjean (2019) studied mobile internet traffic from 18 European markets and found that consumer surplus is maximized in generally markets with 3 symmetric operators. They also suggested that, in mobile mergers, dynamic efficiencies from investment outweigh static efficiencies from market power (except in the case of “3-to-2 mergers”).\nA useful way to take efficiencies into account is to incorporate them in our merger simulation model. For example, in Grzybowski and Pereira (2007), the authors simulated the post-merger effects of the Portuguese mobile telephony market and considered three scenarios: (i) there are no cost efficiencies, (ii) a 5% marginal cost reduction in the costs of the Parties, and (iii) a 10% reduction. On average, the authors found price increases of about 6–10% with a 10% marginal cost reduction.\nAccording the Taragin and Sandfort (2012), we can update our model to evaluate these efficiencies in two different ways. First, when computing the post-merger equilibrium prices, we can insert post-merger marginal costs by multiplying pre-merger marginal costs with the claimed proportional reduction in marginal costs, 1+\\Delta mc (“mcDelta”).\nSecond, we can compute the compensating marginal cost reduction (CMCR) on the merging parties’ products. CMCR is the percentage decrease in the marginal costs of the merging parties’ products necessary to prevent a post-merger price increase (Miller and Sheu 2021). For the Bertrand case, the matrix formula of the compensating marginal cost reduction, expressed as a percentage of pre-merger costs, is:\n\n\\text{CMCR}= (m_{\\text{post}}-m_{\\text{pre}}) \\circ \\frac{1}{1-m_{\\text{pre}}}"
  },
  {
    "objectID": "posts/merger_simulation/Assignment1.html#alternative-elasticity-function",
    "href": "posts/merger_simulation/Assignment1.html#alternative-elasticity-function",
    "title": "Merger Simulation of Broadband Internet Services",
    "section": "Alternative Elasticity Function",
    "text": "Alternative Elasticity Function\nAnother way to calculate the elasticities is to just plug-in the market shares s_i into the formula in the following way:\n\nelast1 &lt;- function(products, alpha) {\n  own &lt;- alpha*products$price*(1-products$share)\n  cross &lt;- -alpha*products$price %o% products$share\n  E &lt;- cross\n  diag(E) &lt;- own\n  return(E)\n}\n\n\n\n\n\n\n\n\nFigure 5: Own and Cross-Price Elasticities"
  },
  {
    "objectID": "posts/merger_simulation/Assignment1.html#footnotes",
    "href": "posts/merger_simulation/Assignment1.html#footnotes",
    "title": "Merger Simulation of Broadband Internet Services",
    "section": "Footnotes",
    "text": "Footnotes\n\nAs in current practice, post-merger market shares are calculated on the assumption that the post-merger combined market share of the merging parties is the sum of their pre-merger market shares.↩︎\nAs stated in the Guidelines, the Commission is unlikely to identify horizontal competition concerns in a merger with a post-merger HHI between 1000 and 2000 and a delta below 250, or a merger with a post-merger HHI above 2000 and a delta below 150↩︎\nCloseness of competition is defined with reference to the propensity of the customers of one of the merging parties to switch to products supplied by the other merging party. The greater that propensity, the closer competitors the two firms are said to be. See paragraph 28, EC Merger Guidelines.↩︎\nThe product-price effects are averaged using the post-merger market shares as weights.↩︎"
  },
  {
    "objectID": "posts/parallel_trends_sensitivity/parallel_trends_sensitivity.html",
    "href": "posts/parallel_trends_sensitivity/parallel_trends_sensitivity.html",
    "title": "Sensitivity Analysis for Parallel Trends",
    "section": "",
    "text": "Passing a pre-trends test does not mean parallel trends holds — it may just mean your test has low power (Roth, 2022). And even a well-powered test cannot rule out violations smaller than its detection threshold.\nSo what should an applied researcher do after running an event study? This post presents a toolkit of four complementary approaches for stress-testing the parallel trends assumption:\nEach tool answers a different question. Together, they provide a much more complete picture than “the pre-trends look flat” ever could.\nWe use the diff-diff Python package, which already includes HonestDiD and pre-trends power analysis. We implement the smoothest confounding path and functional SCBs from scratch."
  },
  {
    "objectID": "posts/parallel_trends_sensitivity/parallel_trends_sensitivity.html#infimum-scbs-pre-treatment-equivalence-testing",
    "href": "posts/parallel_trends_sensitivity/parallel_trends_sensitivity.html#infimum-scbs-pre-treatment-equivalence-testing",
    "title": "Sensitivity Analysis for Parallel Trends",
    "section": "5.1 Infimum SCBs (Pre-treatment: Equivalence Testing)",
    "text": "5.1 Infimum SCBs (Pre-treatment: Equivalence Testing)\nStandard sup-t bands test whether any coefficient differs from zero. But for pre-trends, we want the opposite: evidence that all coefficients are close to zero. Infimum SCBs are designed for this:\n\\hat{\\beta}(t) \\pm \\hat{c}_{\\inf,\\alpha} \\cdot \\hat{\\sigma}(t) \\quad \\text{where} \\quad P\\left(\\inf_t \\frac{|\\hat{\\beta}(t) - \\beta(t)|}{\\hat{\\sigma}(t)} \\leq \\hat{c}_{\\inf,\\alpha}\\right) = 1 - \\alpha\nIf the entire infimum band lies within a pre-specified equivalence region [-\\varepsilon, +\\varepsilon], we have positive evidence for parallel trends — not just failure to reject.\nCrucially, infimum bands are tighter than pointwise CIs. This is the opposite of sup-t bands."
  },
  {
    "objectID": "posts/parallel_trends_sensitivity/parallel_trends_sensitivity.html#supremum-scbs-post-treatment-relevance-testing",
    "href": "posts/parallel_trends_sensitivity/parallel_trends_sensitivity.html#supremum-scbs-post-treatment-relevance-testing",
    "title": "Sensitivity Analysis for Parallel Trends",
    "section": "5.2 Supremum SCBs (Post-treatment: Relevance Testing)",
    "text": "5.2 Supremum SCBs (Post-treatment: Relevance Testing)\nFor post-treatment, we want to show that the effect is significant at all periods simultaneously:\n\\hat{\\beta}(t) \\pm \\hat{c}_{\\sup,\\alpha} \\cdot \\hat{\\sigma}(t) \\quad \\text{where} \\quad P\\left(\\sup_t \\frac{|\\hat{\\beta}(t) - \\beta(t)|}{\\hat{\\sigma}(t)} \\leq \\hat{c}_{\\sup,\\alpha}\\right) = 1 - \\alpha\nIf the supremum band excludes zero everywhere in the post-treatment period, the effect is significant at every period simultaneously. These are equivalent to the sup-t bands from Montiel Olea & Plagborg-Møller (2019) — they are wider than pointwise CIs.\nThe key innovation: use different band types for different regions — tighter bands where you want equivalence (pre), wider bands where you want significance (post).\n\ndef compute_scb_critical_values(vcov, alpha=0.05, n_sim=50000, seed=42):\n    \"\"\"\n    Compute both infimum and supremum SCB critical values.\n    \n    Returns\n    -------\n    dict with 'c_inf' (tighter, for equivalence) and 'c_sup' (wider, for relevance)\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    k = vcov.shape[0]\n    \n    # Correlation matrix\n    sd = np.sqrt(np.diag(vcov))\n    corr = vcov / np.outer(sd, sd)\n    \n    # Ensure PSD\n    eigvals = np.linalg.eigvalsh(corr)\n    if np.any(eigvals &lt; -1e-10):\n        corr = corr + (-min(eigvals) + 1e-6) * np.eye(k)\n    \n    # Simulate\n    draws = rng.multivariate_normal(np.zeros(k), corr, size=n_sim)\n    abs_draws = np.abs(draws)\n    \n    # Supremum: max|z_i| for each draw -&gt; (1-alpha) quantile\n    max_abs = np.max(abs_draws, axis=1)\n    c_sup = np.quantile(max_abs, 1 - alpha)\n    \n    # Infimum: min|z_i| for each draw -&gt; (1-alpha) quantile\n    min_abs = np.min(abs_draws, axis=1)\n    c_inf = np.quantile(min_abs, 1 - alpha)\n    \n    return {'c_inf': c_inf, 'c_sup': c_sup}\n\n\n# Compute SCBs for pre and post separately\nvcov_pre = vcov_es[np.ix_(pre_idx, pre_idx)]\nvcov_post = vcov_es[np.ix_(post_idx, post_idx)]\n\nscb_pre = compute_scb_critical_values(vcov_pre)\nscb_post = compute_scb_critical_values(vcov_post)\n\nc_pw = scipy_stats.norm.ppf(0.975)\n\nprint(f'Pre-treatment SCBs:')\nprint(f'  Pointwise z:     {c_pw:.3f}')\nprint(f'  Infimum c_inf:   {scb_pre[\"c_inf\"]:.3f}  (tighter — for equivalence)')\nprint(f'  Supremum c_sup:  {scb_pre[\"c_sup\"]:.3f}  (wider — for joint significance)')\nprint()\nprint(f'Post-treatment SCBs:')\nprint(f'  Pointwise z:     {c_pw:.3f}')\nprint(f'  Infimum c_inf:   {scb_post[\"c_inf\"]:.3f}')\nprint(f'  Supremum c_sup:  {scb_post[\"c_sup\"]:.3f}')\n\nPre-treatment SCBs:\n  Pointwise z:     1.960\n  Infimum c_inf:   0.857  (tighter — for equivalence)\n  Supremum c_sup:  2.467  (wider — for joint significance)\n\nPost-treatment SCBs:\n  Pointwise z:     1.960\n  Infimum c_inf:   0.745\n  Supremum c_sup:  2.548\n\n\n\n\nCode\nfig, ax = plt.subplots(figsize=(11, 6.5))\n\npre_est_arr = estimates[pre_idx]\npre_se_arr = ses[pre_idx]\npost_est_arr = estimates[post_idx]\npost_se_arr = ses[post_idx]\npre_p_arr = np.array(pre_periods)\npost_p_arr = np.array(post_periods)\n\n# Equivalence region\neps = 1.0  # practical equivalence threshold\nax.axhspan(-eps, eps, alpha=0.08, color='gray', label=f'Equivalence region ($\\\\pm${eps})')\n\n# Pre-treatment: infimum bands (tighter)\nax.fill_between(pre_p_arr,\n                pre_est_arr - scb_pre['c_inf'] * pre_se_arr,\n                pre_est_arr + scb_pre['c_inf'] * pre_se_arr,\n                alpha=0.3, color='#16a34a', label=f'Infimum SCB (pre, $c$ = {scb_pre[\"c_inf\"]:.2f})')\n\n# Post-treatment: supremum bands (wider)\nax.fill_between(post_p_arr,\n                post_est_arr - scb_post['c_sup'] * post_se_arr,\n                post_est_arr + scb_post['c_sup'] * post_se_arr,\n                alpha=0.2, color='#dc2626', label=f'Supremum SCB (post, $c$ = {scb_post[\"c_sup\"]:.2f})')\n\n# Pointwise CIs for reference\nax.fill_between(periods_plot, est_plot - c_pw * se_plot, est_plot + c_pw * se_plot,\n                alpha=0.15, color='#2563eb', label='Pointwise 95% CI')\n\n# Point estimates\nax.plot(periods_plot, est_plot, 'o-', color='#2563eb', markersize=7,\n        linewidth=1.5, zorder=3)\n\nax.axhline(0, color='gray', linestyle='--', linewidth=1)\nax.axvline(ref_period + 0.5, color='gray', linestyle=':', linewidth=1.5, alpha=0.7)\nax.set_xlabel('Period', fontsize=12)\nax.set_ylabel('Treatment Effect', fontsize=12)\nax.set_title('Functional SCBs: Equivalence (Pre) + Relevance (Post)', fontsize=14, fontweight='bold')\nax.legend(fontsize=9, loc='upper left')\nax.grid(True, alpha=0.2)\nplt.tight_layout()\nplt.show()\n\n# Check equivalence\npre_inf_lo = pre_est_arr - scb_pre['c_inf'] * pre_se_arr\npre_inf_hi = pre_est_arr + scb_pre['c_inf'] * pre_se_arr\nequiv = np.all(pre_inf_lo &gt;= -eps) and np.all(pre_inf_hi &lt;= eps)\nprint(f'Pre-treatment equivalence (eps={eps}): {\"PASS\" if equiv else \"FAIL\"}')\n\n# Check relevance\npost_sup_lo = post_est_arr - scb_post['c_sup'] * post_se_arr\nrelevant = np.all(post_sup_lo &gt; 0)\nprint(f'Post-treatment relevance (excludes zero): {\"PASS\" if relevant else \"FAIL\"}')\n\n\n\n\n\n\n\n\nFigure 6: Functional SCBs: tighter infimum bands (green) for pre-treatment equivalence testing, wider supremum bands (red) for post-treatment relevance testing. The gray shaded region marks the equivalence zone.\n\n\n\n\n\nPre-treatment equivalence (eps=1.0): PASS\nPost-treatment relevance (excludes zero): PASS\n\n\nThe functional SCB approach provides two distinct conclusions:\n\nPre-treatment equivalence: If the infimum band (green) lies entirely within the equivalence region (gray), we have positive evidence for parallel trends — not just failure to reject. This is a fundamentally stronger statement.\nPost-treatment relevance: If the supremum band (red) excludes zero at all post-treatment periods, the effect is significant everywhere simultaneously.\n\nNotice that the infimum bands are narrower than pointwise CIs — this is the key insight from Fang & Liebl (2026). By asking “are all deviations small?” instead of “is any deviation large?”, we get tighter bands that are better suited for equivalence testing."
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "Order By\n      Default\n      \n        Date - Oldest\n      \n      \n        Date - Newest\n      \n      \n        Title\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\ndidkit: Enhanced Plotting & Inference for Difference-in-Differences\n\n\n\nPython\n\nCausal Inference\n\nDifference-in-Differences\n\nData Visualization\n\nOpen Source\n\n\n\nA Python package bringing R’s ggfixest plotting capabilities and HonestDiD inference tools to Python. Features event study plots with nested CIs, simultaneous confidence…\n\n\n\n18 February 2026\n\n\n\n\n\n\n\n\n\n\n\n\n\nCustomer Segmentation & Recommendation Engine\n\n\n\nPython\n\nPySpark\n\nMachine Learning\n\nNLP\n\nDocker\n\nGCP\n\nFastAPI\n\nscikit-learn\n\n\n\nEnd-to-end ML system: PySpark feature engineering, BERT sentiment analysis, K-Means segmentation, and hybrid recommendation engine. Deployed as a live API on GCP Cloud Run…\n\n\n\n17 February 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nPriceScope: Open-Source Conjoint Analysis with Synthetic Respondents\n\n\n\nPython\n\nConjoint Analysis\n\nDiscrete Choice\n\nPyBLP\n\nFastAPI\n\n\n\n\n15 January 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nMachine Learning with Python: A Quick Guide\n\n\n\nMachine Learning\n\nPython\n\nscikit-learn\n\n\n\n\n30 March 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\nAmsterdam Airbnb Price Prediction: A Machine Learning Approach\n\n\n\nPython\n\nGeospatial Analysis\n\nMachine Learning\n\ngeopandas\n\nscikit-learn\n\n\n\n\n11 March 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\nCustomer Analytics with R\n\n\n\nR & Rstudio\n\nA/B Testing\n\nMachine Learning\n\n\n\n\n01 December 2022\n\n\n\n\n\n\nNo matching items\n Back to top"
  }
]